REPO DUMP
Generated: 2026-02-24 14:13:44.698730
Root: E:\Skrypty\GRN_v2\GRN_v2\mRNA-PLUM



================================================================================
.gitignore
================================================================================

__pycache__/
*.pyc
.venv/
OUT/
IN/
*.duckdb
*.log
dist/
build/


================================================================================
.vscode\settings.json
================================================================================

{
    "python-envs.defaultEnvManager": "ms-python.python:system",
    "python-envs.pythonProjects": []
}

================================================================================
config.example.yaml
================================================================================

# Szukamy wszystkich CSV w katalogu root i podkatalogach (możesz zawęzić np. "logs/**/*.csv")
input_glob: "**/*.csv"

# Plik formularza (Excel) zawierający arkusz KEYS
keys_workbook: "RNA_PLUM_Form.xlsx"
keys_sheet: "KEYS"

# Nazwy kolumn w logach
col_time: "Czas"
col_context: "Kontekst zdarzenia"
col_desc: "Opis"
col_component: "Składnik"
col_event_name: "Nazwa zdarzenia"

# Kurs i okres z Kontekst zdarzenia
course_regex: "Kurs:\\s*([^\\s]+)"
period_regex: "(\\d{4}/\\d{2}[zl])"

chunk_rows: 200000


================================================================================
config.yaml
================================================================================

app:
  name: "RNA-PLUM"
  version: "2.0"
  timezone: "Europe/Warsaw"

paths:
  logs_root: "D:/RNA/logs_raw"                 # folder z podfolderami kursów
  merged_logs_out: "D:/RNA/_staging/merged"    # opcjonalnie, jeśli chcesz zachować CSV po scaleniu
  db_path: "D:/RNA/_db/rna_plum.duckdb"
  parquet_root: "D:/RNA/_parquet"             # partycje surowe/kanoniczne
  output_root: "D:/RNA/_out"
  reports_excel: "D:/RNA/_out/raport_zbiorczy.xlsx"
  individual_reports_dir: "D:/RNA/_out/raporty_indywidualne"
  pdf_dir: "D:/RNA/_out/pdf"
  excel_template: "D:/RNA/templates/wzor_raportu.xlsx"

inputs:
  # opcjonalne
  teachers_xlsx: "D:/RNA/dane/Dane_NA.xlsx"          # kolumny: ID_PLUM, Pełna nazwa, ...
  html_id_email: "D:/RNA/dane/ID_Email.html"         # alternatywne źródło email->ID
  courses_meta: "D:/RNA/dane/courses_meta.xlsx"      # course_key->kierunek/wydzial

ingest:
  filename_coursekey_regex: "^logs_(?P<coursekey>.+?)_\\d{8}-\\d{4}\\.csv$"
  recursive: true
  delimiter_detect: true
  encoding_detect: true
  normalize_to_utf8: true
  dedup:
    mode: "full_row_hash"     # zgodnie z VBA: dedup tylko identyczny wiersz
    hash: "sha1"
  sort:
    by_time_column_names: ["czas","time","timecreated","data","date"]
    newest_first: true

userid_extraction:
  # zgodne z VBA: "The user with id '7997' ..."
  regexes:
    - "user with id\\s*'(?P<id>\\d+)'"
    - "id\\s*=\\s*(?P<id>\\d+)"
  prefer_columns: ["userid","user id","ID","ID_PLUM"]  # jeśli obecne w logach
  fallback_to_description: true

filters:
  drop_if_email_contains:
    - "student"       # jak VBA usuwa wiersze, gdzie mail zawiera student
  drop_if_username_contains:
    - "student"
  # dodatkowo: whitelist domen, jeśli potrzebne
  allowed_email_domains: []   # np. ["umw.edu.pl"]
  # zakres czasu (opcjonalnie)
  date_from: null
  date_to: null

activity_mapping:
  # "Legenda1": exact match po event_name (odpowiednik kol. D)
  exact:
    # raw_event_name: activity_label
    "Course module created": "Materiały"
    "Question created": "Pytania"
  # "Legenda2": contains match po description (odpowiednik szukania fragmentu w M)
  contains:
    "mod_quiz": "Testy/Quizy"
    "forum post": "Forum"
  # opcjonalnie regex (najbardziej elastyczne)
  regex:
    - pattern: "assign(ment)?"
      label: "Zadania"
      flags: "i"
      priority: 50

aggregation:
  group_keys:
    - teacher_id
    - course_key
    - activity_label
  percent_denominators:
    course: ["course_key"]
    kierunek: ["kierunek"]
    wydzial: ["wydzial"]
    uczelnia: []  # global
  min_count_thresholds:
    # progi do “flagowania” a nie do kasowania
    low_activity_total: 1
    suspiciously_high_per_day: 500

reports:
  excel:
    include_qc_log: true
    sheets:
      zliczenie: "ZLICZENIE_AKTYWNOSCI_NA"
      uczelnia: "PODSUM_UCZELNIA"
      wydzial: "PODSUM_WYDZIAL"
      kierunek: "PODSUM_KIERUNEK"
      kurs: "PODSUM_KURS"
      qc: "QC_LOG"

  individual_xlsx:
    sheet_courses: "DANE_KURSY"
    sheet_person: "DANE_PERS"

  pdf:
    enabled: true
    engine: "excel_com"        # "excel_com" (Windows) albo "libreoffice" (fallback)
    template_fill_mode: "anchors"  # "anchors" (label->value) lub "named_ranges"
    export_quality: "standard"

parse_events:
  keys_xlsx: "{root}/KEYS.xlsx"   # VBA podmieni {root} lub przekaże absolutną ścieżkę
  keys_sheet: "KEYS"
  fetch_size: 5000
  insert_batch_size: 20000
  export_parquet: true

  filters:
    student_email_domain: "@student.umw.edu.pl"

    tech_key_whitelist: []      # np. ["URL_CREATE","URL_DELETE"]
    tech_key_blacklist: []      # np. ["FOLDER_VIEWED"]

    source_whitelist: []        # np. ["web", "cli"]
    source_blacklist: []        # np. ["cron"]

    date_from: null
    date_to: null

build_activities_state:
  snapshots_dir: "_data/snapshots"
  snapshots_glob: "*.csv"

  # jak interpretujemy activity_id w snapshotach:
  activity_id_kind: "cmid"  # "cmid" | "module_id" | "unknown"

  deletion:
    delete_operations: ["DELETE"]     # jeśli operation jest standaryzowane
    delete_tech_keys: []              # np. ["activity_delete", "mod_delete_*"] – jeśli KEYS tak steruje
    delete_activity_labels_regex: []  # jeśli w KEYS activity_label niesie semantykę usunięcia
    disappearance_grace_period_days: 14
    min_missing_snapshots_to_confirm: 2
    deleted_at_policy: "first_missing"  # "first_missing" | "last_seen"

  mapping:
    use_activity_id_map_table: true
    allow_fuzzy_name_type_match: false
    name_normalization:
      lower: true
      strip: true
      collapse_ws: true
      remove_diacritics: true
      remove_punctuation: true

  incremental:
    checkpoint_table: "raw.pipeline_checkpoints"
    checkpoint_key: "build_activities_state"
    process_only_new_snapshots: true
    process_only_new_events: true

  qa:
    write_all_conflicts: true

duckdb_path: _run/warehouse.duckdb
run_dir: _run

period:
  ay: "2025/26"
  term: "Z"   # np. Z/L

rebuild_full: false

aggregation:
  include_deleted_in_percent: false

stats:
  pct_round_decimals: 4

mapping:
  teacher_id_email: data/mapping/teacher_id_email.xlsx

hr:
  file: data/hr/HR.xlsx
  sheet: "HR"   # jeśli masz, jak nie to usuń i weź pierwszy arkusz
  teacher_id_col: "ID_PLUM"
  email_col: "E-mail"
  full_name_col: "Pełna nazwa"
  wydzial_col: "Wydział jednostki zatrudnienia"
  jednostka_col: "Jednostka podlegajaca rozliczeniu"
  teachers_only: true
  passthrough_cols:
    - "ID bazus"

export:
  activity_column: "activity_label"
  course_column: "course_name"
  include_hr_cols: true
  exclude_zero_counts: true
  percent_excel_format: true
  max_rows_excel: 1000000
  overflow_strategy: "error"

export_individual_reports(con, cfg)

================================================================================
FULL_REPO_DUMP.txt
================================================================================



================================================================================
KEYS.xlsx
================================================================================

[BINARY OR UNREADABLE FILE]


================================================================================
src\FULL_REPO_DUMP.txt
================================================================================



================================================================================
FULL_REPO_DUMP.txt
================================================================================



================================================================================
mrna_plum\__init__.py
================================================================================

__all__ = ["__version__"]
__version__ = "0.1.0"

================================================================================
mrna_plum\__main__.py
================================================================================

from .cli import app

if __name__ == "__main__":
    app()


================================================================================
mrna_plum\activities\__init__.py
================================================================================



================================================================================
mrna_plum\activities\activities_state.py
================================================================================

from __future__ import annotations

from dataclasses import dataclass
from typing import List, Any, Dict, Optional
import json
import uuid

import duckdb


# --- Config dataclasses expected by tests ---

@dataclass(frozen=True)
class DeletionConfig:
    delete_operations: List[str]
    delete_tech_keys: List[str]
    delete_activity_labels_regex: List[str]
    disappearance_grace_period_days: int
    min_missing_snapshots_to_confirm: int
    deleted_at_policy: str  # "first_missing" | "last_seen"


@dataclass(frozen=True)
class MappingConfig:
    use_activity_id_map_table: bool
    allow_fuzzy_name_type_match: bool


@dataclass(frozen=True)
class IncrementalConfig:
    checkpoint_table: str
    checkpoint_key: str
    process_only_new_snapshots: bool
    process_only_new_events: bool


@dataclass(frozen=True)
class BuildConfig:
    deletion: DeletionConfig
    mapping: MappingConfig
    incremental: IncrementalConfig


def _ensure_tables(con: duckdb.DuckDBPyConnection) -> None:
    con.execute("create schema if not exists raw;")
    con.execute("create schema if not exists mart;")

    con.execute("""
    create table if not exists mart.activities_state (
      course_code varchar not null,
      ay varchar,
      term varchar,
      wydzial_code varchar,
      kierunek_code varchar,
      track_code varchar,
      semester_code varchar,

      activity_id varchar not null,
      type varchar,
      name_last varchar,

      first_seen_at timestamp,
      last_seen_at timestamp,

      last_snapshot_at timestamp,
      last_event_at timestamp,

      visible_last boolean,
      deleted_at timestamp,

      status_final varchar not null,
      evidence_deleted varchar not null,
      confidence_deleted double not null,

      notes varchar,
      updated_at timestamp default now(),

      primary key(course_code, activity_id)
    );
    """)

    con.execute("""
    create table if not exists mart.activities_qa (
      qa_id varchar,
      qa_type varchar not null,
      course_code varchar,
      activity_id varchar,
      object_id varchar,
      details_json varchar,
      created_at timestamp default now()
    );
    """)


def _qa(con: duckdb.DuckDBPyConnection, qa_type: str,
        course_code: Optional[str], activity_id: Optional[str],
        object_id: Optional[str], details: Dict[str, Any]) -> None:
    con.execute(
        """
        insert into mart.activities_qa(qa_id, qa_type, course_code, activity_id, object_id, details_json)
        values (?,?,?,?,?,?)
        """,
        [str(uuid.uuid4()), qa_type, course_code, activity_id, object_id, json.dumps(details, ensure_ascii=False)],
    )


def build_activities_state(con: duckdb.DuckDBPyConnection, cfg: BuildConfig) -> dict:
    """
    Public API expected by tests.
    Minimal implementation to satisfy:
    - delete from logs
    - disappearance from snapshots
    - hidden
    - conflict QA
    - missing mapping QA
    """
    _ensure_tables(con)

    # Universe: wszystkie (course_code, activity_id) z snapshotów + eventów gdzie object_id nie null
    con.execute("""
    create temp table tmp_universe as
    select distinct course_code, activity_id
    from raw.activities_snapshot
    union
    select distinct course_code, cast(object_id as varchar) as activity_id
    from events_canonical
    where object_id is not null;
    """)

    # Snapshot last
    con.execute("""
    create temp table tmp_snap_last as
    select s.*
    from raw.activities_snapshot s
    join (
      select course_code, activity_id, max(captured_at) as mx
      from raw.activities_snapshot
      group by 1,2
    ) t
    on s.course_code=t.course_code and s.activity_id=t.activity_id and s.captured_at=t.mx;
    """)

    # Snapshot bounds per activity
    con.execute("""
    create temp table tmp_snap_bounds as
    select course_code, activity_id, min(captured_at) as first_snap, max(captured_at) as last_snap
    from raw.activities_snapshot
    group by 1,2;
    """)

    # Event bounds per object_id (map 1:1 -> activity_id)
    con.execute("""
    create temp table tmp_evt_bounds as
    select course_code, cast(object_id as varchar) as activity_id,
           min(ts_utc) as first_evt,
           max(ts_utc) as last_evt
    from events_canonical
    where counted = true and object_id is not null
    group by 1,2;
    """)

    # Delete from logs (operation == DELETE)
    con.execute("""
    create temp table tmp_evt_delete as
    select course_code, cast(object_id as varchar) as activity_id, min(ts_utc) as deleted_at_log
    from events_canonical
    where counted = true
      and object_id is not null
      and upper(operation) = 'DELETE'
    group by 1,2;
    """)

    # Disappearance:
    # - last_seen_snap_at = max(captured_at) for activity
    # - count snapshots of course after last_seen_snap_at (>=min_missing)
    # - last_course_snapshot_at >= first_missing + grace
    con.execute("""
    create temp table tmp_course_snaps as
    select course_code, captured_at
    from raw.activities_snapshot
    group by 1,2;
    """)

    con.execute("""
    create temp table tmp_last_seen as
    select course_code, activity_id,
           max(captured_at) as last_seen_snap_at,
           min(captured_at) as first_seen_snap_at
    from raw.activities_snapshot
    group by 1,2;
    """)

    con.execute("""
    create temp table tmp_missing as
    select
      a.course_code,
      a.activity_id,
      a.last_seen_snap_at,
      min(cs.captured_at) as first_missing_snapshot_at,
      count(*) as missing_count,
      max(cs.captured_at) as last_course_snapshot_at
    from tmp_last_seen a
    join tmp_course_snaps cs
      on cs.course_code=a.course_code and cs.captured_at > a.last_seen_snap_at
    group by 1,2,3;
    """)

    con.execute("""
    create temp table tmp_disappearance as
    select
      course_code,
      activity_id,
      case
        when ? = 'first_missing' then first_missing_snapshot_at
        else last_seen_snap_at
      end as deleted_at_snap,
      first_missing_snapshot_at,
      last_course_snapshot_at,
      missing_count
    from tmp_missing
    where missing_count >= ?
      and last_course_snapshot_at >= (first_missing_snapshot_at + (? || ' days')::interval);
    """, [cfg.deletion.deleted_at_policy, cfg.deletion.min_missing_snapshots_to_confirm, cfg.deletion.disappearance_grace_period_days])

    # Course meta (z eventów)
    con.execute("""
    create temp table tmp_course_meta as
    select course_code,
           any_value(ay) as ay,
           any_value(term) as term,
           any_value(wydzial_code) as wydzial_code,
           any_value(kierunek_code) as kierunek_code,
           any_value(track_code) as track_code,
           any_value(semester_code) as semester_code
    from events_canonical
    group by 1;
    """)

    # Final stage
    con.execute("""
    create temp table tmp_final as
    select
      u.course_code,
      m.ay, m.term, m.wydzial_code, m.kierunek_code, m.track_code, m.semester_code,
      u.activity_id,
      sl.type,
      sl.name as name_last,
      least(sb.first_snap, eb.first_evt) as first_seen_at,
      greatest(sb.last_snap, eb.last_evt) as last_seen_at,
      sb.last_snap as last_snapshot_at,
      eb.last_evt as last_event_at,
      sl.visible_to_students as visible_last,
      dlog.deleted_at_log,
      ds.deleted_at_snap,
      case
        when dlog.deleted_at_log is not null and ds.deleted_at_snap is not null then least(dlog.deleted_at_log, ds.deleted_at_snap)
        when dlog.deleted_at_log is not null then dlog.deleted_at_log
        when ds.deleted_at_snap is not null then ds.deleted_at_snap
        else null
      end as deleted_at,
      case
        when dlog.deleted_at_log is not null and ds.deleted_at_snap is not null then 'both'
        when dlog.deleted_at_log is not null then 'log_delete_event'
        when ds.deleted_at_snap is not null then 'snapshot_disappearance'
        else 'none'
      end as evidence_deleted,
      case
        when dlog.deleted_at_log is not null and ds.deleted_at_snap is not null then 0.95
        when dlog.deleted_at_log is not null then 0.80
        when ds.deleted_at_snap is not null then 0.70
        else 0.0
      end as confidence_deleted
    from tmp_universe u
    left join tmp_course_meta m using(course_code)
    left join tmp_snap_last sl using(course_code, activity_id)
    left join tmp_snap_bounds sb using(course_code, activity_id)
    left join tmp_evt_bounds eb using(course_code, activity_id)
    left join tmp_evt_delete dlog using(course_code, activity_id)
    left join tmp_disappearance ds using(course_code, activity_id);
    """)

    # QA: conflict log delete but snapshot visible after delete
    rows = con.execute("""
    select course_code, activity_id, deleted_at_log, last_snapshot_at
    from tmp_final
    where deleted_at_log is not null
      and last_snapshot_at is not null
      and last_snapshot_at > deleted_at_log
      and visible_last = true;
    """).fetchall()
    for course_code, activity_id, deleted_at_log, last_snapshot_at in rows:
        _qa(con, "conflict_log_delete_but_visible_in_snapshot", course_code, activity_id, None, {
            "deleted_at_log": str(deleted_at_log),
            "last_snapshot_at": str(last_snapshot_at),
        })

    # QA: activity without mapping (snapshot exists but no events for it)
    rows = con.execute("""
    select s.course_code, s.activity_id
    from (select distinct course_code, activity_id from raw.activities_snapshot) s
    left join (select distinct course_code, cast(object_id as varchar) as activity_id from events_canonical where object_id is not null) e
      on e.course_code=s.course_code and e.activity_id=s.activity_id
    where e.activity_id is null;
    """).fetchall()
    for course_code, activity_id in rows:
        _qa(con, "activity_without_object_id_mapping", course_code, activity_id, None, {})

    # status_final
    con.execute("""
    create temp table tmp_final2 as
    select *,
      case
        when deleted_at is not null then 'visible_deleted'
        when visible_last = true then 'visible_active'
        when visible_last = false then 'hidden'
        else 'unknown'
      end as status_final,
      cast(null as varchar) as notes
    from tmp_final;
    """)

    # Upsert (delete+insert for simplicity in tests)
    con.execute("delete from mart.activities_state;")
    con.execute("""
    insert into mart.activities_state(
      course_code, ay, term, wydzial_code, kierunek_code, track_code, semester_code,
      activity_id, type, name_last,
      first_seen_at, last_seen_at,
      last_snapshot_at, last_event_at,
      visible_last,
      deleted_at,
      status_final, evidence_deleted, confidence_deleted,
      notes
    )
    select
      course_code, ay, term, wydzial_code, kierunek_code, track_code, semester_code,
      activity_id, type, name_last,
      first_seen_at, last_seen_at,
      last_snapshot_at, last_event_at,
      visible_last,
      deleted_at,
      status_final, evidence_deleted, confidence_deleted,
      notes
    from tmp_final2;
    """)

    return {"ok": True}

================================================================================
mrna_plum\activities\snapshots_load.py
================================================================================

from __future__ import annotations

import csv
import hashlib
import json
import re
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Iterable, Optional

import duckdb


def _sha1(text: str) -> str:
    return hashlib.sha1(text.encode("utf-8", errors="replace")).hexdigest()


def _captured_at_from_filename(path: Path) -> datetime:
    # np. 20260206-0826_2526z_zawartosc_kursow.csv
    m = re.search(r"(\d{8})-(\d{4})", path.name)
    if not m:
        return datetime.fromtimestamp(path.stat().st_mtime)
    ymd = m.group(1)  # YYYYMMDD
    hm = m.group(2)   # HHMM
    return datetime(
        int(ymd[0:4]), int(ymd[4:6]), int(ymd[6:8]),
        int(hm[0:2]), int(hm[2:4]),
    )


@dataclass(frozen=True)
class SnapshotRowPL:
    course_code: str
    activity_id: str
    name: str
    type: str
    visible_to_students: bool
    captured_at: datetime
    source_file: str
    row_key: str


def iter_snapshot_csv_plum_visible(path: Path) -> Iterable[SnapshotRowPL]:
    captured_at = _captured_at_from_filename(path)

    with path.open("r", encoding="utf-8-sig", newline="") as f:
        reader = csv.DictReader(f)

        req = {"Nazwa kursu", "ID aktywności", "Nazwa aktywności", "Format aktywności"}
        missing = req - set(reader.fieldnames or [])
        if missing:
            raise ValueError(f"Snapshot(PL) missing columns {sorted(missing)} in {path}")

        for r in reader:
            course_code = (r.get("Nazwa kursu") or "").strip()
            activity_id = (r.get("ID aktywności") or "").strip()
            if not course_code or not activity_id:
                continue

            name = (r.get("Nazwa aktywności") or "").strip()
            typ = (r.get("Format aktywności") or "").strip()

            payload = json.dumps(
                {
                    "course_code": course_code,
                    "activity_id": activity_id,
                    "name": name,
                    "type": typ,
                    "visible_to_students": True,
                    "captured_at": captured_at.isoformat(),
                },
                ensure_ascii=False,
                separators=(",", ":"),
            )
            row_key = _sha1(payload)

            yield SnapshotRowPL(
                course_code=course_code,
                activity_id=activity_id,
                name=name,
                type=typ,
                visible_to_students=True,
                captured_at=captured_at,
                source_file=str(path),
                row_key=row_key,
            )


def ensure_snapshot_table(con: duckdb.DuckDBPyConnection) -> None:
    con.execute("create schema if not exists raw;")
    con.execute(
        """
        create table if not exists raw.activities_snapshot (
          course_code varchar not null,
          activity_id varchar not null,
          name varchar,
          type varchar,
          visible_to_students boolean,
          captured_at timestamp not null,
          source_file varchar,
          row_key varchar not null,
          inserted_at timestamp default now()
        );
        """
    )
    con.execute(
        "create unique index if not exists ux_activities_snapshot_rowkey on raw.activities_snapshot(row_key);"
    )


def load_plum_snapshot_file_into_duckdb(
    con: duckdb.DuckDBPyConnection,
    snapshot_file: Path,
) -> dict:
    ensure_snapshot_table(con)

    inserted = 0
    scanned = 0
    max_captured_at: Optional[datetime] = None

    for row in iter_snapshot_csv_plum_visible(snapshot_file):
        scanned += 1
        con.execute(
            """
            insert into raw.activities_snapshot
            (course_code, activity_id, name, type, visible_to_students, captured_at, source_file, row_key)
            select ?,?,?,?,?,?,?,?
            where not exists (select 1 from raw.activities_snapshot where row_key = ?)
            """,
            [
                row.course_code,
                row.activity_id,
                row.name,
                row.type,
                row.visible_to_students,
                row.captured_at,
                row.source_file,
                row.row_key,
                row.row_key,
            ],
        )
        inserted += int(con.execute("select changes()").fetchone()[0])

        if max_captured_at is None or row.captured_at > max_captured_at:
            max_captured_at = row.captured_at

    return {
        "snapshot_file": str(snapshot_file),
        "scanned_rows": scanned,
        "inserted_rows": inserted,
        "captured_at": max_captured_at.isoformat() if max_captured_at else None,
    }

================================================================================
mrna_plum\cli.py
================================================================================

from __future__ import annotations
import sys
from pathlib import Path
import typer

from .paths import ProjectPaths
from .config import load_config
from .logging_run import setup_file_logger
from .ui_bridge import ProgressWriter
from .errors import ConfigError, InputDataError, MixedPeriodsError, ProcessingError

from .io.excel_keys import load_keys_sheet
from .rules.engine import compile_rules
from .merge import merge_logs_to_parquet
from .parse import parse_merged_parquet
from .store import DuckDbStore
from .stats import compute_stats
from .reports import export_excel_aggregates, export_individual_packages
from mrna_plum.store.duckdb_store import open_store
from mrna_plum.merge.merge_logs import merge_logs_into_duckdb
from mrna_plum.parse.parse_events import run_parse_events

from .activities.snapshots_load import load_snapshots_into_duckdb  # albo loader PL
from .activities.activities_state import build_activities_state, BuildConfig, DeletionConfig, MappingConfig, IncrementalConfig
from mrna_plum.activities.snapshots_load import load_plum_snapshot_file_into_duckdb

app = typer.Typer(add_completion=False)

# Exit codes
EC_OK = 0
EC_CONFIG = 2
EC_INPUT = 10
EC_MIXED = 20
EC_PROC = 30
EC_INTERNAL = 40

def _resolve_root(root: str) -> Path:
    p = Path(root).resolve()
    return p

def _resolve_config(root: Path, config: str | None) -> Path:
    if config:
        return Path(config).resolve()
    return (root / "config.yaml").resolve()

def _ensure_dirs(paths: ProjectPaths) -> None:
    paths.run_dir.mkdir(parents=True, exist_ok=True)
    paths.data_dir.mkdir(parents=True, exist_ok=True)
    paths.parquet_dir.mkdir(parents=True, exist_ok=True)

def _write_marker(paths: ProjectPaths, step: str) -> None:
    paths.marker_path(step).write_text("ok", encoding="utf-8")

def _collect_input_files(root: Path, input_glob: str) -> list[Path]:
    files = sorted([p for p in root.glob(input_glob) if p.is_file()])
    return files

def _main_guard(fn):
    def wrapper(*args, **kwargs):
        try:
            return fn(*args, **kwargs)
        except ConfigError as e:
            typer.echo(str(e), err=True)
            raise typer.Exit(code=EC_CONFIG)
        except InputDataError as e:
            typer.echo(str(e), err=True)
            raise typer.Exit(code=EC_INPUT)
        except MixedPeriodsError as e:
            typer.echo(str(e), err=True)
            raise typer.Exit(code=EC_MIXED)
        except ProcessingError as e:
            typer.echo(str(e), err=True)
            raise typer.Exit(code=EC_PROC)
        except Exception as e:
            typer.echo(f"Internal error: {e}", err=True)
            raise typer.Exit(code=EC_INTERNAL)
    return wrapper

@app.command("merge-logs")
@_main_guard
def cmd_merge_logs(
    root: str = typer.Option(..., "--root", help="Root folder passed from VBA (ThisWorkbook.Path)"),
    config: str | None = typer.Option(None, "--config", help="Config path; default {root}/config.yaml"),
):
    root_p = _resolve_root(root)
    cfg_p = _resolve_config(root_p, config)
    paths = ProjectPaths(root=root_p)
    _ensure_dirs(paths)

    logger = setup_file_logger(paths.run_dir / "run.log")
    progress = ProgressWriter(paths.run_dir / "progress.jsonl")

    cfg = load_config(cfg_p)

    input_files = _collect_input_files(root_p, cfg.input_glob)
    if not input_files:
        raise InputDataError(f"No input files found under {root_p} with glob {cfg.input_glob}")

    progress.emit("merge", "start", "Starting merge", current=0, total=len(input_files), extra={"root": str(root_p)})
    logger.info("[merge] files=%s", len(input_files))

    merged_parquet = paths.parquet_dir / "merged_raw.parquet"
    total_rows = merge_logs_to_parquet(input_files, merged_parquet, dedup_per_file=True)

    progress.emit("merge", "done", "Merge finished", current=len(input_files), total=len(input_files), extra={"rows": total_rows, "parquet": str(merged_parquet)})
    _write_marker(paths, "merge")
    logger.info("[merge] done rows=%s parquet=%s", total_rows, merged_parquet)


@app.command("build-db")
@_main_guard
def cmd_build_db(
    root: str = typer.Option(..., "--root"),
    config: str | None = typer.Option(None, "--config"),
):
    root_p = _resolve_root(root)
    cfg_p = _resolve_config(root_p, config)
    paths = ProjectPaths(root=root_p)
    _ensure_dirs(paths)

    logger = setup_file_logger(paths.run_dir / "run.log")
    progress = ProgressWriter(paths.run_dir / "progress.jsonl")

    cfg = load_config(cfg_p)

    # KEYS
    keys_wb = (root_p / cfg.keys_workbook).resolve()
    keys_df = load_keys_sheet(keys_wb, cfg.keys_sheet)
    rules = compile_rules(keys_df)

    merged_parquet = paths.parquet_dir / "merged_raw.parquet"
    if not merged_parquet.exists():
        raise InputDataError(f"Missing merged parquet. Run: mrna_plum merge-logs --root ...  Expected: {merged_parquet}")

    progress.emit("parse", "start", "Parsing merged logs & applying rules")
    logger.info("[parse] start rules=%s", len(rules))

    parsed_parquet = paths.parquet_dir / "parsed.parquet"
    n_rows, run_period = parse_merged_parquet(merged_parquet, parsed_parquet, cfg, rules)

    # DuckDB
    store = DuckDbStore(paths.duckdb_path)
    store.init_schema()

    progress.emit("db", "start", "Loading parsed parquet into DuckDB", extra={"db": str(paths.duckdb_path)})
    store.load_parquet_to_raw(parsed_parquet)

    progress.emit("db", "done", "DB built", extra={"rows": n_rows, "period": run_period})
    _write_marker(paths, "build_db")
    logger.info("[db] done rows=%s period=%s db=%s", n_rows, run_period, paths.duckdb_path)


@app.command("compute-stats")
@_main_guard
def cmd_compute_stats(
    root: str = typer.Option(..., "--root"),
    config: str | None = typer.Option(None, "--config"),
):
    root_p = _resolve_root(root)
    cfg_p = _resolve_config(root_p, config)
    paths = ProjectPaths(root=root_p)
    _ensure_dirs(paths)

    logger = setup_file_logger(paths.run_dir / "run.log")
    progress = ProgressWriter(paths.run_dir / "progress.jsonl")
    cfg = load_config(cfg_p)

    store = DuckDbStore(paths.duckdb_path)
    store.init_schema()

    # opcjonalnie: period z danych (weź pierwszy nie-null z raw_logs)
    with store.connect() as con:
        row = con.execute("SELECT period FROM raw_logs WHERE period IS NOT NULL LIMIT 1").fetchone()
        period = row[0] if row else None

    progress.emit("stats", "start", "Computing stats", extra={"period": period})
    compute_stats(store, period)
    progress.emit("stats", "done", "Stats computed", extra={"period": period})
    _write_marker(paths, "stats")
    logger.info("[stats] done period=%s", period)


@app.command("export-excel")
@_main_guard
def cmd_export_excel(
    root: str = typer.Option(..., "--root"),
    out: str | None = typer.Option(None, "--out", help="Output xlsx; default {root}/_out/aggregates.xlsx"),
    config: str | None = typer.Option(None, "--config"),
):
    root_p = _resolve_root(root)
    cfg_p = _resolve_config(root_p, config)
    paths = ProjectPaths(root=root_p)
    _ensure_dirs(paths)

    logger = setup_file_logger(paths.run_dir / "run.log")
    progress = ProgressWriter(paths.run_dir / "progress.jsonl")
    _ = load_config(cfg_p)

    store = DuckDbStore(paths.duckdb_path)

    out_xlsx = Path(out).resolve() if out else (root_p / "_out" / "aggregates.xlsx").resolve()
    progress.emit("export_excel", "start", "Exporting Excel aggregates", extra={"out": str(out_xlsx)})
    export_excel_aggregates(store, out_xlsx)
    progress.emit("export_excel", "done", "Excel exported", extra={"out": str(out_xlsx)})
    _write_marker(paths, "export_excel")
    logger.info("[export_excel] out=%s", out_xlsx)


@app.command("export-individual")
@_main_guard
def cmd_export_individual(
    root: str = typer.Option(..., "--root"),
    out_dir: str | None = typer.Option(None, "--out-dir", help="Output folder; default {root}/_out/individual"),
    config: str | None = typer.Option(None, "--config"),
):
    root_p = _resolve_root(root)
    cfg_p = _resolve_config(root_p, config)
    paths = ProjectPaths(root=root_p)
    _ensure_dirs(paths)

    logger = setup_file_logger(paths.run_dir / "run.log")
    progress = ProgressWriter(paths.run_dir / "progress.jsonl")
    _ = load_config(cfg_p)

    store = DuckDbStore(paths.duckdb_path)
    out_folder = Path(out_dir).resolve() if out_dir else (root_p / "_out" / "individual").resolve()

    progress.emit("export_individual", "start", "Exporting individual packages", extra={"out_dir": str(out_folder)})
    n = export_individual_packages(store, out_folder)
    progress.emit("export_individual", "done", "Individual packages exported", extra={"count": n, "out_dir": str(out_folder)})
    _write_marker(paths, "export_individual")
    logger.info("[export_individual] count=%s out_dir=%s", n, out_folder)

from mrna_plum.init_project import init_project

def cmd_init(args):
    created = init_project(args.root)
    print(f"Created {len(created)} folders")

@app.command("merge-logs")
def merge_logs_cmd(
    logs_root: Path = typer.Option(..., "--logs-root", exists=True, file_okay=False, dir_okay=True),
    db_path: Path = typer.Option(..., "--db-path", help="Ścieżka do DuckDB (np. E:/RNA/_db/mrna_plum.duckdb)"),
    export_mode: str = typer.Option(
        "duckdb",
        "--export-mode",
        help="duckdb (tylko zapis do DB) | parquet (eksport per-kurs) | csv (eksport per-kurs *_full_log.csv)",
    ),
    export_dir: Path | None = typer.Option(
        None,
        "--export-dir",
        help="Wymagane dla export-mode=csv/parquet. Folder wyjściowy per kurs.",
    ),
    chunk_size: int = typer.Option(2000, "--chunk-size", min=100, help="Batch insert do DuckDB"),
):
    export_mode = export_mode.lower().strip()
    if export_mode not in ("duckdb", "parquet", "csv"):
        raise typer.BadParameter("export-mode must be one of: duckdb, parquet, csv")

    if export_mode in ("csv", "parquet") and export_dir is None:
        raise typer.BadParameter("--export-dir is required for export-mode=csv/parquet")

    con = open_store(db_path)
    try:
        res = merge_logs_into_duckdb(
            root=logs_root,
            con=con,
            export_mode=export_mode,
            export_dir=export_dir,
            chunk_size=chunk_size,
        )
    finally:
        con.close()

    typer.echo(f"OK: courses={res.courses}, files={res.files}, inserted_rows={res.inserted_rows}")

@app.command("parse-events")
def parse_events_cmd(
    root: str,
    config: str,
    keys_xlsx: str = None,
):
    """
    Parse raw Moodle/PLUM CSV logs into canonical events table (DuckDB + optional Parquet).
    """
    from mrna_plum.config import load_config
    cfg = load_config(config)

    exit_code = run_parse_events(
        cfg,
        root=root,
        keys_xlsx_override=keys_xlsx,
    )
    raise SystemExit(exit_code)

@app.command("build-activities-state")
@_main_guard
def cmd_build_activities_state(
    root: str = typer.Option(..., "--root"),
    config: str | None = typer.Option(None, "--config"),
    snapshot_file: str = typer.Option(..., "--snapshot-file", help="CSV 'zawartość kursów' wybrany w VBA"),
):
    root_p = _resolve_root(root)
    cfg_p = _resolve_config(root_p, config)
    paths = ProjectPaths(root=root_p)
    _ensure_dirs(paths)

    logger = setup_file_logger(paths.run_dir / "run.log")
    progress = ProgressWriter(paths.run_dir / "progress.jsonl")

    cfg = load_config(cfg_p)

    snap_path = Path(snapshot_file).resolve()
    if not snap_path.exists():
        raise InputDataError(f"Snapshot file not found: {snap_path}")

    # DB
    store = DuckDbStore(paths.duckdb_path)
    store.init_schema()

    progress.emit("activities_state", "start", "Loading snapshots & building activities_state",
                  extra={"snapshot_file": str(snap_path), "db": str(paths.duckdb_path)})
    logger.info("[activities_state] start snapshot=%s", snap_path)

    with store.connect() as con:
        # 1) load snapshot do raw.activities_snapshot (idempotent)
        #    UWAGA: tu wywołaj loader dopasowany do formatu PL (Nazwa kursu/ID aktywności/...)
        load_stats = load_plum_snapshot_file_into_duckdb(con, snap_path)
        progress.emit("activities_state", "snapshots_loaded", "Snapshots loaded", extra=load_stats)

        # 2) build state (MERGE do mart.activities_state + QA + view)
        bcfg = cfg.build_activities_state  # zależy jak masz config model; jeśli to dict -> cfg["build_activities_state"]
        build_cfg = BuildConfig(
            deletion=DeletionConfig(
                delete_operations=["DELETE"],  # bo u Ciebie zawsze DELETE
                delete_tech_keys=[],
                delete_activity_labels_regex=[],
                disappearance_grace_period_days=int(bcfg.deletion.disappearance_grace_period_days),
                min_missing_snapshots_to_confirm=int(bcfg.deletion.min_missing_snapshots_to_confirm),
                deleted_at_policy="first_missing",
            ),
            mapping=MappingConfig(
                use_activity_id_map_table=True,
                allow_fuzzy_name_type_match=False,
            ),
            incremental=IncrementalConfig(
                checkpoint_table="raw.pipeline_checkpoints",
                checkpoint_key="build_activities_state",
                process_only_new_snapshots=True,
                process_only_new_events=True,
            ),
        )

        stats = build_activities_state(con, build_cfg)

    progress.emit("activities_state", "done", "Activities state built", extra=stats)
    _write_marker(paths, "activities_state")
    logger.info("[activities_state] done %s", stats)

import typer
from pathlib import Path
from mrna_plum.stats.compute_stats import compute_stats

app = typer.Typer()

@app.command("compute-stats")
def compute_stats_cmd(
    root: Path = typer.Option(..., "--root", exists=True, file_okay=False, dir_okay=True),
    ay: str | None = typer.Option(None, "--ay"),
    term: str | None = typer.Option(None, "--term"),
):
    compute_stats(root=root, ay=ay, term=term)

import typer
import duckdb
from pathlib import Path

from mrna_plum.reports.export_excel import export_summary_excel, EXIT_OVERFLOW, ExportOverflowError

app = typer.Typer()

@app.command("export-excel")
def export_excel_cmd(
    root: str = typer.Option(..., "--root", help="Root katalog pipeline (ma mieć _run i _out)"),
    db_path: str = typer.Option(None, "--db-path", help="Ścieżka do DuckDB; jeśli brak, weź z config"),
    config_path: str = typer.Option(None, "--config", help="Opcjonalnie: config.yaml"),
):
    # TODO: wpiąć w Wasz loader configa (tu minimalny szkic)
    cfg = {
        "root": root,
        "report": {"ay": "UNKNOWN_AY", "term": "UNKNOWN_TERM"},
        "export": {"max_rows_excel": 1_000_000, "overflow_strategy": "error"},
    }

    # jeśli masz loader yaml -> tutaj go użyj i nadpisz cfg

    # db path: preferuj config
    db = db_path or cfg.get("paths", {}).get("db_path")
    if not db:
        raise typer.BadParameter("Brak --db-path i brak paths.db_path w config")

    con = duckdb.connect(db)

    try:
        code, out_path = export_summary_excel(con, cfg)
        raise typer.Exit(code)
    except ExportOverflowError:
        raise typer.Exit(EXIT_OVERFLOW)

================================================================================
mrna_plum\config.py
================================================================================

from dataclasses import dataclass
from pathlib import Path
import yaml
from .errors import ConfigError

@dataclass(frozen=True)
class AppConfig:
    # gdzie szukać logów wejściowych (rekurencyjnie)
    input_glob: str = "**/*.csv"

    # excel z arkuszem KEYS (jeśli pusty -> będzie szukane w root)
    keys_workbook: str = "mRNA_PLUM_Form.xlsx"
    keys_sheet: str = "KEYS"

    # nazwy kolumn w CSV (jeśli różne — dostosuj w config.yaml)
    col_time: str = "Czas"
    col_context: str = "Kontekst zdarzenia"
    col_desc: str = "Opis"
    col_component: str = "Składnik"
    col_event_name: str = "Nazwa zdarzenia"

    # regex kursu/okresu z kontekstu (starter)
    # Kurs: WF/An/stj/5sem-Nazwa-2025/26z
    course_regex: str = r"Kurs:\s*([^\s]+)"
    period_regex: str = r"(\d{4}/\d{2}[zl])"   # np. 2025/26z lub 2025/26l

    # limity / performance
    chunk_rows: int = 200_000  # na przyszłość, gdy będziesz czytał w chunkach

def load_config(config_path: Path) -> AppConfig:
    if not config_path.exists():
        raise ConfigError(f"Config not found: {config_path}")

    data = yaml.safe_load(config_path.read_text(encoding="utf-8")) or {}
    # pozwalamy na brak kluczy (defaulty z dataclass)
    try:
        return AppConfig(**data)
    except TypeError as e:
        raise ConfigError(f"Invalid config schema: {e}") from e


================================================================================
mrna_plum\errors.py
================================================================================

class mRnaPlumError(Exception):
    """Base app error."""


class ConfigError(mRnaPlumError):
    pass


class InputDataError(mRnaPlumError):
    pass


class MixedPeriodsError(mRnaPlumError):
    pass


class ProcessingError(mRnaPlumError):
    pass


================================================================================
mrna_plum\import\__init__.py
================================================================================



================================================================================
mrna_plum\import\import_roster.py
================================================================================

from __future__ import annotations

import csv
import json
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

import duckdb

from mrna_plum.io.csv_read import detect_csv_dialect, iter_csv_rows_streaming  # masz już w projekcie


# ======================================================================================
# Helpers
# ======================================================================================

def _cfg_get(cfg: Any, path: str, default: Any = None) -> Any:
    cur = cfg
    for part in path.split("."):
        if cur is None:
            return default
        if isinstance(cur, dict):
            cur = cur.get(part, None)
        else:
            cur = getattr(cur, part, None)
    return default if cur is None else cur


def _now_ts() -> str:
    return time.strftime("%Y-%m-%d %H:%M:%S")


def _norm_int(x: Any) -> int:
    if x is None:
        return 0
    s = str(x).strip()
    if s == "":
        return 0
    # czasem CSV ma spacje albo "1 234"
    s = s.replace(" ", "").replace("\u00a0", "")
    try:
        return int(float(s))
    except Exception:
        return 0


def _pick(payload: Dict[str, str], *keys: str) -> str:
    for k in keys:
        if k in payload:
            return payload.get(k, "") or ""
    return ""


# ======================================================================================
# Schema
# ======================================================================================

def ensure_roster_tables(con: duckdb.DuckDBPyConnection) -> None:
    con.execute("CREATE SCHEMA IF NOT EXISTS stage;")
    con.execute("CREATE SCHEMA IF NOT EXISTS dim;")
    con.execute("CREATE SCHEMA IF NOT EXISTS mart;")

    con.execute(
        """
        CREATE TABLE IF NOT EXISTS stage.course_roster_raw (
            course_id                    VARCHAR,
            course_name                  VARCHAR,

            users_total                  BIGINT,
            students_total               BIGINT,
            teachers_total               BIGINT,
            teachers_no_edit             BIGINT,
            teachers_responsible         BIGINT,

            students_enrolled            BIGINT,
            students_completed           BIGINT,
            students_in_progress         BIGINT,
            students_before_start        BIGINT,

            source_file                  VARCHAR,
            loaded_at                    TIMESTAMP DEFAULT now(),
            row_key                      VARCHAR,
            payload_json                 VARCHAR
        );
        """
    )

    # 1 rekord per course_id (ostatni import wygrywa)
    con.execute(
        """
        CREATE TABLE IF NOT EXISTS dim.course_roster (
            course_id                    VARCHAR PRIMARY KEY,
            course_name                  VARCHAR,

            users_total                  BIGINT,
            students_total               BIGINT,
            teachers_total               BIGINT,
            teachers_no_edit             BIGINT,
            teachers_responsible         BIGINT,

            students_enrolled            BIGINT,
            students_completed           BIGINT,
            students_in_progress         BIGINT,
            students_before_start        BIGINT,

            source_file                  VARCHAR,
            loaded_at                    TIMESTAMP
        );
        """
    )

    con.execute(
        """
        CREATE TABLE IF NOT EXISTS mart.roster_import_qa (
            source_file      VARCHAR,
            status           VARCHAR,   -- OK / ERROR
            message          VARCHAR,
            rows_inserted    BIGINT,
            imported_at      TIMESTAMP DEFAULT now()
        );
        """
    )


def _row_key(course_id: str, course_name: str, students_enrolled: int, teachers_total: int) -> str:
    # proste i deterministyczne
    return f"{course_id}|{course_name}|{students_enrolled}|{teachers_total}"


# ======================================================================================
# Main import
# ======================================================================================

def import_course_roster_csv(
    con: duckdb.DuckDBPyConnection,
    roster_csv: Path,
) -> Tuple[int, int]:
    """
    Import CSV -> stage.course_roster_raw, then merge into dim.course_roster.
    Returns: (rows_raw_inserted, rows_dim_merged)
    """
    roster_csv = Path(roster_csv)
    if not roster_csv.exists():
        raise FileNotFoundError(roster_csv)

    ensure_roster_tables(con)

    dialect = detect_csv_dialect(roster_csv)
    rows_to_insert: List[Tuple] = []

    # oczekiwane polskie nagłówki
    for header, row in iter_csv_rows_streaming(roster_csv, dialect=dialect):
        payload = {header[i]: (row[i] if i < len(row) else "") for i in range(len(header))}

        course_id = _pick(payload, "ID kursu", "ID kursu ", "course_id").strip()
        course_name = _pick(payload, "Nazwa kursu", "course_name").strip()

        users_total = _norm_int(_pick(payload, "Użytkownicy"))
        students_total = _norm_int(_pick(payload, "Studenci"))
        teachers_total = _norm_int(_pick(payload, "Nauczyciele"))
        teachers_no_edit = _norm_int(_pick(payload, "Nauczyciele bez praw edycji"))
        teachers_responsible = _norm_int(_pick(payload, "Nauczyciele odpowiedzialny", "Nauczyciele odpowiedzialni"))

        students_enrolled = _norm_int(_pick(payload, "Studenci zapisani"))
        students_completed = _norm_int(_pick(payload, "Studenci po ukończeniu"))
        students_in_progress = _norm_int(_pick(payload, "Studenci w trakcie"))
        students_before_start = _norm_int(_pick(payload, "Studenci przed rozpocząciem", "Studenci przed rozpoczęciem"))

        # minimalny wymóg: course_id
        if course_id == "":
            continue

        rk = _row_key(course_id, course_name, students_enrolled, teachers_total)
        payload_json = json.dumps(payload, ensure_ascii=False, separators=(",", ":"))

        rows_to_insert.append(
            (
                course_id,
                course_name,
                users_total,
                students_total,
                teachers_total,
                teachers_no_edit,
                teachers_responsible,
                students_enrolled,
                students_completed,
                students_in_progress,
                students_before_start,
                str(roster_csv),
                rk,
                payload_json,
            )
        )

    # insert raw
    con.executemany(
        """
        INSERT INTO stage.course_roster_raw (
            course_id, course_name,
            users_total, students_total, teachers_total, teachers_no_edit, teachers_responsible,
            students_enrolled, students_completed, students_in_progress, students_before_start,
            source_file, row_key, payload_json
        )
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);
        """,
        rows_to_insert,
    )
    rows_raw = len(rows_to_insert)

    # merge -> dim (last loaded_at wins)
    # bierzemy ostatni rekord z raw per course_id wg loaded_at (i ewentualnie rowid)
    con.execute(
        """
        INSERT OR REPLACE INTO dim.course_roster
        SELECT
            course_id,
            any_value(course_name) AS course_name,

            any_value(users_total) AS users_total,
            any_value(students_total) AS students_total,
            any_value(teachers_total) AS teachers_total,
            any_value(teachers_no_edit) AS teachers_no_edit,
            any_value(teachers_responsible) AS teachers_responsible,

            any_value(students_enrolled) AS students_enrolled,
            any_value(students_completed) AS students_completed,
            any_value(students_in_progress) AS students_in_progress,
            any_value(students_before_start) AS students_before_start,

            any_value(source_file) AS source_file,
            max(loaded_at) AS loaded_at
        FROM stage.course_roster_raw
        WHERE source_file = ?
        GROUP BY course_id;
        """,
        [str(roster_csv)],
    )

    # ile w dim “dotknięto” – przybliżenie: distinct course_id z tego pliku
    rows_dim = con.execute(
        "SELECT COUNT(DISTINCT course_id) FROM stage.course_roster_raw WHERE source_file = ?;",
        [str(roster_csv)],
    ).fetchone()[0]

    con.execute(
        "INSERT INTO mart.roster_import_qa(source_file, status, message, rows_inserted) VALUES (?, 'OK', ?, ?);",
        [str(roster_csv), f"Imported roster: {rows_raw} raw rows, {rows_dim} courses", rows_raw],
    )

    return int(rows_raw), int(rows_dim)


def build_course_facts_views(con: duckdb.DuckDBPyConnection) -> None:
    """
    Creates/updates views:
      - mart.course_roster_mapped  (course_id -> course_key + roster counts)
      - mart.course_teachers_active (teachers_active per course_key)
      - mart.course_facts (course-level facts for reporting)
    """
    con.execute("CREATE SCHEMA IF NOT EXISTS mart;")
    con.execute("CREATE SCHEMA IF NOT EXISTS dim;")

    # IMPORTANT: musisz mieć gdzieś mapę course_id -> course_key.
    # Najczęściej w pipeline to jest dim.courses(course_id, course_key, course_name) albo podobnie.
    # Tu zakładamy istnienie: dim.courses(course_id, course_key, course_name).
    con.execute(
        """
        CREATE OR REPLACE VIEW mart.course_roster_mapped AS
        SELECT
            r.course_id,
            c.course_key,
            COALESCE(c.course_name, r.course_name) AS course_name,

            COALESCE(r.students_enrolled, 0) AS students_enrolled,
            COALESCE(r.teachers_total, 0) AS teachers_enrolled,

            COALESCE(r.users_total, 0) AS users_total,
            COALESCE(r.students_total, 0) AS students_total,
            COALESCE(r.teachers_no_edit, 0) AS teachers_no_edit,
            COALESCE(r.teachers_responsible, 0) AS teachers_responsible,

            COALESCE(r.students_completed, 0) AS students_completed,
            COALESCE(r.students_in_progress, 0) AS students_in_progress,
            COALESCE(r.students_before_start, 0) AS students_before_start
        FROM dim.course_roster r
        LEFT JOIN dim.courses c
          ON c.course_id::VARCHAR = r.course_id::VARCHAR;
        """
    )

    con.execute(
        """
        CREATE OR REPLACE VIEW mart.course_teachers_active AS
        SELECT
            course_key,
            COUNT(DISTINCT teacher_id) AS teachers_active
        FROM mart.metrics_long
        WHERE visible_active = TRUE
          AND count_value > 0
          AND course_key IS NOT NULL
          AND TRIM(course_key) <> ''
        GROUP BY course_key;
        """
    )

    con.execute(
        """
        CREATE OR REPLACE VIEW mart.course_facts AS
        SELECT
            rm.course_key,
            rm.course_id,
            rm.course_name,
            rm.students_enrolled,
            rm.teachers_enrolled,
            COALESCE(ta.teachers_active, 0) AS teachers_active
        FROM mart.course_roster_mapped rm
        LEFT JOIN mart.course_teachers_active ta
          ON ta.course_key = rm.course_key;
        """
    )

================================================================================
mrna_plum\init_project.py
================================================================================

from __future__ import annotations
from pathlib import Path

DEFAULT_DIRS = [
    "IN/logs",
    "IN/activities",
    "OUT/logs",
    "OUT/db",
    "OUT/merged",
    "OUT/excel",
    "OUT/individual",
    "OUT/pdf",
]

def init_project(root: str | Path) -> list[Path]:
    root = Path(root).resolve()
    created: list[Path] = []
    for rel in DEFAULT_DIRS:
        p = root / rel
        if not p.exists():
            p.mkdir(parents=True, exist_ok=True)
            created.append(p)
    return created


================================================================================
mrna_plum\io\__init__.py
================================================================================



================================================================================
mrna_plum\io\csv_read.py
================================================================================

from __future__ import annotations

import csv
import io
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable, Iterator, Optional, Tuple, List
import pandas as pd

def read_csv_safely(path: Path) -> pd.DataFrame:
    # logi mają polskie znaki → utf-8-sig / cp1250 bywa w praktyce
    # starter: próbujemy kilka kodowań
    encodings = ["utf-8-sig", "utf-8", "cp1250", "latin2"]
    last_err: Exception | None = None

    for enc in encodings:
        try:
            return pd.read_csv(path, encoding=enc, dtype=str, low_memory=False)
        except Exception as e:
            last_err = e

    raise last_err  # type: ignore

@dataclass(frozen=True)
class CsvDialectInfo:
    delimiter: str
    encoding: str  # "utf-8-sig" lub "cp1250"


_TIME_COL_CANDIDATES = ("Czas", "Time", "Date", "TimeCreated")


def detect_encoding(path: Path) -> str:
    raw = path.read_bytes()
    sample = raw[:32768]

    if sample.startswith(b"\xef\xbb\xbf"):
        return "utf-8-sig"

    try:
        sample.decode("utf-8", errors="strict")
        return "utf-8"
    except UnicodeDecodeError:
        pass

    # najczęstsze w PL logach
    try:
        sample.decode("cp1250", errors="strict")
        return "cp1250"
    except UnicodeDecodeError:
        return "iso-8859-2"


def detect_delimiter(sample_text: str) -> str:
    """
    Wymagania: wykrywaj delimiter (TAB / ; / ,)
    Prosta i stabilna heurystyka: policz wystąpienia w pierwszych liniach.
    """
    lines = [ln for ln in sample_text.splitlines() if ln.strip()]
    head = "\n".join(lines[:20]) if lines else sample_text

    candidates = ["\t", ";", ","]
    counts = {c: head.count(c) for c in candidates}
    # Jeśli wszystko 0 -> domyślnie ';' (częste w PL)
    best = max(counts, key=lambda k: counts[k])
    return best if counts[best] > 0 else ";"


def detect_csv_dialect(path: Path) -> CsvDialectInfo:
    enc = detect_encoding(path)
    # czytaj próbkę tekstu do wykrycia delimitera
    with path.open("r", encoding=enc, errors="replace", newline="") as f:
        sample = f.read(16384)
    delim = detect_delimiter(sample)
    # jeśli enc="utf-8" bez BOM, OK; jeśli BOM był, to utf-8-sig
    if enc == "utf-8" and path.read_bytes().startswith(b"\xef\xbb\xbf"):
        enc = "utf-8-sig"
    return CsvDialectInfo(delimiter=delim, encoding=enc)


def iter_csv_rows_streaming(
    path: Path,
    *,
    dialect: Optional[CsvDialectInfo] = None,
) -> Iterator[tuple[list[str], list[str]]]:
    d = dialect or detect_csv_dialect(path)

    # Kolejność prób: najpierw to co wykryte, potem sensowne fallbacki
    enc_try = [d.encoding]
    for e in ("utf-8-sig", "utf-8", "cp1250", "latin2"):
        if e not in enc_try:
            enc_try.append(e)

    last_err: Exception | None = None

    for enc in enc_try:
        try:
            with path.open("r", encoding=enc, errors="strict", newline="") as f:
                reader = csv.reader(f, delimiter=d.delimiter)
                header: Optional[list[str]] = None

                for row in reader:
                    row = [c.strip() for c in row]
                    if not any(row):
                        continue
                    if header is None:
                        header = row
                        continue
                    yield header, row
            return  # <- cały plik przeczytany OK w tym encodingu

        except UnicodeDecodeError as e:
            last_err = e
            continue

    # jeśli nic nie zadziałało
    raise last_err  # type: ignore

def pick_time_column_index(header: list[str]) -> Optional[int]:
    """
    Rozpoznaj: "Czas" (preferowane), ale też Time/Date/TimeCreated
    """
    lowered = [h.strip().lower() for h in header]
    for cand in _TIME_COL_CANDIDATES:
        c = cand.lower()
        if c in lowered:
            return lowered.index(c)
    return None

================================================================================
mrna_plum\io\excel_keys.py
================================================================================

from __future__ import annotations
from dataclasses import dataclass
from pathlib import Path
from typing import List
import pandas as pd

from openpyxl import load_workbook
from ..errors import InputDataError

REQUIRED_COLS = [
    "AKTYWNOSC",
    "KLUCZ_TECHNICZNY",
    "OPERACJA",
    "LICZYC_DO_RAPORTU",
    "REGEX_DOPASOWANIA_(Opis)",
    "REGEX_USER_ID_(Opis)",
    "REGEX_OBIEKT_ID_(z dopasowania)",
    "PRIORYTET",
]

def load_keys_sheet(workbook_path: Path, sheet_name: str) -> pd.DataFrame:
    if not workbook_path.exists():
        raise InputDataError(f"KEYS workbook not found: {workbook_path}")

    wb = load_workbook(workbook_path, read_only=True, data_only=True)
    if sheet_name not in wb.sheetnames:
        raise InputDataError(f"KEYS sheet not found: {sheet_name} in {workbook_path}")

    ws = wb[sheet_name]
    rows = list(ws.values)
    if not rows:
        raise InputDataError("KEYS sheet is empty")

    header = [str(x).strip() if x is not None else "" for x in rows[0]]
    data_rows = rows[1:]
    df = pd.DataFrame(data_rows, columns=header)

    missing = [c for c in REQUIRED_COLS if c not in df.columns]
    if missing:
        raise InputDataError(f"KEYS missing columns: {missing}")

    # normalizacja
    df = df.copy()
    df["PRIORYTET"] = pd.to_numeric(df["PRIORYTET"], errors="coerce").fillna(0).astype(int)
    df["LICZYC_DO_RAPORTU"] = df["LICZYC_DO_RAPORTU"].fillna("").astype(str).str.strip()

    # usuń puste reguły
    df = df[df["KLUCZ_TECHNICZNY"].notna() & (df["KLUCZ_TECHNICZNY"].astype(str).str.strip() != "")]
    return df.reset_index(drop=True)


================================================================================
mrna_plum\logging_run.py
================================================================================

from __future__ import annotations
import logging
from pathlib import Path

def setup_file_logger(log_path: Path) -> logging.Logger:
    log_path.parent.mkdir(parents=True, exist_ok=True)

    logger = logging.getLogger("mrna_plum")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()

    fh = logging.FileHandler(log_path, encoding="utf-8")
    fmt = logging.Formatter("%(asctime)s %(levelname)s %(message)s")
    fh.setFormatter(fmt)
    logger.addHandler(fh)

    # opcjonalnie: stdout (przy CLI wygodne)
    sh = logging.StreamHandler()
    sh.setFormatter(fmt)
    logger.addHandler(sh)

    return logger


================================================================================
mrna_plum\merge\__init__.py
================================================================================

from mrna_plum.store.duckdb_store import open_store
from .merge_logs import merge_logs_into_duckdb

__all__ = [
    "open_store",
    "merge_logs_into_duckdb",
]

================================================================================
mrna_plum\merge\merge_logs.py
================================================================================

from __future__ import annotations

import json
import re
import hashlib
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Dict, Iterator, List, Optional

import pyarrow as pa
import pyarrow.parquet as pq

from mrna_plum.io.csv_read import detect_csv_dialect, iter_csv_rows_streaming, pick_time_column_index
from mrna_plum.store.duckdb_store import (
    EventRawRow,
    create_stage_table,
    insert_stage_rows,
    merge_stage_into_events_raw,
    export_course_to_csv,
    export_course_to_parquet,
)

_LOG_NAME_RE = re.compile(r"^logs_(?P<course>.+?)_(?P<ts>\d{8}-\d{4})\.csv$", re.IGNORECASE)

_TIME_FORMATS = (
    # ISO / quasi-ISO
    "%Y-%m-%d %H:%M:%S",
    "%Y-%m-%d %H:%M",
    "%Y-%m-%dT%H:%M:%S",
    "%Y-%m-%dT%H:%M:%S.%f",
    "%Y-%m-%dT%H:%M",
    # PL
    "%d.%m.%Y %H:%M:%S",
    "%d.%m.%Y %H:%M",
)


def merge_logs_to_parquet(
    input_files: list[Path],
    parquet_out: Path,
    *,
    dedup_per_file: bool = True,
    row_group_size: int = 50_000,
) -> int:
    """
    Streamingowy zapis do Parquet:
    - nie używa pandas
    - nie trzyma wszystkich danych w RAM
    - dedup per plik (opcjonalnie)
    """
    parquet_out.parent.mkdir(parents=True, exist_ok=True)

    writer: Optional[pq.ParquetWriter] = None
    total_rows = 0

    try:
        for fp in input_files:
            dialect = detect_csv_dialect(fp)

            # dedup tylko w obrębie tego pliku (set resetowany per plik)
            seen: set[str] = set() if dedup_per_file else set()

            header_current: Optional[list[str]] = None
            batch_cols: dict[str, list[str]] = {}
            batch_cols["_source_file"] = []

            for header, row in iter_csv_rows_streaming(fp, dialect=dialect):
                if header_current is None:
                    header_current = header
                    for h in header_current:
                        batch_cols.setdefault(h, [])

                if dedup_per_file:
                    key = "\x1f".join(row)
                    if key in seen:
                        continue
                    seen.add(key)

                for i, h in enumerate(header):
                    batch_cols[h].append(row[i] if i < len(row) else "")
                batch_cols["_source_file"].append(str(fp))

                if len(batch_cols["_source_file"]) >= row_group_size:
                    table = pa.table(batch_cols)
                    if writer is None:
                        writer = pq.ParquetWriter(parquet_out, table.schema)
                    writer.write_table(table)
                    total_rows += table.num_rows
                    batch_cols = {k: [] for k in table.schema.names}

            if header_current is not None and len(batch_cols["_source_file"]) > 0:
                table = pa.table(batch_cols)
                if writer is None:
                    writer = pq.ParquetWriter(parquet_out, table.schema)
                writer.write_table(table)
                total_rows += table.num_rows

        if writer is None:
            empty = pa.table({})
            pq.write_table(empty, parquet_out)

        return total_rows

    finally:
        if writer is not None:
            writer.close()


def _parse_time_to_iso(value: str) -> Optional[str]:
    v = value.strip()
    if not v:
        return None

    try:
        dt = datetime.fromisoformat(v.replace("Z", "+00:00"))
        return dt.isoformat()
    except Exception:
        pass

    for fmt in _TIME_FORMATS:
        try:
            dt = datetime.strptime(v, fmt)
            return dt.isoformat(sep="T")
        except Exception:
            continue
    return None


def _normalize_fields_key(fields: List[str]) -> str:
    """
    Dedup: CAŁY wiersz identyczny po Trim (reader trimuje).
    Stabilny hash niezależny od delimitera.
    """
    joined = "\x1f".join(fields)
    return hashlib.sha256(joined.encode("utf-8")).hexdigest()


def iter_log_files(root: Path) -> Iterator[Path]:
    for p in root.rglob("*.csv"):
        if p.is_file():
            yield p


def group_logs_by_course(root: Path) -> Dict[str, List[Path]]:
    grouped: Dict[str, List[Path]] = {}
    for p in iter_log_files(root):
        m = _LOG_NAME_RE.match(p.name)
        if not m:
            continue
        course = m.group("course")
        grouped.setdefault(course, []).append(p)

    # stabilne sortowanie po nazwie (timestamp jest w nazwie)
    for course in list(grouped.keys()):
        grouped[course].sort(key=lambda x: x.name)
    return grouped


@dataclass(frozen=True)
class MergeLogsResult:
    courses: int
    files: int
    inserted_rows: int


def merge_logs_into_duckdb(
    *,
    root: Path,
    con,  # duckdb connection
    export_mode: str = "duckdb",  # "duckdb" | "parquet" | "csv"
    export_dir: Optional[Path] = None,
    chunk_size: int = 2000,
) -> MergeLogsResult:
    """
    Publiczny entrypoint zgodny z testami/CLI.

    - root: folder z logami logs_<KURS>_<YYYYMMDD-HHMM>.csv (rekurencyjnie)
    - export_mode: "duckdb" (domyślnie) lub "csv"/"parquet"
    - export_dir: wymagany dla "csv"/"parquet"
    - chunk_size: batch insert do stage
    """
    export_mode = (export_mode or "duckdb").lower().strip()
    if export_mode not in ("duckdb", "csv", "parquet"):
        raise ValueError(f"export_mode must be 'duckdb'|'csv'|'parquet', got: {export_mode}")

    if export_mode in ("csv", "parquet") and export_dir is None:
        raise ValueError("export_dir is required for export_mode=csv/parquet")

    return _merge_logs_into_duckdb_impl(
        root=root,
        con=con,
        export_mode=export_mode,
        export_dir=export_dir,
        chunk_size=chunk_size,
    )

_COURSE_ID_PATTERNS = [
    # najczęstsze w Moodle logs (EN)
    re.compile(r"course with id\s+'(\d+)'", re.IGNORECASE),
    re.compile(r"course with id\s+(\d+)", re.IGNORECASE),

    # czasem bez apostrofów
    re.compile(r"\bcourse id\b\s*[:=]?\s*(\d+)", re.IGNORECASE),

    # PL warianty (na wszelki wypadek)
    re.compile(r"\bid kursu\b\s*[:=]?\s*(\d+)", re.IGNORECASE),
]


def _extract_course_id_from_payload(payload: Dict[str, str]) -> Optional[int]:
    """
    Szukamy course_id głównie w polu 'Opis' (czasem 'Description'),
    ale dla bezpieczeństwa przeszukujemy też cały payload.
    """
    candidates: List[str] = []

    # preferowane pola
    for k in ("Opis", "Description", "Event description", "Nazwa zdarzenia", "Kontekst zdarzenia"):
        v = payload.get(k)
        if v:
            candidates.append(v)

    # fallback: cały payload (join wartości)
    if not candidates:
        candidates.append(" | ".join([v for v in payload.values() if v]))

    text = " \n ".join(candidates)

    for rx in _COURSE_ID_PATTERNS:
        m = rx.search(text)
        if m:
            try:
                return int(m.group(1))
            except Exception:
                return None
    return None

def _merge_logs_into_duckdb_impl(
    *,
    root: Path,
    con,
    export_mode: str,
    export_dir: Optional[Path],
    chunk_size: int,
) -> MergeLogsResult:
    """
    Rdzeń logiki scalania.
    """
    try:
        grouped = group_logs_by_course(root)
        total_files = sum(len(v) for v in grouped.values())
        total_inserted = 0

        for course, files in grouped.items():
            create_stage_table(con)
            buf: List[EventRawRow] = []

            for fpath in files:
                dialect = detect_csv_dialect(fpath)

                time_idx: Optional[int] = None
                header_ref: Optional[List[str]] = None

                for header, row in iter_csv_rows_streaming(fpath, dialect=dialect):
                    if header_ref is None:
                        header_ref = header
                        time_idx = pick_time_column_index(header_ref)

                    payload = {header[i]: (row[i] if i < len(row) else "") for i in range(len(header))}
                    payload_json = json.dumps(payload, ensure_ascii=False, separators=(",", ":"))
                    course_id = _extract_course_id_from_payload(payload)

                    time_text = None
                    time_iso = None
                    if time_idx is not None and time_idx < len(row):
                        time_text = row[time_idx]
                        time_iso = _parse_time_to_iso(time_text)

                    row_key = _normalize_fields_key(row)

                    buf.append(
                        EventRawRow(
                            course=course,
                            course_id=course_id,
                            time_text=time_text,
                            time_ts_iso=time_iso,
                            row_key=row_key,
                            payload_json=payload_json,
                            source_file=str(fpath),
                        )
                    )

                    if len(buf) >= chunk_size:
                        insert_stage_rows(con, buf)
                        buf.clear()

            if buf:
                insert_stage_rows(con, buf)
                buf.clear()

            inserted = merge_stage_into_events_raw(con)
            total_inserted += inserted

            if export_mode in ("csv", "parquet"):
                assert export_dir is not None
                export_dir.mkdir(parents=True, exist_ok=True)

                if export_mode == "csv":
                    out_csv = export_dir / f"{course}_full_log.csv"
                    export_course_to_csv(con, course=course, out_csv=out_csv)
                else:
                    out_pq = export_dir / f"{course}_full_log.parquet"
                    export_course_to_parquet(con, course=course, out_parquet=out_pq)

        return MergeLogsResult(courses=len(grouped), files=total_files, inserted_rows=total_inserted)

    except UnicodeDecodeError as e:
        # Bezpieczny komunikat (bez ryzyka UnboundLocalError)
        raise RuntimeError(f"UnicodeDecodeError while reading CSV under root={root}") from e


__all__ = ["merge_logs_into_duckdb", "MergeLogsResult", "merge_logs_to_parquet"]

================================================================================
mrna_plum\merge\test_merge_logs.py
================================================================================

from __future__ import annotations

import csv
from pathlib import Path

import duckdb
import pytest

from mrna_plum.store.duckdb_store import open_store
from mrna_plum.merge.merge_logs import merge_logs_to_parquet
from mrna_plum.merge.merge_logs import merge_logs_into_duckdb, MergeLogsResult


def _write_csv_bytes(path: Path, data: bytes) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_bytes(data)


def _make_logs_name(course: str, ts: str) -> str:
    return f"logs_{course}_{ts}.csv"


def test_bom_utf8_and_delimiter_detection_and_insert(tmp_path: Path):
    root = tmp_path / "logs"
    db = tmp_path / "db.duckdb"

    # UTF-8 with BOM, delimiter ';'
    content = (
        "\ufeffCzas;Akcja;Opis\n"
        "2026-02-18 10:00:00;ADD;Zażółć gęślą\n"
    ).encode("utf-8")

    _write_csv_bytes(root / _make_logs_name("KURS1", "20260218-1000"), content)

    con = open_store(db)
    try:
        res = merge_logs_into_duckdb(root=root, con=con, export_mode="duckdb")
        assert res.courses == 1
        assert res.files == 1
        assert res.inserted_rows == 1

        row = con.execute(
            "SELECT course, payload_json FROM events_raw WHERE course='KURS1'"
        ).fetchone()
        assert row[0] == "KURS1"
        assert "Zażółć gęślą" in row[1]
    finally:
        con.close()


def test_windows_1250_polish_chars(tmp_path: Path):
    root = tmp_path / "logs"
    db = tmp_path / "db.duckdb"

    # cp1250, delimiter TAB
    text = "Czas\tAkcja\tOpis\n2026-02-18 10:00:00\tADD\tŚliwka w kompot\n"
    _write_csv_bytes(root / _make_logs_name("KURS2", "20260218-1000"), text.encode("cp1250"))

    con = open_store(db)
    try:
        res = merge_logs_into_duckdb(root=root, con=con, export_mode="duckdb")
        assert res.inserted_rows == 1

        payload = con.execute(
            "SELECT payload_json FROM events_raw WHERE course='KURS2'"
        ).fetchone()[0]
        assert "Śliwka w kompot" in payload
    finally:
        con.close()


def test_dedup_identical_whole_row_after_trim(tmp_path: Path):
    root = tmp_path / "logs"
    db = tmp_path / "db.duckdb"

    # dwa pliki, ten sam wiersz różniący się tylko spacingiem -> po trim ma być 1 wpis
    c1 = (
        "Czas;Akcja;Opis\n"
        "2026-02-18 10:00:00;ADD;  Duplikat  \n"
    ).encode("utf-8")
    c2 = (
        "Czas;Akcja;Opis\n"
        "2026-02-18 10:00:00;ADD;Duplikat\n"
    ).encode("utf-8")

    _write_csv_bytes(root / _make_logs_name("KURS3", "20260218-1000"), c1)
    _write_csv_bytes(root / _make_logs_name("KURS3", "20260218-1001"), c2)

    con = open_store(db)
    try:
        res = merge_logs_into_duckdb(root=root, con=con, export_mode="duckdb")
        assert res.inserted_rows == 1

        cnt = con.execute("SELECT COUNT(*) FROM events_raw WHERE course='KURS3'").fetchone()[0]
        assert cnt == 1
    finally:
        con.close()


def test_sorting_desc_by_time_on_export_csv(tmp_path: Path):
    root = tmp_path / "logs"
    db = tmp_path / "db.duckdb"
    out_dir = tmp_path / "out"

    # kolejność w pliku: starszy potem nowszy (celowo)
    content = (
        "Czas;Akcja;Opis\n"
        "2026-02-18 09:00:00;ADD;A\n"
        "2026-02-18 10:00:00;ADD;B\n"
    ).encode("utf-8")

    _write_csv_bytes(root / _make_logs_name("KURS4", "20260218-1000"), content)

    con = open_store(db)
    try:
        res = merge_logs_into_duckdb(root=root, con=con, export_mode="csv", export_dir=out_dir)
        assert res.inserted_rows == 2

        out_csv = out_dir / "KURS4_full_log.csv"
        assert out_csv.exists()

        # W eksporcie ma być B przed A (czas malejąco)
        rows = out_csv.read_text(encoding="utf-8").splitlines()
        # Header w COPY jest: course;time_text;time_ts;payload_json;source_file
        assert len(rows) >= 3
        assert '"B"' in rows[1] or "B" in rows[1]
        assert '"A"' in rows[2] or "A" in rows[2]
    finally:
        con.close()


================================================================================
mrna_plum\parse\__init__.py
================================================================================

from .parse_logs import parse_merged_parquet
__all__ = ["parse_merged_parquet"]


================================================================================
mrna_plum\parse\context.py
================================================================================

from __future__ import annotations
import re
from dataclasses import dataclass
from typing import Optional

@dataclass(frozen=True)
class ContextInfo:
    course_code: Optional[str]
    period: Optional[str]

def parse_context(context: str, course_regex: str, period_regex: str) -> ContextInfo:
    course = None
    period = None

    cm = re.search(course_regex, context or "")
    if cm:
        course = cm.group(1)

    pm = re.search(period_regex, context or "")
    if pm:
        period = pm.group(1)

    return ContextInfo(course_code=course, period=period)


================================================================================
mrna_plum\parse\parse_events.py
================================================================================

# src/mrna_plum/parse/parse_events.py

from __future__ import annotations

import json
import os
import re
import sys
from dataclasses import asdict
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

import pandas as pd

from mrna_plum.rules.activity_rules import ActivityRuleEngine, load_keys_rules
from mrna_plum.store.database import EventStore


COURSE_CTX_RX = re.compile(
    r"Kurs:\s*(?P<course_code>[A-Z]{1,3}/[A-Za-z]{1,6}/[A-Za-z0-9_]+/(?P<semester>\d+sem)-(?P<name>.+?)-(?P<ay>\d{4}/\d{2})(?P<term>[zl])",
    re.IGNORECASE,
)

# fallback, jeśli czas nieparsowalny
def parse_ts_to_utc(s: str) -> Optional[datetime]:
    if not s:
        return None
    s = str(s).strip()
    if not s:
        return None
    # epoch?
    if s.isdigit():
        try:
            x = int(s)
            if x > 10_000_000_000:
                x //= 1000
            return datetime.fromtimestamp(x, tz=timezone.utc)
        except Exception:
            return None
    try:
        ts = pd.to_datetime(s, utc=True, errors="coerce")
        if pd.isna(ts):
            return None
        return ts.to_pydatetime()
    except Exception:
        return None


def parse_course_context(kontekst: str) -> Optional[Dict[str, str]]:
    if not kontekst:
        return None
    m = COURSE_CTX_RX.search(kontekst)
    if not m:
        return None
    gd = m.groupdict()
    course_code = gd["course_code"]
    parts = course_code.split("/")
    # WF/An/stj/5sem-...
    wydzial = parts[0] if len(parts) > 0 else ""
    kierunek = parts[1] if len(parts) > 1 else ""
    track = parts[2] if len(parts) > 2 else ""

    return {
        "course_code": course_code,
        "wydzial_code": wydzial,
        "kierunek_code": kierunek,
        "track_code": track,
        "semester_code": gd.get("semester", ""),
        "course_name": gd.get("name", "").strip(),
        "ay": gd.get("ay", ""),
        "term": gd.get("term", "").lower(),
    }


def ensure_run_paths(root: Path) -> Dict[str, Path]:
    run_dir = root / "_run"
    run_dir.mkdir(parents=True, exist_ok=True)
    return {
        "run_dir": run_dir,
        "run_log": run_dir / "run.log",
        "progress": run_dir / "progress.jsonl",
        "ok": run_dir / "parse.ok",
    }


class RunLogger:
    def __init__(self, run_log: Path, progress: Path):
        self.run_log = run_log
        self.progress = progress

    def log(self, msg: str) -> None:
        line = f"{datetime.now().isoformat(timespec='seconds')} {msg}\n"
        self.run_log.open("a", encoding="utf-8").write(line)

    def progress_event(self, obj: Dict[str, Any]) -> None:
        self.progress.open("a", encoding="utf-8").write(json.dumps(obj, ensure_ascii=False) + "\n")


def _cfg_filters(cfg: Dict[str, Any]) -> Dict[str, Any]:
    f = cfg.get("filters", {}) or {}
    parse_cfg = cfg.get("parse_events", {}) or {}
    # dopuszczamy override w parse_events.*
    return {**f, **(parse_cfg.get("filters") or {})}


def _is_student_email(payload: Dict[str, Any], student_domain: str) -> bool:
    # payload może mieć różne kolumny; przeszukaj wszystkie wartości z '@'
    domain = student_domain.lower().strip()
    if not domain:
        return False
    blob = " ".join([str(v) for v in payload.values() if v is not None])
    return domain in blob.lower()


def _source_allowed(payload: Dict[str, Any], allowed_sources: List[str], blocked_sources: List[str]) -> bool:
    src = str(payload.get("Źródło") or payload.get("Zrodlo") or payload.get("Source") or "").strip()
    if blocked_sources and any(b.lower() in src.lower() for b in blocked_sources):
        return False
    if allowed_sources:
        return any(a.lower() in src.lower() for a in allowed_sources)
    return True


def _techkey_allowed(tech_key: str, wl: List[str], bl: List[str]) -> bool:
    if bl and tech_key in bl:
        return False
    if wl and tech_key not in wl:
        return False
    return True


def _date_allowed(ts: datetime, date_from: Optional[str], date_to: Optional[str]) -> bool:
    if date_from:
        d1 = pd.to_datetime(date_from, utc=True, errors="coerce")
        if not pd.isna(d1) and ts < d1.to_pydatetime():
            return False
    if date_to:
        d2 = pd.to_datetime(date_to, utc=True, errors="coerce")
        if not pd.isna(d2) and ts > d2.to_pydatetime():
            return False
    return True


def run_parse_events(
    cfg: Dict[str, Any],
    root: str,
    keys_xlsx_override: Optional[str] = None,
) -> int:
    """
    Exit codes:
      0 = OK
      2 = mixed periods
      1 = other error
    """
    root_p = Path(root)
    paths = ensure_run_paths(root_p)
    logger = RunLogger(paths["run_log"], paths["progress"])

    try:
        logger.log("[PARSE] start parse-events")
        store = EventStore(cfg)
        store.ensure_schema()

        # KEYS
        keys_cfg = cfg.get("parse_events", {}) or {}

        # 1️ priorytet: CLI override
        keys_xlsx = keys_xlsx_override

        # 2️ fallback: config.yaml
        if not keys_xlsx:
            keys_xlsx = keys_cfg.get("keys_xlsx")

        if not keys_xlsx:
            raise ValueError(
                "Brak KEYS: podaj --keys-xlsx lub ustaw parse_events.keys_xlsx w config.yaml"
            )

        # jeśli w config używasz {root}
        if "{root}" in keys_xlsx:
            keys_xlsx = keys_xlsx.replace("{root}", root)

        keys_sheet = keys_cfg.get("keys_sheet", "KEYS")
        rules = load_keys_rules(keys_xlsx, sheet_name=keys_sheet)
        engine = ActivityRuleEngine(rules, drop_mode_nie=True)
        logger.log(f"[PARSE] loaded KEYS rules: {len(rules)}")

        # filtry
        fil = _cfg_filters(cfg)
        student_domain = (fil.get("student_email_domain") or "@student.umw.edu.pl").lower()
        tech_wl = fil.get("tech_key_whitelist") or []
        tech_bl = fil.get("tech_key_blacklist") or []
        allowed_sources = fil.get("source_whitelist") or []
        blocked_sources = fil.get("source_blacklist") or []
        date_from = fil.get("date_from")
        date_to = fil.get("date_to")

        # DuckDB streaming z events_raw
        import duckdb
        con = duckdb.connect(str(Path(cfg["paths"]["db_path"])))
        con.execute("PRAGMA enable_progress_bar=false;")

        # incremental: bierz tylko te, których row_key nie ma w canonical_raw
        query = """
            SELECT course, time_text, time_ts_iso, row_key, payload_json, source_file
            FROM events_raw r
            WHERE NOT EXISTS (SELECT 1 FROM events_canonical_raw c WHERE c.row_key = r.row_key)
        """
        cur = con.execute(query)

        batch: List[Dict[str, Any]] = []
        conf_batch: List[Dict[str, Any]] = []

        seen_period: Optional[Tuple[str, str]] = None  # (ay, term)
        mixed_period = False

        total_read = 0
        total_matched = 0
        total_inserted = 0

        FETCH = int(keys_cfg.get("fetch_size", 5000))
        INSERT_BATCH = int(keys_cfg.get("insert_batch_size", 20000))

        while True:
            rows = cur.fetchmany(FETCH)
            if not rows:
                break

            for course, time_text, time_ts_iso, row_key, payload_json, source_file in rows:
                total_read += 1

                # payload
                try:
                    payload = json.loads(payload_json)
                except Exception:
                    continue

                # źródło filter
                if not _source_allowed(payload, allowed_sources, blocked_sources):
                    continue

                # student filter
                if student_domain and _is_student_email(payload, student_domain):
                    continue

                # czas
                ts = None
                if time_ts_iso:
                    try:
                        ts = pd.to_datetime(time_ts_iso, utc=True, errors="coerce")
                        ts = None if pd.isna(ts) else ts.to_pydatetime()
                    except Exception:
                        ts = None
                if ts is None:
                    ts = parse_ts_to_utc(str(payload.get("Czas") or payload.get("Time") or payload.get("Date") or time_text or ""))
                if ts is None:
                    continue

                if not _date_allowed(ts, date_from, date_to):
                    continue

                # kontekst kursu
                kontekst = str(payload.get("Kontekst zdarzenia") or payload.get("Event context") or "")
                ctx = parse_course_context(kontekst)
                if not ctx:
                    continue

                # okres: ay+term
                period = (ctx["ay"], ctx["term"])
                if seen_period is None:
                    seen_period = period
                elif period != seen_period:
                    mixed_period = True
                    # logujemy i kończymy po batchu
                    logger.log(f"[PARSE][ERR] mixed periods: first={seen_period} next={period} row_key={row_key}")
                    break

                # opis (KEYS dopasowanie)
                opis = str(payload.get("Opis") or payload.get("Description") or "")
                m = engine.match(opis)
                if not m:
                    continue  # aktywności spoza KEYS → pomijamy

                # whitelist/blacklist tech_key
                if not _techkey_allowed(m.tech_key, tech_wl, tech_bl):
                    continue

                total_matched += 1

                if m.conflict:
                    conf_batch.append(
                        {
                            "row_key": row_key,
                            "course_code": ctx["course_code"],
                            "teacher_id": m.teacher_id,
                            "tech_key": m.tech_key,
                            "operation": m.operation,
                            "object_id": m.object_id,
                            "note": f"KEYS conflict: multiple matches with same priority={m.priority}",
                        }
                    )

                row = {
                    "row_key": row_key,
                    "course": course,
                    "course_code": ctx["course_code"],
                    "wydzial_code": ctx["wydzial_code"],
                    "kierunek_code": ctx["kierunek_code"],
                    "track_code": ctx["track_code"],
                    "semester_code": ctx["semester_code"],
                    "course_name": ctx["course_name"],
                    "ay": ctx["ay"],
                    "term": ctx["term"],
                    "ts_utc": ts,
                    "teacher_id": m.teacher_id,
                    "operation": m.operation,
                    "tech_key": m.tech_key,
                    "activity_label": m.activity_label,
                    "object_id": m.object_id,
                    "count_mode": m.count_mode,
                    "raw_line_hash": row_key,  # row_key już jest hashem całego wiersza po trim
                    "source_file": source_file,
                    "payload_json": payload_json,
                }
                batch.append(row)

                if len(batch) >= INSERT_BATCH:
                    total_inserted += store.insert_raw_batch(batch)
                    batch.clear()
                if len(conf_batch) >= 2000:
                    store.insert_conflicts_batch(conf_batch)
                    conf_batch.clear()

            if mixed_period:
                break

            # progress co chunk
            if total_read % (FETCH * 2) == 0:
                logger.progress_event(
                    {
                        "stage": "parse-events",
                        "read": total_read,
                        "matched": total_matched,
                        "inserted_raw": total_inserted,
                        "period": {"ay": seen_period[0], "term": seen_period[1]} if seen_period else None,
                    }
                )

        if batch:
            total_inserted += store.insert_raw_batch(batch)
            batch.clear()
        if conf_batch:
            store.insert_conflicts_batch(conf_batch)
            conf_batch.clear()

        con.close()

        if mixed_period:
            logger.log("[PARSE][ERR] mixed periods -> abort")
            return 2

        # finalize counted/unieważnienia
        store.finalize_canonical()
        pq = store.export_parquet() if (cfg.get("parse_events", {}) or {}).get("export_parquet", False) else None

        paths["ok"].write_text("OK\n", encoding="utf-8")
        logger.log(f"[PARSE] OK read={total_read} matched={total_matched} inserted_raw={total_inserted} parquet={pq}")
        logger.progress_event(
            {
                "stage": "parse-events",
                "status": "ok",
                "read": total_read,
                "matched": total_matched,
                "inserted_raw": total_inserted,
                "period": {"ay": seen_period[0], "term": seen_period[1]} if seen_period else None,
            }
        )
        return 0

    except Exception as e:
        logger.log(f"[PARSE][ERR] {type(e).__name__}: {e}")
        return 1

================================================================================
mrna_plum\parse\parse_logs.py
================================================================================

from __future__ import annotations
from pathlib import Path
import pandas as pd

from ..config import AppConfig
from ..errors import MixedPeriodsError, ProcessingError
from ..rules.engine import match_best_rule
from .context import parse_context

def parse_merged_parquet(
    parquet_in: Path,
    parquet_out: Path,
    config: AppConfig,
    rules: list,
) -> tuple[int, str | None]:
    df = pd.read_parquet(parquet_in)
    if df.empty:
        parquet_out.parent.mkdir(parents=True, exist_ok=True)
        df.to_parquet(parquet_out, index=False)
        return 0, None

    # wymagane kolumny
    need = [config.col_time, config.col_context, config.col_desc, config.col_component, config.col_event_name]
    missing = [c for c in need if c not in df.columns]
    if missing:
        raise ProcessingError(f"Missing required CSV columns: {missing}")

    # wyciągnij kontekst (kurs, okres)
    ctx = df[config.col_context].astype(str).apply(lambda x: parse_context(x, config.course_regex, config.period_regex))
    df["course_code"] = ctx.apply(lambda c: c.course_code)
    df["period"] = ctx.apply(lambda c: c.period)

    # mixed periods check (ignorujemy None)
    periods = sorted({p for p in df["period"].dropna().unique().tolist() if str(p).strip() != ""})
    if len(periods) > 1:
        raise MixedPeriodsError(f"mixed periods detected: {periods}")
    run_period = periods[0] if periods else None

    # dopasuj regułę po opisie
    tech_keys = []
    activities = []
    operations = []
    count_flags = []
    teacher_ids = []
    object_ids = []
    matched_prio = []

    for desc in df[config.col_desc].astype(str).tolist():
        m = match_best_rule(desc, rules)
        if m is None:
            tech_keys.append(None)
            activities.append(None)
            operations.append(None)
            count_flags.append(False)
            teacher_ids.append(None)
            object_ids.append(None)
            matched_prio.append(None)
        else:
            tech_keys.append(m.tech_key)
            activities.append(m.activity)
            operations.append(m.operation)
            count_flags.append(bool(m.count_to_report))
            teacher_ids.append(m.teacher_id)
            object_ids.append(m.object_id)
            matched_prio.append(m.priority)

    df["tech_key"] = tech_keys
    df["activity"] = activities
    df["operation"] = operations
    df["count_to_report"] = count_flags
    df["teacher_id"] = teacher_ids
    df["object_id"] = object_ids
    df["rule_priority"] = matched_prio

    parquet_out.parent.mkdir(parents=True, exist_ok=True)
    df.to_parquet(parquet_out, index=False)
    return len(df), run_period


================================================================================
mrna_plum\paths.py
================================================================================

from dataclasses import dataclass
from pathlib import Path

@dataclass(frozen=True)
class ProjectPaths:
    root: Path

    @property
    def run_dir(self) -> Path:
        return self.root / "_run"

    @property
    def data_dir(self) -> Path:
        return self.root / "_data"

    @property
    def parquet_dir(self) -> Path:
        return self.data_dir / "parquet"

    @property
    def duckdb_path(self) -> Path:
        return self.data_dir / "mrna_plum.duckdb"

    @property
    def markers_dir(self) -> Path:
        return self.run_dir

    def marker_path(self, step: str) -> Path:
        return self.markers_dir / f"{step}.ok"


================================================================================
mrna_plum\reports\__init__.py
================================================================================

from .export_excel import export_summary_excel, ExportOverflowError


__all__ = ["export_summary_excel", "ExportOverflowError"]

================================================================================
mrna_plum\reports\export_excel.py
================================================================================

from __future__ import annotations

import json
import logging
import math
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple

# xlsxwriter is used directly (fast, pyinstaller-friendly)
import xlsxwriter


# ===== Exit codes (align with project) =====
EXIT_OK = 0
EXIT_OVERFLOW = 30


@dataclass(frozen=True)
class ExportExcelConfig:
    max_rows_excel: int = 1_000_000
    overflow_strategy: str = "error"  # error | split | skip
    activity_column: str = "activity_label"  # NOW default
    course_column: str = "course_name"       # NOW default

    include_hr_cols: bool = True
    exclude_zero_counts: bool = True
    percent_excel_format: bool = True

class ExportExcelError(RuntimeError):
    pass


class ExportOverflowError(ExportExcelError):
    """Raised when row-count exceeds Excel limit and strategy=error."""
    pass


def _ensure_dirs(root: Path) -> Tuple[Path, Path]:
    run_dir = root / "_run"
    out_dir = root / "_out"
    run_dir.mkdir(parents=True, exist_ok=True)
    out_dir.mkdir(parents=True, exist_ok=True)
    return run_dir, out_dir


def _setup_logger(run_dir: Path) -> logging.Logger:
    logger = logging.getLogger("mrna_plum.export_excel")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()

    fh = logging.FileHandler(run_dir / "run.log", encoding="utf-8")
    fmt = logging.Formatter("%(asctime)s %(levelname)s %(message)s")
    fh.setFormatter(fmt)
    logger.addHandler(fh)

    # optional console handler (kept minimal)
    sh = logging.StreamHandler()
    sh.setFormatter(fmt)
    logger.addHandler(sh)

    return logger


def _progress_append(run_dir: Path, event: Dict[str, Any]) -> None:
    p = run_dir / "progress.jsonl"
    event = dict(event)
    event["ts"] = datetime.now(timezone.utc).isoformat()
    with p.open("a", encoding="utf-8") as f:
        f.write(json.dumps(event, ensure_ascii=False) + "\n")


def _duckdb_has_column(con, table_fqn: str, col: str) -> bool:
    # Works for DuckDB: INFORMATION_SCHEMA.COLUMNS
    sql = """
        SELECT 1
        FROM information_schema.columns
        WHERE table_schema = ? AND table_name = ? AND column_name = ?
        LIMIT 1
    """
    if "." in table_fqn:
        schema, name = table_fqn.split(".", 1)
    else:
        schema, name = "main", table_fqn
    row = con.execute(sql, [schema, name, col]).fetchone()
    return row is not None


def _select_metrics_long_sql(con, activity_col: str, course_col: str,
                            include_hr_cols: bool, exclude_zero_counts: bool,
                            percent_excel_format: bool) -> str:
    table = "mart.metrics_long"

    # activity_label fallback
    if activity_col == "activity_label" and not _duckdb_has_column(con, table, "activity_label"):
        activity_col = "tech_key"

    # course_name fallback
    if course_col == "course_name" and not _duckdb_has_column(con, table, "course_name"):
        course_col = "course_code"

    pct_course = "pct_course" if _duckdb_has_column(con, table, "pct_course") else "NULL"
    pct_program = "pct_program" if _duckdb_has_column(con, table, "pct_program") else "NULL"
    pct_faculty = "pct_faculty" if _duckdb_has_column(con, table, "pct_faculty") else "NULL"
    pct_university = "pct_university" if _duckdb_has_column(con, table, "pct_university") else "NULL"

    # Excel %: zapisujemy ułamek (12.3 -> 0.123)
    def pct_expr(expr: str) -> str:
        if expr == "NULL":
            return "NULL"
        return f"({expr} / 100.0)" if percent_excel_format else expr

    pct_course_e = pct_expr(pct_course)
    pct_program_e = pct_expr(pct_program)
    pct_faculty_e = pct_expr(pct_faculty)
    pct_university_e = pct_expr(pct_university)

    order_course = "course_code" if _duckdb_has_column(con, table, "course_code") else course_col
    order_tech = "tech_key" if _duckdb_has_column(con, table, "tech_key") else activity_col

    where_parts = []
    if _duckdb_has_column(con, table, "visible_active"):
        where_parts.append("visible_active")
    if exclude_zero_counts and _duckdb_has_column(con, table, "count_value"):
        where_parts.append("count_value <> 0")

    where_clause = f"WHERE {' AND '.join(where_parts)}" if where_parts else ""

    # HR columns (optional, only if present)
    hr_cols = []
    if include_hr_cols:
        for col in ("hr_faculty", "hr_unit", "hr_department", "hr_org"):
            if _duckdb_has_column(con, table, col):
                hr_cols.append(f"{col}::VARCHAR AS {col}")

    hr_select = (",\n            " + ",\n            ".join(hr_cols)) if hr_cols else ""

    sql = f"""
        SELECT
            full_name::VARCHAR AS full_name,
            teacher_id::VARCHAR AS teacher_id{hr_select},
            {course_col}::VARCHAR AS course_value,
            {activity_col}::VARCHAR AS activity_value,
            count_value::BIGINT AS count_value,
            {pct_course_e}::DOUBLE AS pct_course,
            {pct_program_e}::DOUBLE AS pct_program,
            {pct_faculty_e}::DOUBLE AS pct_faculty,
            {pct_university_e}::DOUBLE AS pct_university,
            {order_tech}::VARCHAR AS _order_tech
        FROM {table}
        {where_clause}
        ORDER BY
            full_name ASC,
            {order_course} ASC,
            {order_tech} ASC
    """
    return sql


def _count_rows(con, base_sql: str) -> int:
    # Wrap in subquery; DuckDB handles it well.
    sql = f"SELECT COUNT(*)::BIGINT FROM ({base_sql}) t"
    return int(con.execute(sql).fetchone()[0])


def _select_metrics_qa_sql(con) -> str:
    table = "mart.metrics_qa"
    # Minimal required columns; if missing -> NULL
    type_col = "type" if _duckdb_has_column(con, table, "type") else "NULL"
    teacher_id = "teacher_id" if _duckdb_has_column(con, table, "teacher_id") else "NULL"
    course_code = "course_code" if _duckdb_has_column(con, table, "course_code") else "NULL"
    tech_key = "tech_key" if _duckdb_has_column(con, table, "tech_key") else "NULL"
    description = "description" if _duckdb_has_column(con, table, "description") else "NULL"

    sql = f"""
        SELECT
            {type_col}::VARCHAR AS type,
            {teacher_id}::VARCHAR AS teacher_id,
            {course_code}::VARCHAR AS course_code,
            {tech_key}::VARCHAR AS tech_key,
            {description}::VARCHAR AS description
        FROM {table}
        ORDER BY type ASC, teacher_id ASC, course_code ASC, tech_key ASC
    """
    return sql


def _write_sheet_header(ws, header_fmt, headers: Sequence[str]) -> None:
    ws.write_row(0, 0, list(headers), header_fmt)
    ws.freeze_panes(1, 0)


def _iter_cursor_rows(cur, batch_size: int) -> Iterable[Tuple[Any, ...]]:
    while True:
        rows = cur.fetchmany(batch_size)
        if not rows:
            break
        for r in rows:
            yield r


def _write_metrics_long_split_streaming(
    workbook: xlsxwriter.Workbook,
    con,
    sql: str,
    max_rows: int,
    overflow_strategy: str,
    main_sheet_base: str,
    logger: logging.Logger,
) -> Tuple[int, int]:
    """
    Stream rows from DuckDB and write to one or multiple sheets.
    Returns: (written_rows, sheets_count) excluding header row.
    """
    headers = [
        "Użytkownik",
        "ID",
        "Kurs",
        "Aktywność",
        "Liczba",
        "% kurs",
        "% kierunek",
        "% wydział",
        "% uczelnia",
    ]

    header_fmt = workbook.add_format({"bold": True, "border": 1})
    # Percent format: number with 1 decimal place, NOT Excel percentage
    pct_fmt = workbook.add_format({"num_format": "0.0"})
    int_fmt = workbook.add_format({"num_format": "0"})
    text_fmt = workbook.add_format({})  # default

    # column widths (optional but helpful)
    col_widths = [28, 12, 18, 26, 10, 10, 12, 12, 12]

    def make_sheet(idx: int):
        name = main_sheet_base if idx == 1 else f"{main_sheet_base}_{idx}"
        ws = workbook.add_worksheet(name[:31])
        for i, w in enumerate(col_widths):
            ws.set_column(i, i, w)
        _write_sheet_header(ws, header_fmt, headers)
        return ws

    ws = make_sheet(1)
    sheet_idx = 1
    row_in_sheet = 1  # start after header
    total_written = 0

    cur = con.execute(sql)
    colnames = [d[0] for d in cur.description]
    hr_present = [c for c in colnames if c.startswith("hr_")]
    cur.arraysize = 10_000

    for rec in _iter_cursor_rows(cur, batch_size=10_000):
        # If current sheet is full:
        if row_in_sheet > max_rows:
            if overflow_strategy == "split":
                sheet_idx += 1
                ws = make_sheet(sheet_idx)
                row_in_sheet = 1
            else:
                # overflow_strategy 'skip' shouldn't land here (handled earlier),
                # and 'error' should be prevented by pre-count logic.
                break

        # rec order:
        # full_name, teacher_id, course_value, activity_value, count_value,
        # pct_course, pct_program, pct_faculty, pct_university, _order_tech
        full_name, teacher_id, course_value, activity_value, count_value, p1, p2, p3, p4, _ = rec

        # Write row in one call (fast), with formats for numeric columns only
        ws.write(row_in_sheet, 0, full_name, text_fmt)
        ws.write(row_in_sheet, 1, teacher_id, text_fmt)
        ws.write(row_in_sheet, 2, course_value, text_fmt)
        ws.write(row_in_sheet, 3, activity_value, text_fmt)

        # count (int)
        ws.write_number(row_in_sheet, 4, float(count_value or 0), int_fmt)

        # percents (numbers with 1 decimal place display)
        # Note: ROUND is already done upstream; we only format.
        def write_pct(col: int, val: Any):
            if val is None:
                ws.write_blank(row_in_sheet, col, None)
            else:
                ws.write_number(row_in_sheet, col, float(val), pct_fmt)

        write_pct(5, p1)
        write_pct(6, p2)
        write_pct(7, p3)
        write_pct(8, p4)

        row_in_sheet += 1
        total_written += 1

    logger.info("Wrote %s rows into %s sheet(s).", total_written, sheet_idx)
    return total_written, sheet_idx


def _write_qa_sheet(workbook: xlsxwriter.Workbook, con, logger: logging.Logger) -> int:
    ws = workbook.add_worksheet("QA")
    header_fmt = workbook.add_format({"bold": True, "border": 1})
    text_fmt = workbook.add_format({})

    headers = ["type", "teacher_id", "course_code", "tech_key", "description"]
    ws.set_column(0, 0, 22)
    ws.set_column(1, 3, 16)
    ws.set_column(4, 4, 80)

    _write_sheet_header(ws, header_fmt, headers)

    sql = _select_metrics_qa_sql(con)
    cur = con.execute(sql)
    cur.arraysize = 10_000

    r = 1
    for row in _iter_cursor_rows(cur, batch_size=10_000):
        ws.write_row(r, 0, list(row), text_fmt)
        r += 1

    logger.info("QA rows: %s", r - 1)
    return r - 1


def _write_info_sheet(
    workbook: xlsxwriter.Workbook,
    con,
    base_sql_metrics_long: str,
    ay: str,
    term: str,
    generated_at: datetime,
    logger: logging.Logger,
) -> None:
    ws = workbook.add_worksheet("INFO")
    bold = workbook.add_format({"bold": True})
    ws.set_column(0, 0, 22)
    ws.set_column(1, 1, 40)

    # counts from SQL (no pandas aggregation)
    total = int(con.execute(f"SELECT COUNT(*)::BIGINT FROM ({base_sql_metrics_long}) t").fetchone()[0])

    teachers = int(
        con.execute(f"SELECT COUNT(DISTINCT teacher_id)::BIGINT FROM ({base_sql_metrics_long}) t").fetchone()[0]
    )

    courses = int(
        con.execute(f"SELECT COUNT(DISTINCT course_value)::BIGINT FROM ({base_sql_metrics_long}) t").fetchone()[0]
    )

    rows = [
        ("ay", ay),
        ("term", term),
        ("data_wygenerowania_utc", generated_at.isoformat()),
        ("liczba_nauczycieli", teachers),
        ("liczba_kursow", courses),
        ("liczba_rekordow", total),
    ]

    ws.write(0, 0, "pole", bold)
    ws.write(0, 1, "wartosc", bold)
    for i, (k, v) in enumerate(rows, start=1):
        ws.write(i, 0, k)
        ws.write(i, 1, v)

    logger.info("INFO: teachers=%s courses=%s records=%s", teachers, courses, total)


def _export_skip_strategy_sql(con, activity_col: str) -> str:
    """
    "skip" is ambiguous in prompt. We implement safe minimal output:
    aggregate per teacher + activity only, without course. Percents set NULL.
    """
    table = "mart.metrics_long"
    if activity_col == "activity_label" and not _duckdb_has_column(con, table, "activity_label"):
        activity_col = "tech_key"

    where_clause = "WHERE visible_active" if _duckdb_has_column(con, table, "visible_active") else ""
    sql = f"""
        SELECT
            full_name::VARCHAR AS full_name,
            teacher_id::VARCHAR AS teacher_id,
            NULL::VARCHAR AS course_value,
            {activity_col}::VARCHAR AS activity_value,
            SUM(count_value)::BIGINT AS count_value,
            NULL::DOUBLE AS pct_course,
            NULL::DOUBLE AS pct_program,
            NULL::DOUBLE AS pct_faculty,
            NULL::DOUBLE AS pct_university,
            {activity_col}::VARCHAR AS _order_tech
        FROM {table}
        {where_clause}
        GROUP BY 1,2,3,4,10
        ORDER BY full_name ASC, _order_tech ASC
    """
    return sql


def export_summary_excel(con, cfg: Dict[str, Any]) -> Tuple[int, Path]:
    """
    Main entry point. Returns (exit_code, output_path).
    Requires:
      cfg["paths"]["root"] or cfg["root"] (depending on your config shape)
      cfg["report"]["ay"], cfg["report"]["term"] (or equivalents)
      cfg["export"]["max_rows_excel"], cfg["export"]["overflow_strategy"] (optional)
    """
    # --- Resolve config fields (keep tolerant to shape) ---
    root = Path(cfg.get("root") or cfg.get("paths", {}).get("root") or cfg.get("paths", {}).get("output_root", "."))
    ay = str(cfg.get("report", {}).get("ay") or cfg.get("ay") or "")
    term = str(cfg.get("report", {}).get("term") or cfg.get("term") or "")

    export_cfg = ExportExcelConfig(
        max_rows_excel=int(cfg.get("export", {}).get("max_rows_excel", 1_000_000)),
        overflow_strategy=str(cfg.get("export", {}).get("overflow_strategy", "error")),
        activity_column=str(cfg.get("export", {}).get("activity_column", "activity_label")),
        course_column=str(cfg.get("export", {}).get("course_column", "course_name")),
        include_hr_cols=bool(cfg.get("export", {}).get("include_hr_cols", True)),
        exclude_zero_counts=bool(cfg.get("export", {}).get("exclude_zero_counts", True)),
        percent_excel_format=bool(cfg.get("export", {}).get("percent_excel_format", True)),
     )

    run_dir, out_dir = _ensure_dirs(root)
    logger = _setup_logger(run_dir)

    _progress_append(run_dir, {"step": "export-excel", "status": "start", "ay": ay, "term": term})

    out_path = out_dir / f"Raport_Zbiorczy_NA_{ay}_{term}.xlsx"
    ok_flag = run_dir / "export-excel.ok"

    logger.info("Exporting XLSX to: %s", out_path)
    logger.info("Export config: %s", export_cfg)

    # Build base SQL (ordered) for main sheet
    base_sql = _select_metrics_long_sql(
    con,
    export_cfg.activity_column,
    export_cfg.course_column,
    export_cfg.include_hr_cols,
    export_cfg.exclude_zero_counts,
    export_cfg.percent_excel_format,
    )

    # Pre-count rows to enforce overflow strategy deterministically
    total_rows = _count_rows(con, base_sql)
    logger.info("metrics_long rows to export: %s", total_rows)

    # Decide strategy
    strategy = export_cfg.overflow_strategy.lower().strip()
    max_rows = export_cfg.max_rows_excel

    if total_rows > max_rows and strategy == "error":
        _progress_append(
            run_dir,
            {
                "step": "export-excel",
                "status": "error",
                "reason": "overflow",
                "rows": total_rows,
                "max_rows_excel": max_rows,
                "strategy": strategy,
            },
        )
        raise ExportOverflowError(f"Too many rows for Excel: {total_rows} > {max_rows}")

    if total_rows > max_rows and strategy == "skip":
        logger.warning("Overflow with strategy=skip; exporting aggregated per teacher (course=NULL).")
        base_sql = _export_skip_strategy_sql(con, export_cfg.activity_column)
        total_rows = _count_rows(con, base_sql)

    # Create workbook (overwrite, idempotent)
    generated_at = datetime.now(timezone.utc)

    workbook = xlsxwriter.Workbook(
        out_path.as_posix(),
        {
            "constant_memory": True,  # good for large datasets
            "strings_to_numbers": False,
            "strings_to_formulas": False,
            "strings_to_urls": False,
        },
    )

    try:
        main_sheet_base = "ZLICZENIE_AKTYWNOSCI_NA"

        # Main data
        _progress_append(run_dir, {"step": "export-excel", "status": "writing_main", "rows": total_rows})
        written_rows, sheets_count = _write_metrics_long_split_streaming(
            workbook=workbook,
            con=con,
            sql=base_sql,
            max_rows=max_rows,
            overflow_strategy=("split" if strategy == "split" else "error"),
            main_sheet_base=main_sheet_base,
            logger=logger,
        )

        # QA
        _progress_append(run_dir, {"step": "export-excel", "status": "writing_qa"})
        qa_rows = _write_qa_sheet(workbook, con, logger)

        # INFO
        _progress_append(run_dir, {"step": "export-excel", "status": "writing_info"})
        _write_info_sheet(workbook, con, base_sql, ay, term, generated_at, logger)

    finally:
        workbook.close()

    ok_flag.write_text("OK\n", encoding="utf-8")
    _progress_append(
        run_dir,
        {
            "step": "export-excel",
            "status": "done",
            "output": str(out_path),
            "main_rows": written_rows,
            "qa_rows": qa_rows,
            "main_sheets": sheets_count,
        },
    )

    logger.info("DONE: %s", out_path)
    return EXIT_OK, out_path

================================================================================
mrna_plum\reports\export_individual.py
================================================================================

from __future__ import annotations

import json
import re
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterator, List, Optional, Sequence, Tuple

import duckdb
import xlsxwriter
from concurrent.futures import ThreadPoolExecutor, as_completed


# ======================================================================================
# Config helpers (tolerant: cfg can be dict-like or attr-like)
# ======================================================================================

def _cfg_get(cfg: Any, path: str, default: Any = None) -> Any:
    """
    Read cfg value using dotted path, supports both dict-like and attribute-like objects.
    Example: _cfg_get(cfg, "reports.max_workers", 4)
    """
    cur = cfg
    for part in path.split("."):
        if cur is None:
            return default
        if isinstance(cur, dict):
            cur = cur.get(part, None)
        else:
            cur = getattr(cur, part, None)
    return default if cur is None else cur


# ======================================================================================
# Logging & artifacts
# ======================================================================================

@dataclass(frozen=True)
class RunArtifacts:
    run_dir: Path
    run_log: Path
    progress_jsonl: Path
    ok_file: Path


def _ensure_artifacts(root: Path) -> RunArtifacts:
    run_dir = root / "_run"
    run_dir.mkdir(parents=True, exist_ok=True)
    run_log = run_dir / "run.log"
    progress_jsonl = run_dir / "progress.jsonl"
    ok_file = run_dir / "export-individual.ok"
    return RunArtifacts(run_dir, run_log, progress_jsonl, ok_file)


def _log(line: str, run_log: Path) -> None:
    ts = time.strftime("%Y-%m-%d %H:%M:%S")
    msg = f"{ts} [export-individual] {line}"
    print(msg)
    with run_log.open("a", encoding="utf-8") as f:
        f.write(msg + "\n")


def _progress(evt: Dict[str, Any], progress_jsonl: Path) -> None:
    evt = dict(evt)
    evt.setdefault("ts", time.strftime("%Y-%m-%d %H:%M:%S"))
    with progress_jsonl.open("a", encoding="utf-8") as f:
        f.write(json.dumps(evt, ensure_ascii=False) + "\n")


# ======================================================================================
# Filename sanitization
# ======================================================================================

_WIN_ILLEGAL = r'<>:"/\|?*'
_WIN_ILLEGAL_RE = re.compile(rf"[{re.escape(_WIN_ILLEGAL)}]")


def sanitize_filename(name: str, max_len: int = 120) -> str:
    """
    - replace illegal Windows filename chars with '_'
    - collapse whitespace
    - trim
    - limit length
    """
    if not name:
        return ""
    s = str(name).strip()
    s = _WIN_ILLEGAL_RE.sub("_", s)
    s = re.sub(r"\s+", " ", s)
    s = s.strip(" .")  # Windows hates trailing dot/space
    if len(s) > max_len:
        s = s[:max_len].rstrip(" .")
    return s


# ======================================================================================
# DuckDB SQL
# ======================================================================================

def _ensure_qa_table(con: duckdb.DuckDBPyConnection) -> None:
    con.execute("CREATE SCHEMA IF NOT EXISTS mart;")
    con.execute(
        """
        CREATE TABLE IF NOT EXISTS mart.individual_export_qa (
            teacher_id      VARCHAR,
            status          VARCHAR,  -- OK / SKIPPED_* / ERROR
            message         VARCHAR,
            output_file     VARCHAR,
            rows_exported   BIGINT,
            exported_at     TIMESTAMP DEFAULT now()
        );
        """
    )


def _qa_insert(
    con: duckdb.DuckDBPyConnection,
    teacher_id: str,
    status: str,
    message: str,
    output_file: Optional[str],
    rows_exported: int,
) -> None:
    con.execute(
        """
        INSERT INTO mart.individual_export_qa
            (teacher_id, status, message, output_file, rows_exported)
        VALUES (?, ?, ?, ?, ?)
        """,
        [teacher_id, status, message, output_file, rows_exported],
    )


def _detect_hr_columns(con: duckdb.DuckDBPyConnection) -> List[str]:
    """
    Prefer HR embedded in mart.metrics_long as dynamic columns hr_*.
    Return list of column names present in metrics_long matching hr_*.
    """
    rows = con.execute("DESCRIBE mart.metrics_long;").fetchall()
    col_names = [r[0] for r in rows]
    return [c for c in col_names if c.lower().startswith("hr_")]


def _list_teachers(con: duckdb.DuckDBPyConnection) -> List[Tuple[str, str, str, str]]:
    """
    Teachers to export (metrics_long already HR-whitelisted).
    EXPORT RULE:
      - require teacher_id AND email (non-empty); otherwise teacher is not included here.
      - require id_bazus for filename (we still include them here; missing bazus handled in worker).
    Deterministic order: teacher_id asc.
    Returns: (teacher_id, full_name, email, id_bazus)
    """
    rows = con.execute(
        """
        SELECT
            teacher_id::VARCHAR AS teacher_id,
            COALESCE(NULLIF(TRIM(full_name), ''), '') AS full_name,
            COALESCE(NULLIF(TRIM(email), ''), '') AS email,
            COALESCE(NULLIF(TRIM(id_bazus), ''), '') AS id_bazus
        FROM mart.metrics_long
        WHERE visible_active = TRUE
        GROUP BY 1, 2, 3, 4
        HAVING teacher_id IS NOT NULL
           AND TRIM(teacher_id) <> ''
           AND email IS NOT NULL
           AND TRIM(email) <> ''
        ORDER BY teacher_id ASC
        """
    ).fetchall()
    return [(str(tid), str(fn), str(em), str(bz)) for tid, fn, em, bz in rows]


def _fetch_teacher_rows(
    con: duckdb.DuckDBPyConnection,
    teacher_id: str,
) -> Iterator[Tuple[Any, ...]]:
    """
    Rows for DANE_KURSY:
      course_name, activity_label, count_value,
      pct_course/100.0, pct_kierunek/100.0, pct_wydzial/100.0, pct_uczelnia/100.0

    Deterministic sort, count_value>0 only.
    """
    cur = con.execute(
        """
        SELECT
            course_name,
            activity_label,
            count_value,
            (pct_course / 100.0)   AS pct_course_xlsx,
            (pct_kierunek / 100.0) AS pct_kierunek_xlsx,
            (pct_wydzial / 100.0)  AS pct_wydzial_xlsx,
            (pct_uczelnia / 100.0) AS pct_uczelnia_xlsx
        FROM mart.metrics_long
        WHERE visible_active = TRUE
          AND teacher_id::VARCHAR = ?
          AND count_value > 0
        ORDER BY
            course_name ASC,
            activity_label ASC
        """,
        [teacher_id],
    )
    while True:
        batch = cur.fetchmany(10_000)
        if not batch:
            break
        for row in batch:
            yield row


def _fetch_teacher_pers(
    con: duckdb.DuckDBPyConnection,
    teacher_id: str,
    hr_cols: List[str],
) -> Dict[str, Any]:
    """
    One row with "metrics_long embedded HR" preference.
    We take MAX(...) as safe collapse (same per teacher).
    Assumes email + id_bazus exist in schema (per your rules).
    """
    select_parts = [
        "teacher_id::VARCHAR AS teacher_id",
        "MAX(COALESCE(NULLIF(TRIM(full_name), ''), '')) AS full_name",
        "MAX(COALESCE(NULLIF(TRIM(email), ''), '')) AS email",
        "MAX(COALESCE(NULLIF(TRIM(id_bazus), ''), '')) AS id_bazus",
    ]

    for c in hr_cols:
        select_parts.append(f"MAX(COALESCE(NULLIF(TRIM({c}), ''), '')) AS {c}")

    sql = f"""
        SELECT {", ".join(select_parts)}
        FROM mart.metrics_long
        WHERE visible_active = TRUE
          AND teacher_id::VARCHAR = ?
        GROUP BY teacher_id
    """
    row = con.execute(sql, [teacher_id]).fetchone()
    if row is None:
        base = {"teacher_id": teacher_id, "full_name": "", "email": "", "id_bazus": ""}
        for c in hr_cols:
            base[c] = ""
        return base

    keys = ["teacher_id", "full_name", "email", "id_bazus"] + hr_cols
    return dict(zip(keys, row))


def _hr_human_label(col: str) -> str:
    """
    Map hr_* columns to human-friendly labels for DANE_PERS.
    Extend as you standardize HR fields.
    """
    m = {
        "hr_wydzial": "Wydział",
        "hr_jednostka": "Jednostka",
        "hr_katedra": "Katedra",
        "hr_zaklad": "Zakład",
        "hr_stanowisko": "Stanowisko",
        "hr_tytul": "Tytuł / Stopień",
        "hr_umowa": "Rodzaj umowy",
    }
    low = col.lower()
    return m.get(low, col)  # fallback to raw column name


# ======================================================================================
# XLSX writing
# ======================================================================================

_COURSES_HEADERS = ["Kurs", "Aktywność", "Liczba", "% kurs", "% kierunek", "% wydział", "% uczelnia"]


def _write_teacher_xlsx(
    out_file: Path,
    teacher_id: str,
    full_name: str,
    rows_iter: Iterator[Tuple[Any, ...]],
    pers: Dict[str, Any],
    hr_cols: List[str],
) -> int:
    """
    Returns rows_exported (DANE_KURSY count).
    """
    out_file.parent.mkdir(parents=True, exist_ok=True)

    wb = xlsxwriter.Workbook(out_file.as_posix(), {"constant_memory": True})
    try:
        fmt_header = wb.add_format({"bold": True})
        fmt_pct = wb.add_format({"num_format": "0.0%"})
        fmt_int = wb.add_format({"num_format": "0"})  # count_value

        # Sheet 1: DANE_KURSY
        ws1 = wb.add_worksheet("DANE_KURSY")
        for c, h in enumerate(_COURSES_HEADERS):
            ws1.write(0, c, h, fmt_header)

        r = 1
        for (
            course_name,
            activity_label,
            count_value,
            pct_course_x,
            pct_kierunek_x,
            pct_wydzial_x,
            pct_uczelnia_x,
        ) in rows_iter:
            ws1.write(r, 0, course_name if course_name is not None else "")
            ws1.write(r, 1, activity_label if activity_label is not None else "")

            try:
                ws1.write_number(r, 2, float(count_value), fmt_int)
            except Exception:
                ws1.write(r, 2, count_value)

            for j, v in enumerate([pct_course_x, pct_kierunek_x, pct_wydzial_x, pct_uczelnia_x], start=3):
                if v is None:
                    ws1.write_blank(r, j, None)
                else:
                    ws1.write_number(r, j, float(v), fmt_pct)

            r += 1

        rows_exported = r - 1

        # Sheet 2: DANE_PERS (vertical key -> value)
        ws2 = wb.add_worksheet("DANE_PERS")
        ws2.write(0, 0, "Pole", fmt_header)
        ws2.write(0, 1, "Wartość", fmt_header)

        # Required minimum fields + your "Imię i nazwisko"
        name_val = pers.get("full_name", full_name) or full_name or ""
        kv: List[Tuple[str, Any]] = [
            ("ID_PLUM", teacher_id),
            ("Imię i nazwisko", name_val),
            ("E-mail", pers.get("email", "") or ""),
            ("ID bazus", pers.get("id_bazus", "") or ""),
        ]

        # HR -> human labels (dynamic)
        for c in hr_cols:
            kv.append((_hr_human_label(c), pers.get(c, "") or ""))

        # Ensure at least Wydział/Jednostka rows exist even if hr cols absent
        if not any(_hr_human_label(c) == "Wydział" for c in hr_cols):
            kv.append(("Wydział", ""))
        if not any(_hr_human_label(c) == "Jednostka" for c in hr_cols):
            kv.append(("Jednostka", ""))

        for i, (k, v) in enumerate(kv, start=1):
            ws2.write(i, 0, k)
            ws2.write(i, 1, v)

        return rows_exported
    finally:
        wb.close()


# ======================================================================================
# Public API
# ======================================================================================

def export_individual_reports(
    con: duckdb.DuckDBPyConnection,
    cfg: Any,
) -> Tuple[int, str]:
    """
    Public entrypoint:
      export_individual_reports(con, cfg) -> (exit_code, out_dir)

    Rules applied:
      - SQL-first (DuckDB)
      - only visible_active
      - exclude count_value=0 from export
      - require teacher_id + email (non-empty) or SKIP
      - filename: <NazwiskoImie>_<BAZUS ID>.xlsx (sanitized)
      - pct columns: DB 0-100 -> XLSX pct/100.0 with format 0.0%
      - deterministic sort: course_name ASC, activity_label ASC
      - idempotent overwrite
    """
    root = Path(_cfg_get(cfg, "root", ".")).resolve()
    arts = _ensure_artifacts(root)

    out_rel = _cfg_get(cfg, "reports.individual_dir", "_out/indywidualne")
    out_dir = (root / out_rel).resolve()
    max_workers = int(_cfg_get(cfg, "reports.max_workers", 4))
    batch_teachers = int(_cfg_get(cfg, "reports.batch_teachers", 50))

    _log(f"root={root}", arts.run_log)
    _log(f"out_dir={out_dir}", arts.run_log)
    _log(f"max_workers={max_workers} batch_teachers={batch_teachers}", arts.run_log)

    _ensure_qa_table(con)
    hr_cols = _detect_hr_columns(con)
    _log(f"Detected HR columns in mart.metrics_long: {hr_cols}", arts.run_log)

    teachers = _list_teachers(con)  # (teacher_id, full_name, email, id_bazus) with email required
    _log(f"Teachers to export (email required): {len(teachers)}", arts.run_log)

    # For parallel: DuckDB connection is not safely shared across threads.
    # We'll open a separate connection per worker using cfg.paths.db_path.
    db_path = _cfg_get(cfg, "paths.db_path", None)

    def _worker(teacher_id: str, full_name: str, email: str, id_bazus: str) -> Tuple[str, str, str, Optional[str], int]:
        """
        Returns: (teacher_id, status, message, output_file, rows_exported)
        """
        local_con = con
        must_close = False
        try:
            # hard guards (your rule)
            if not teacher_id or not str(teacher_id).strip():
                return (str(teacher_id), "SKIPPED_NO_ID", "Missing teacher_id", None, 0)
            if not email or not str(email).strip():
                return (str(teacher_id), "SKIPPED_NO_EMAIL", "Missing email", None, 0)
            if not id_bazus or not str(id_bazus).strip():
                return (str(teacher_id), "SKIPPED_NO_BAZUS", "Missing BAZUS ID for filename", None, 0)

            if max_workers and max_workers > 1:
                if not db_path:
                    # no db_path -> do not parallelize safely
                    local_con = con
                else:
                    local_con = duckdb.connect(str(db_path))
                    must_close = True

            safe_name = sanitize_filename(full_name, max_len=120) or "UNKNOWN"
            safe_bazus = sanitize_filename(str(id_bazus), max_len=60) or "BAZUS_UNKNOWN"

            filename = f"{safe_name}_{safe_bazus}.xlsx"
            if len(filename) > 180:
                safe_name2 = sanitize_filename(safe_name, max_len=120)
                filename = f"{safe_name2}_{safe_bazus}.xlsx"

            out_file = out_dir / filename

            # fetch rows (streaming)
            rows_iter = _fetch_teacher_rows(local_con, teacher_id)

            # Need to know if any rows exist without consuming iterator -> buffer first item
            buffered: List[Tuple[Any, ...]] = []
            try:
                buffered.append(next(rows_iter))
            except StopIteration:
                buffered = []

            if not buffered:
                return (teacher_id, "SKIPPED_NO_DATA", "No rows with count_value>0", None, 0)

            def _chain() -> Iterator[Tuple[Any, ...]]:
                for x in buffered:
                    yield x
                for x in rows_iter:
                    yield x

            pers = _fetch_teacher_pers(local_con, teacher_id, hr_cols)

            # idempotent overwrite
            if out_file.exists():
                out_file.unlink()

            rows_exported = _write_teacher_xlsx(
                out_file=out_file,
                teacher_id=teacher_id,
                full_name=full_name,
                rows_iter=_chain(),
                pers=pers,
                hr_cols=hr_cols,
            )

            if rows_exported == 0:
                if out_file.exists():
                    out_file.unlink()
                return (teacher_id, "SKIPPED_NO_DATA", "No rows with count_value>0", None, 0)

            return (teacher_id, "OK", "Exported", str(out_file), int(rows_exported))

        except Exception as e:
            return (teacher_id, "ERROR", f"{type(e).__name__}: {e}", None, 0)
        finally:
            if must_close:
                try:
                    local_con.close()
                except Exception:
                    pass

    exported_ok = 0
    exported_err = 0
    exported_skip = 0

    def _batched(seq: Sequence[Tuple[str, str, str, str]], n: int) -> Iterator[List[Tuple[str, str, str, str]]]:
        for i in range(0, len(seq), n):
            yield list(seq[i : i + n])

    for batch in _batched(teachers, batch_teachers):
        if max_workers and max_workers > 1 and db_path:
            with ThreadPoolExecutor(max_workers=max_workers) as ex:
                futs = [ex.submit(_worker, tid, fn, em, bz) for (tid, fn, em, bz) in batch]
                for fut in as_completed(futs):
                    tid, status, msg, out_file, rows_exported = fut.result()
                    _qa_insert(con, tid, status, msg, out_file, rows_exported)
                    _progress(
                        {
                            "teacher_id": tid,
                            "status": status,
                            "message": msg,
                            "output_file": out_file,
                            "rows_exported": rows_exported,
                        },
                        arts.progress_jsonl,
                    )
                    if status == "OK":
                        exported_ok += 1
                    elif status.startswith("SKIPPED"):
                        exported_skip += 1
                    else:
                        exported_err += 1
        else:
            for (tid, fn, em, bz) in batch:
                tid, status, msg, out_file, rows_exported = _worker(tid, fn, em, bz)
                _qa_insert(con, tid, status, msg, out_file, rows_exported)
                _progress(
                    {
                        "teacher_id": tid,
                        "status": status,
                        "message": msg,
                        "output_file": out_file,
                        "rows_exported": rows_exported,
                    },
                    arts.progress_jsonl,
                )
                if status == "OK":
                    exported_ok += 1
                elif status.startswith("SKIPPED"):
                    exported_skip += 1
                else:
                    exported_err += 1

    _log(f"Done. OK={exported_ok} SKIPPED={exported_skip} ERROR={exported_err}", arts.run_log)

    if exported_err == 0:
        arts.ok_file.write_text("OK\n", encoding="utf-8")
        return (0, str(out_dir))
    return (2, str(out_dir))


# ======================================================================================
# Optional CLI wrapper (adapt to your cli.py / Typer)
# ======================================================================================

def cli_export_individual(root: str, config: Any) -> int:
    """
    Example CLI wrapper (adapt to your existing cli.py).
    - root: pipeline root
    - config: loaded config object/dict
    """
    if isinstance(config, dict):
        config = dict(config)
        config["root"] = root
    else:
        setattr(config, "root", root)

    db_path = _cfg_get(config, "paths.db_path", None)
    if not db_path:
        raise RuntimeError("cfg.paths.db_path is required for export-individual")

    con = duckdb.connect(str(db_path))
    try:
        code, _out = export_individual_reports(con, config)
        return int(code)
    finally:
        con.close()

================================================================================
mrna_plum\rules\__init__.py
================================================================================



================================================================================
mrna_plum\rules\activity_rules.py
================================================================================

# src/mrna_plum/rules/activity_rules.py

from __future__ import annotations

import re
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from openpyxl import load_workbook


@dataclass(frozen=True)
class KeyRule:
    activity_label: str
    tech_key: str
    operation: str          # CREATE/DELETE/UPDATE/VIEW/GRADE/...
    count_mode: str         # TAK / TAK_FLAG / NIE
    rx_match: re.Pattern
    rx_user: Optional[re.Pattern]
    rx_object_from_match_group: Optional[str]  # e.g. "object_id" or None
    priority: int


@dataclass(frozen=True)
class RuleMatch:
    activity_label: str
    tech_key: str
    operation: str
    count_mode: str
    teacher_id: Optional[int]
    object_id: Optional[int]
    priority: int
    conflict: bool


def _norm(s: Any) -> str:
    return ("" if s is None else str(s)).strip()


def _compile(pat: str) -> re.Pattern:
    # wszystkie regexy traktujemy case-insensitive
    return re.compile(pat, re.IGNORECASE)


def load_keys_rules(keys_xlsx: str, sheet_name: str = "KEYS") -> List[KeyRule]:
    """
    Oczekiwane kolumny w KEYS:
    AKTYWNOSC
    KLUCZ_TECHNICZNY
    OPERACJA
    LICZYC_DO_RAPORTU
    REGEX_DOPASOWANIA_(Opis)
    REGEX_USER_ID_(Opis)
    REGEX_OBIEKT_ID_(z dopasowania)   # opcjonalnie: nazwa grupy np. object_id
    PRIORYTET
    """
    wb = load_workbook(filename=keys_xlsx, read_only=True, data_only=True)
    if sheet_name not in wb.sheetnames:
        raise ValueError(f"KEYS: brak arkusza '{sheet_name}' w {keys_xlsx}")

    ws = wb[sheet_name]
    rows = ws.iter_rows(values_only=True)
    header = [(_norm(x)) for x in next(rows)]
    idx = {name: i for i, name in enumerate(header) if name}

    def col(name: str) -> int:
        if name not in idx:
            raise ValueError(f"KEYS: brak kolumny '{name}' (nagłówki: {header})")
        return idx[name]

    rules: List[KeyRule] = []

    for r in rows:
        activity_label = _norm(r[col("AKTYWNOSC")])
        tech_key = _norm(r[col("KLUCZ_TECHNICZNY")])
        operation = _norm(r[col("OPERACJA")]).upper()
        count_mode = _norm(r[col("LICZYC_DO_RAPORTU")]).upper()  # TAK/TAK_FLAG/NIE
        rx_match_txt = _norm(r[col("REGEX_DOPASOWANIA_(Opis)")])
        rx_user_txt = _norm(r[col("REGEX_USER_ID_(Opis)")])
        rx_obj_group = _norm(r[idx.get("REGEX_OBIEKT_ID_(z dopasowania)", -1)]) if "REGEX_OBIEKT_ID_(z dopasowania)" in idx else ""
        prio_txt = _norm(r[col("PRIORYTET")])
        if not rx_match_txt or not tech_key:
            continue

        try:
            prio = int(prio_txt) if prio_txt else 0
        except Exception:
            prio = 0

        rx_match = _compile(rx_match_txt)
        rx_user = _compile(rx_user_txt) if rx_user_txt else None
        rx_obj_group = rx_obj_group or None

        rules.append(
            KeyRule(
                activity_label=activity_label,
                tech_key=tech_key,
                operation=operation,
                count_mode=count_mode,
                rx_match=rx_match,
                rx_user=rx_user,
                rx_object_from_match_group=rx_obj_group,
                priority=prio,
            )
        )

    # najwyższy priorytet wcześniej – przyspiesza
    rules.sort(key=lambda x: x.priority, reverse=True)
    return rules


class ActivityRuleEngine:
    def __init__(self, rules: List[KeyRule], drop_mode_nie: bool = True):
        self.rules = rules
        self.drop_mode_nie = drop_mode_nie

    def match(self, opis: str) -> Optional[RuleMatch]:
        """
        Jeśli wiele reguł pasuje:
        - wybierz najwyższy PRIORYTET
        - jeśli kilka ma ten sam max PRIORYTET -> conflict=True
        """
        opis = opis or ""
        matches: List[Tuple[KeyRule, re.Match]] = []

        for rule in self.rules:
            m = rule.rx_match.search(opis)
            if m:
                matches.append((rule, m))

        if not matches:
            return None

        # wybór max priorytetu
        max_prio = max(rule.priority for rule, _ in matches)
        best = [(rule, m) for rule, m in matches if rule.priority == max_prio]

        # bierzemy pierwszy jako “winner”, ale zaznaczamy konflikt jeśli >1
        rule, m = best[0]
        conflict = len(best) > 1

        # object_id – z grupy nazwanej, jeśli KEYS wskazuje; inaczej spróbuj 1. grupy
        object_id: Optional[int] = None
        if rule.rx_object_from_match_group:
            gd = m.groupdict()
            val = gd.get(rule.rx_object_from_match_group)
            if val:
                try:
                    object_id = int(val)
                except Exception:
                    object_id = None
        else:
            try:
                if m.groups():
                    object_id = int(m.group(1))
            except Exception:
                object_id = None

        # teacher_id – regex osobny z KEYS (na Opis)
        teacher_id: Optional[int] = None
        if rule.rx_user:
            um = rule.rx_user.search(opis)
            if um:
                gd = um.groupdict()
                if "id" in gd and gd["id"]:
                    try:
                        teacher_id = int(gd["id"])
                    except Exception:
                        teacher_id = None
                else:
                    try:
                        teacher_id = int(um.group(1))
                    except Exception:
                        teacher_id = None

        if self.drop_mode_nie and rule.count_mode == "NIE":
            return None

        return RuleMatch(
            activity_label=rule.activity_label,
            tech_key=rule.tech_key,
            operation=rule.operation,
            count_mode=rule.count_mode,
            teacher_id=teacher_id,
            object_id=object_id,
            priority=rule.priority,
            conflict=conflict,
        )

================================================================================
mrna_plum\rules\engine.py
================================================================================

from __future__ import annotations
from dataclasses import dataclass
from typing import Optional
import re
import pandas as pd

from .models import Rule

@dataclass(frozen=True)
class MatchResult:
    tech_key: str
    activity: str
    operation: str
    count_to_report: bool
    teacher_id: Optional[str]
    object_id: Optional[str]
    priority: int

def compile_rules(keys_df: pd.DataFrame) -> list[Rule]:
    rules: list[Rule] = []
    for _, r in keys_df.iterrows():
        def _p(s: object) -> str:
            return "" if s is None else str(s)

        match_rx = re.compile(_p(r["REGEX_DOPASOWANIA_(Opis)"]))
        user_rx_s = _p(r["REGEX_USER_ID_(Opis)"]).strip()
        obj_rx_s  = _p(r["REGEX_OBIEKT_ID_(z dopasowania)"]).strip()

        user_rx = re.compile(user_rx_s) if user_rx_s else None
        obj_rx  = re.compile(obj_rx_s) if obj_rx_s else None

        count_flag = _p(r["LICZYC_DO_RAPORTU"]).upper() in ("TAK", "1", "TRUE", "YES")

        rules.append(
            Rule(
                activity=_p(r["AKTYWNOSC"]).strip(),
                tech_key=_p(r["KLUCZ_TECHNICZNY"]).strip(),
                operation=_p(r["OPERACJA"]).strip(),
                count_to_report=count_flag,
                regex_match_desc=match_rx,
                regex_user_id=user_rx,
                regex_object_id=obj_rx,
                priority=int(r["PRIORYTET"]),
            )
        )
    # wyższy priorytet pierwszy
    rules.sort(key=lambda x: x.priority, reverse=True)
    return rules

def match_best_rule(description: str, rules: list[Rule]) -> Optional[MatchResult]:
    for rule in rules:
        m = rule.regex_match_desc.search(description or "")
        if not m:
            continue

        teacher_id = None
        object_id = None

        if rule.regex_user_id:
            um = rule.regex_user_id.search(description or "")
            if um and um.groups():
                teacher_id = um.group(1)
            elif um:
                teacher_id = um.group(0)

        if rule.regex_object_id:
            # object_id może być z dopasowania głównego (m) albo z opisu
            om = rule.regex_object_id.search(m.group(0)) or rule.regex_object_id.search(description or "")
            if om and om.groups():
                object_id = om.group(1)
            elif om:
                object_id = om.group(0)

        return MatchResult(
            tech_key=rule.tech_key,
            activity=rule.activity,
            operation=rule.operation,
            count_to_report=rule.count_to_report,
            teacher_id=teacher_id,
            object_id=object_id,
            priority=rule.priority,
        )
    return None


================================================================================
mrna_plum\rules\models.py
================================================================================

from __future__ import annotations
from dataclasses import dataclass
import re
from typing import Optional

@dataclass(frozen=True)
class Rule:
    activity: str
    tech_key: str
    operation: str          # np. TAK / TAK_FLAG / NIE / etc.
    count_to_report: bool
    regex_match_desc: re.Pattern
    regex_user_id: Optional[re.Pattern]
    regex_object_id: Optional[re.Pattern]
    priority: int


================================================================================
mrna_plum\stats\__init__.py
================================================================================

from .compute_stats import compute_stats
__all__ = ["compute_stats"]


================================================================================
mrna_plum\stats\compute_stats.py
================================================================================

from __future__ import annotations

import json
from dataclasses import dataclass
from pathlib import Path
from datetime import datetime, timezone
from typing import Optional, Sequence

import duckdb
import pandas as pd
import yaml


# ----------------------------
# Helpers / config
# ----------------------------

@dataclass(frozen=True)
class StatsConfig:
    duckdb_path: Path
    run_dir: Path
    include_deleted_in_percent: bool
    rebuild_full: bool

    # mapping sources
    map_teacher_id_email_path: Optional[Path]
    map_email_hr_path: Optional[Path]

    # rounding
    pct_round_decimals: int

    # period
    ay: Optional[str]
    term: Optional[str]


def _load_config(root: Path) -> dict:
    cfg_path = root / "config.yaml"
    if not cfg_path.exists():
        raise FileNotFoundError(f"Brak config.yaml pod: {cfg_path}")
    return yaml.safe_load(cfg_path.read_text(encoding="utf-8"))


def _resolve_path(root: Path, p: Optional[str]) -> Optional[Path]:
    if not p:
        return None
    pp = Path(p)
    return pp if pp.is_absolute() else (root / pp)


def _ensure_run_artifacts(run_dir: Path) -> None:
    run_dir.mkdir(parents=True, exist_ok=True)


def _log_progress(run_dir: Path, payload: dict) -> None:
    p = run_dir / "progress.jsonl"
    payload2 = dict(payload)
    payload2["ts"] = datetime.now(timezone.utc).isoformat()
    with p.open("a", encoding="utf-8") as f:
        f.write(json.dumps(payload2, ensure_ascii=False) + "\n")


def _write_ok(run_dir: Path) -> None:
    (run_dir / "compute-stats.ok").write_text(
        datetime.now(timezone.utc).isoformat(),
        encoding="utf-8",
    )


def _read_mapping_teacher_email(path: Optional[Path]) -> pd.DataFrame:
    """
    Oczekiwane kolumny: teacher_id, email
    """
    if path is None:
        return pd.DataFrame(columns=["teacher_id", "email"])
    if not path.exists():
        raise FileNotFoundError(f"Brak pliku mapowania teacher_id→email: {path}")

    if path.suffix.lower() in [".xlsx", ".xls"]:
        df = pd.read_excel(path, dtype=str)
    else:
        df = pd.read_csv(path, dtype=str)

    df = df.rename(columns={c: c.strip() for c in df.columns})
    # normalizacja nazw (tolerancyjnie)
    cols = {c.lower(): c for c in df.columns}
    tid = cols.get("teacher_id") or cols.get("id") or cols.get("userid")
    eml = cols.get("email") or cols.get("mail")
    if not tid or not eml:
        raise ValueError(f"Plik {path} musi mieć kolumny teacher_id oraz email (lub równoważne).")

    out = df[[tid, eml]].copy()
    out.columns = ["teacher_id", "email"]
    out["teacher_id"] = out["teacher_id"].astype(str).str.strip()
    out["email"] = out["email"].astype(str).str.strip().str.lower()
    out = out.dropna().drop_duplicates()
    return out


def read_hr_table(hr_file: Path, sheet: str | None,
                  email_col: str, full_name_col: str | None,
                  wydzial_col: str | None, jednostka_col: str | None,
                  passthrough_cols: list[str] | None = None) -> pd.DataFrame:
    if not hr_file.exists():
        raise FileNotFoundError(f"Brak pliku HR: {hr_file}")

    if hr_file.suffix.lower() in [".xlsx", ".xls"]:
        df = pd.read_excel(hr_file, sheet_name=sheet or 0, dtype=str)
    else:
        # jeśli kiedyś HR będzie w TSV/CSV
        df = pd.read_csv(hr_file, sep=None, engine="python", dtype=str)

    df = df.rename(columns={c: str(c).strip() for c in df.columns})

    def pick(colname: str | None) -> pd.Series:
        if not colname:
            return pd.Series([""] * len(df))
        if colname not in df.columns:
            raise ValueError(f"HR: brak kolumny '{colname}'. Dostępne: {list(df.columns)}")
        return df[colname].astype(str)

    out = pd.DataFrame()
    out["email"] = pick(email_col).str.strip().str.lower()
    out["full_name"] = pick(full_name_col).str.strip() if full_name_col else ""
    out["wydzial"] = pick(wydzial_col).str.strip() if wydzial_col else ""
    out["jednostka"] = pick(jednostka_col).str.strip() if jednostka_col else ""

    if passthrough_cols:
        for c in passthrough_cols:
            if c not in df.columns:
                raise ValueError(f"HR passthrough: brak kolumny '{c}'")
            out[c] = df[c].astype(str).str.strip()

    out = out[out["email"].notna() & (out["email"] != "")]
    out = out.drop_duplicates(subset=["email"])
    return out


def _read_mapping_email_hr(path: Optional[Path]) -> pd.DataFrame:
    """
    Oczekiwane minimum: email, full_name, wydzial, jednostka
    (kolumny mogą się nazywać inaczej -> dopasowanie tolerancyjne)
    """
    if path is None:
        return pd.DataFrame(columns=["email", "full_name", "wydzial", "jednostka"])
    if not path.exists():
        raise FileNotFoundError(f"Brak pliku mapowania email→HR: {path}")

    if path.suffix.lower() in [".xlsx", ".xls"]:
        df = pd.read_excel(path, dtype=str)
    else:
        df = pd.read_csv(path, dtype=str)

    df = df.rename(columns={c: c.strip() for c in df.columns})
    cols = {c.lower(): c for c in df.columns}

    email = cols.get("email") or cols.get("mail") or cols.get("e-mail")
    full_name = cols.get("full_name") or cols.get("imie_nazwisko") or cols.get("name") or cols.get("nazwiskoimie")
    wydzial = cols.get("wydzial") or cols.get("wydział")
    jednostka = cols.get("jednostka") or cols.get("unit") or cols.get("katedra")

    if not email:
        raise ValueError(f"Plik HR {path} musi mieć kolumnę email/mail/e-mail.")

    out = pd.DataFrame()
    out["email"] = df[email].astype(str).str.strip().str.lower()

    out["full_name"] = df[full_name].astype(str).str.strip() if full_name else ""
    out["wydzial"] = df[wydzial].astype(str).str.strip() if wydzial else ""
    out["jednostka"] = df[jednostka].astype(str).str.strip() if jednostka else ""

    out = out.dropna().drop_duplicates(subset=["email"])
    return out


# ----------------------------
# Main compute
# ----------------------------

def compute_stats(root: Path, ay: Optional[str] = None, term: Optional[str] = None) -> None:
    cfg = _load_config(root)

    # minimalne oczekiwane ścieżki (dopasuj do Twojego config-a)
    duckdb_path = _resolve_path(root, cfg.get("duckdb_path") or cfg.get("warehouse", {}).get("duckdb_path") or "_run/warehouse.duckdb")
    run_dir = _resolve_path(root, cfg.get("run_dir") or "_run") or (root / "_run")

    aggregation = cfg.get("aggregation", {}) or {}
    include_deleted_in_percent = bool(aggregation.get("include_deleted_in_percent", False))
    rebuild_full = bool(cfg.get("rebuild_full", False))

    pct_round_decimals = int(cfg.get("stats", {}).get("pct_round_decimals", 4))

    map_teacher_id_email_path = _resolve_path(root, cfg.get("mapping", {}).get("teacher_id_email"))
    map_email_hr_path = _resolve_path(root, cfg.get("mapping", {}).get("email_hr"))

    # okres: CLI ma pierwszeństwo, potem config
    ay_eff = ay or cfg.get("period", {}).get("ay")
    term_eff = term or cfg.get("period", {}).get("term")
    if not rebuild_full and (not ay_eff or not term_eff):
        raise ValueError("Brak ay/term. Ustaw period.ay + period.term w config.yaml albo podaj w CLI, albo włącz rebuild_full=true.")

    sc = StatsConfig(
        duckdb_path=duckdb_path,
        run_dir=run_dir,
        include_deleted_in_percent=include_deleted_in_percent,
        rebuild_full=rebuild_full,
        map_teacher_id_email_path=map_teacher_id_email_path,
        map_email_hr_path=map_email_hr_path,
        pct_round_decimals=pct_round_decimals,
        ay=ay_eff,
        term=term_eff,
    )

    _ensure_run_artifacts(sc.run_dir)
    _log_progress(sc.run_dir, {"step": "start", "duckdb_path": str(sc.duckdb_path)})

    # wczytaj mapowania (pandas OK, ale tylko jako lookup tables)
    df_tid_email = _read_mapping_teacher_email(sc.map_teacher_id_email_path)
    df_email_hr = _read_mapping_email_hr(sc.map_email_hr_path)

    con = duckdb.connect(str(sc.duckdb_path))
    try:
        con.execute("CREATE SCHEMA IF NOT EXISTS mart;")

        # wstrzyknij mappingi do DuckDB
        con.register("map_tid_email_df", df_tid_email)
        con.register("map_email_hr_df", df_email_hr)

        con.execute("""
            CREATE OR REPLACE TEMP VIEW map_tid_email AS
            SELECT DISTINCT
                teacher_id,
                lower(trim(email)) AS email
            FROM map_tid_email_df
            WHERE teacher_id IS NOT NULL AND teacher_id <> ''
              AND email IS NOT NULL AND email <> '';
        """)

        con.execute("""
            CREATE OR REPLACE TEMP VIEW map_email_hr AS
            SELECT DISTINCT
                lower(trim(email)) AS email,
                nullif(trim(full_name), '') AS full_name,
                nullif(trim(wydzial), '') AS wydzial,
                nullif(trim(jednostka), '') AS jednostka
            FROM map_email_hr_df
            WHERE email IS NOT NULL AND email <> '';
        """)

        # okres filter
        period_where = ""
        if not sc.rebuild_full:
            period_where = "AND ay = ? AND term = ?"

        _log_progress(sc.run_dir, {"step": "prepare_events_period", "rebuild_full": sc.rebuild_full, "ay": sc.ay, "term": sc.term})

        con.execute(f"""
            CREATE OR REPLACE TEMP VIEW events_period AS
            SELECT
                course_code,
                ay,
                term,
                wydzial_code,
                kierunek_code,
                track_code,
                semester_code,
                ts_utc,
                teacher_id,
                operation,
                tech_key,
                activity_label,
                object_id,
                count_mode
            FROM events_canonical
            WHERE counted = true
            {period_where};
        """, ([] if sc.rebuild_full else [sc.ay, sc.term]))

        # join ze stanem aktywności
        _log_progress(sc.run_dir, {"step": "join_activities_state"})

        con.execute("""
            CREATE OR REPLACE TEMP VIEW joined_state AS
            SELECT
                e.*,
                s.status_final,
                s.deleted_at,
                s.visible_last,
                s.confidence_deleted,
                CASE
                    WHEN s.activity_id IS NULL THEN 1 ELSE 0
                END AS qa_missing_state
            FROM events_period e
            LEFT JOIN mart.activities_state s
              ON s.course_code = e.course_code
             AND s.activity_id = e.object_id;
        """)

        # QA: students + missing mappings
        _log_progress(sc.run_dir, {"step": "qa_teacher_mapping"})

        con.execute("""
            CREATE OR REPLACE TEMP VIEW teacher_enriched AS
            SELECT
                j.*,
                m.email,
                h.full_name,
                h.wydzial AS hr_wydzial,
                h.jednostka AS hr_jednostka,
                CASE WHEN m.email ILIKE '%@student.umw.edu.pl' THEN 1 ELSE 0 END AS is_student,
                CASE WHEN m.email IS NULL THEN 1 ELSE 0 END AS qa_missing_email,
                CASE WHEN m.email IS NOT NULL AND h.email IS NULL THEN 1 ELSE 0 END AS qa_missing_hr
            FROM joined_state j
            LEFT JOIN map_tid_email m
              ON m.teacher_id = j.teacher_id
            LEFT JOIN map_email_hr h
              ON h.email = m.email;
        """)

        # QA table init
        con.execute("""
            CREATE TABLE IF NOT EXISTS mart.metrics_qa (
                ay VARCHAR,
                term VARCHAR,
                qa_type VARCHAR,
                teacher_id VARCHAR,
                course_code VARCHAR,
                tech_key VARCHAR,
                object_id VARCHAR,
                details VARCHAR,
                created_at TIMESTAMP
            );
        """)

        # incremental delete for qa for period (opcjonalnie)
        if not sc.rebuild_full:
            con.execute("DELETE FROM mart.metrics_qa WHERE ay = ? AND term = ?;", [sc.ay, sc.term])

        # wpisy QA: student ignored
        con.execute("""
            INSERT INTO mart.metrics_qa
            SELECT
                ay, term,
                'STUDENT_IGNORED' AS qa_type,
                teacher_id, course_code, tech_key, object_id,
                'email=' || coalesce(email,'') AS details,
                now() AS created_at
            FROM teacher_enriched
            WHERE is_student = 1;
        """)

        # QA: brak email dla teacher_id
        con.execute("""
            INSERT INTO mart.metrics_qa
            SELECT
                ay, term,
                'MISSING_EMAIL_MAPPING' AS qa_type,
                teacher_id, course_code, tech_key, object_id,
                'teacher_id has no email mapping' AS details,
                now() AS created_at
            FROM teacher_enriched
            WHERE is_student = 0 AND qa_missing_email = 1;
        """)

        # QA: brak HR
        con.execute("""
            INSERT INTO mart.metrics_qa
            SELECT
                ay, term,
                'MISSING_HR_MAPPING' AS qa_type,
                teacher_id, course_code, tech_key, object_id,
                'email=' || coalesce(email,'') AS details,
                now() AS created_at
            FROM teacher_enriched
            WHERE is_student = 0 AND qa_missing_email = 0 AND qa_missing_hr = 1;
        """)

        # QA: event bez wpisu w activities_state
        con.execute("""
            INSERT INTO mart.metrics_qa
            SELECT
                ay, term,
                'EVENT_WITHOUT_ACTIVITY_STATE' AS qa_type,
                teacher_id, course_code, tech_key, object_id,
                'no activities_state row for object_id' AS details,
                now() AS created_at
            FROM teacher_enriched
            WHERE qa_missing_state = 1;
        """)

        # QA: confidence_deleted < 1
        con.execute("""
            INSERT INTO mart.metrics_qa
            SELECT
                ay, term,
                'CONFIDENCE_LT_1' AS qa_type,
                teacher_id, course_code, tech_key, object_id,
                'confidence_deleted=' || cast(confidence_deleted AS VARCHAR) AS details,
                now() AS created_at
            FROM teacher_enriched
            WHERE confidence_deleted IS NOT NULL AND confidence_deleted < 1;
        """)

        # baza do metryk: tylko teachers z HR + nie-studenci
        con.execute("""
            CREATE OR REPLACE TEMP VIEW teacher_ok AS
            SELECT *
            FROM teacher_enriched
            WHERE is_student = 0
              AND qa_missing_email = 0
              AND qa_missing_hr = 0;
        """)

        # widoczne do procentów
        con.execute("""
            CREATE OR REPLACE TEMP VIEW visible_ok AS
            SELECT *
            FROM teacher_ok
            WHERE status_final = 'visible_active';
        """)

        # agregaty QA (deleted/hidden/unknown) per teacher/course/tech_key
        # (tu zakładam status_final wartości: visible_active / deleted / hidden / unknown; dopasuj jeśli inne)
        con.execute("""
            CREATE OR REPLACE TEMP VIEW qa_counts AS
            SELECT
                ay, term,
                teacher_id,
                course_code,
                tech_key,
                SUM(CASE WHEN status_final = 'deleted' THEN 1 ELSE 0 END) AS deleted_count,
                SUM(CASE WHEN status_final = 'hidden' THEN 1 ELSE 0 END) AS hidden_count,
                SUM(CASE WHEN status_final IS NULL OR status_final = 'unknown' THEN 1 ELSE 0 END) AS unknown_count,
                MAX(CASE WHEN confidence_deleted IS NOT NULL AND confidence_deleted < 1 THEN 1 ELSE 0 END) AS confidence_flag
            FROM teacher_ok
            GROUP BY 1,2,3,4,5;
        """)

        # counts (widoczne) z uwzględnieniem count_mode
        # UWAGA: zakładam, że w obrębie (teacher, course, tech_key) count_mode jest spójny.
        con.execute("""
            CREATE OR REPLACE TEMP VIEW counts_visible AS
            SELECT
                ay, term,
                teacher_id,
                course_code,
                wydzial_code,
                kierunek_code,
                tech_key,
                any_value(activity_label) AS activity_label,
                CASE
                    WHEN max(count_mode) = 'object-based' THEN COUNT(DISTINCT object_id)
                    ELSE COUNT(*)
                END AS count_value,
                CASE WHEN min(count_mode) <> max(count_mode) THEN 1 ELSE 0 END AS qa_mixed_count_mode
            FROM visible_ok
            GROUP BY 1,2,3,4,5,6,7;
        """)

        # QA: mixed count_mode
        con.execute("""
            INSERT INTO mart.metrics_qa
            SELECT
                ay, term,
                'MIXED_COUNT_MODE' AS qa_type,
                teacher_id, course_code, tech_key, NULL AS object_id,
                'min!=max count_mode in group' AS details,
                now() AS created_at
            FROM counts_visible
            WHERE qa_mixed_count_mode = 1;
        """)

        # procenty - tylko visible_active
        # pct_course: sum per course_code+tech_key
        # pct_kierunek: sum per kierunek_code+tech_key
        # pct_wydzial: sum per wydzial_code+tech_key
        # pct_uczelnia: sum per ay+term+tech_key
        con.execute(f"""
            CREATE OR REPLACE TEMP VIEW metrics_core AS
            SELECT
                c.ay, c.term,
                c.teacher_id,
                c.course_code,
                c.wydzial_code,
                c.kierunek_code,
                c.tech_key,
                c.activity_label,
                c.count_value,

                ROUND(
                    c.count_value
                    / NULLIF(SUM(c.count_value) OVER (PARTITION BY c.ay, c.term, c.course_code, c.tech_key), 0),
                    {sc.pct_round_decimals}
                ) AS pct_course,

                ROUND(
                    c.count_value
                    / NULLIF(SUM(c.count_value) OVER (PARTITION BY c.ay, c.term, c.kierunek_code, c.tech_key), 0),
                    {sc.pct_round_decimals}
                ) AS pct_kierunek,

                ROUND(
                    c.count_value
                    / NULLIF(SUM(c.count_value) OVER (PARTITION BY c.ay, c.term, c.wydzial_code, c.tech_key), 0),
                    {sc.pct_round_decimals}
                ) AS pct_wydzial,

                ROUND(
                    c.count_value
                    / NULLIF(SUM(c.count_value) OVER (PARTITION BY c.ay, c.term, c.tech_key), 0),
                    {sc.pct_round_decimals}
                ) AS pct_uczelnia
            FROM counts_visible c;
        """)

        # metrics_long table
        con.execute("""
            CREATE TABLE IF NOT EXISTS mart.metrics_long (
                ay VARCHAR,
                term VARCHAR,
                teacher_id VARCHAR,
                full_name VARCHAR,
                email VARCHAR,
                wydzial VARCHAR,
                jednostka VARCHAR,
                course_code VARCHAR,
                tech_key VARCHAR,
                activity_label VARCHAR,
                count_value BIGINT,
                pct_course DOUBLE,
                pct_kierunek DOUBLE,
                pct_wydzial DOUBLE,
                pct_uczelnia DOUBLE,
                deleted_count BIGINT,
                hidden_count BIGINT,
                unknown_count BIGINT,
                confidence_flag BOOLEAN
            );
        """)

        # incremental delete for long for period
        if not sc.rebuild_full:
            _log_progress(sc.run_dir, {"step": "incremental_delete_long", "ay": sc.ay, "term": sc.term})
            con.execute("DELETE FROM mart.metrics_long WHERE ay = ? AND term = ?;", [sc.ay, sc.term])
        else:
            _log_progress(sc.run_dir, {"step": "rebuild_full_long"})
            con.execute("DELETE FROM mart.metrics_long;")

        # insert long = core + HR + QA counts
        _log_progress(sc.run_dir, {"step": "insert_metrics_long"})

        con.execute("""
            INSERT INTO mart.metrics_long
            SELECT
                mc.ay,
                mc.term,
                mc.teacher_id,
                h.full_name,
                m.email,
                h.hr_wydzial AS wydzial,
                h.hr_jednostka AS jednostka,
                mc.course_code,
                mc.tech_key,
                mc.activity_label,
                mc.count_value,
                mc.pct_course,
                mc.pct_kierunek,
                mc.pct_wydzial,
                mc.pct_uczelnia,
                coalesce(q.deleted_count, 0) AS deleted_count,
                coalesce(q.hidden_count, 0) AS hidden_count,
                coalesce(q.unknown_count, 0) AS unknown_count,
                coalesce(q.confidence_flag, 0) = 1 AS confidence_flag
            FROM metrics_core mc
            JOIN (
                SELECT DISTINCT teacher_id, email FROM map_tid_email
            ) m ON m.teacher_id = mc.teacher_id
            JOIN (
                SELECT DISTINCT
                    t.teacher_id,
                    t.email,
                    t.full_name,
                    t.hr_wydzial,
                    t.hr_jednostka
                FROM teacher_ok t
            ) h ON h.teacher_id = mc.teacher_id
            LEFT JOIN qa_counts q
              ON q.ay = mc.ay AND q.term = mc.term
             AND q.teacher_id = mc.teacher_id
             AND q.course_code = mc.course_code
             AND q.tech_key = mc.tech_key;
        """)

        # metrics_wide table (dynamic columns)
        con.execute("""
            CREATE TABLE IF NOT EXISTS mart.metrics_wide (
                ay VARCHAR,
                term VARCHAR,
                teacher_id VARCHAR,
                course_code VARCHAR
                -- dynamic columns added by INSERT SELECT (DuckDB allows it if table has those cols; so we recreate per period)
            );
        """)

        # dla wide: lepiej robić per okres -> create/replace temp wide i potem zapisać do tabeli partycjonowanej
        # w DuckDB najprościej: skasować okres i wstawić wynik z dynamicznego SELECT do "mart.metrics_wide_period",
        # a potem zmergować. Tu robię wersję: trzymamy wide jako "append-only per period" i usuwamy tylko okres.
        if not sc.rebuild_full:
            _log_progress(sc.run_dir, {"step": "incremental_delete_wide", "ay": sc.ay, "term": sc.term})
            con.execute("DELETE FROM mart.metrics_wide WHERE ay = ? AND term = ?;", [sc.ay, sc.term])
        else:
            con.execute("DELETE FROM mart.metrics_wide;")

        # pobierz tech_key dla okresu
        tech_keys: Sequence[str] = [r[0] for r in con.execute("""
            SELECT DISTINCT tech_key
            FROM mart.metrics_long
            WHERE ay = ? AND term = ?
            ORDER BY tech_key;
        """, [sc.ay, sc.term]).fetchall()] if not sc.rebuild_full else [r[0] for r in con.execute("""
            SELECT DISTINCT tech_key FROM mart.metrics_long ORDER BY tech_key;
        """).fetchall()]

        _log_progress(sc.run_dir, {"step": "build_metrics_wide", "tech_keys": list(tech_keys)})

        # dynamiczne kolumny (count_*, pct_course_* ...)
        # UWAGA: nazwy kolumn muszą być bezpieczne -> tech_key normalizujemy do [a-zA-Z0-9_]
        def safe_col(s: str) -> str:
            out = []
            for ch in s:
                if ch.isalnum():
                    out.append(ch)
                else:
                    out.append("_")
            return "".join(out)

        select_cols = ["ay", "term", "teacher_id", "course_code"]
        for tk in tech_keys:
            c = safe_col(tk)
            select_cols.append(f"MAX(CASE WHEN tech_key='{tk}' THEN count_value END) AS count_{c}")
            select_cols.append(f"MAX(CASE WHEN tech_key='{tk}' THEN pct_course END) AS pct_course_{c}")
            select_cols.append(f"MAX(CASE WHEN tech_key='{tk}' THEN pct_kierunek END) AS pct_kierunek_{c}")
            select_cols.append(f"MAX(CASE WHEN tech_key='{tk}' THEN pct_wydzial END) AS pct_wydzial_{c}")
            select_cols.append(f"MAX(CASE WHEN tech_key='{tk}' THEN pct_uczelnia END) AS pct_uczelnia_{c}")

        wide_sql = f"""
            INSERT INTO mart.metrics_wide
            SELECT
                {", ".join(select_cols)}
            FROM mart.metrics_long
            {"WHERE ay = ? AND term = ?" if not sc.rebuild_full else ""}
            GROUP BY ay, term, teacher_id, course_code;
        """

        if not sc.rebuild_full:
            con.execute(wide_sql, [sc.ay, sc.term])
        else:
            con.execute(wide_sql)

        # artefakty
        _log_progress(sc.run_dir, {"step": "done"})
        _write_ok(sc.run_dir)

    finally:
        con.close()

================================================================================
mrna_plum\store\__init__.py
================================================================================

from .duckdb_store import DuckDbStore
__all__ = ["DuckDbStore"]


================================================================================
mrna_plum\store\database.py
================================================================================

# src/mrna_plum/store/database.py

from __future__ import annotations

from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional

import pandas as pd


class EventStore:
    def __init__(self, cfg: Dict[str, Any]):
        self.cfg = cfg
        self.db_path = Path(cfg["paths"]["db_path"])
        self.parquet_root = Path(cfg["paths"]["parquet_root"]) if cfg.get("paths", {}).get("parquet_root") else None

    def _connect(self):
        import duckdb
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        return duckdb.connect(str(self.db_path))

    def ensure_schema(self) -> None:
        con = self._connect()
        try:
            con.execute(
                """
                CREATE TABLE IF NOT EXISTS events_canonical_raw (
                    row_key VARCHAR,
                    course VARCHAR,
                    course_code VARCHAR,
                    wydzial_code VARCHAR,
                    kierunek_code VARCHAR,
                    track_code VARCHAR,
                    semester_code VARCHAR,
                    course_name VARCHAR,
                    ay VARCHAR,
                    term VARCHAR,
                    ts_utc TIMESTAMP,
                    teacher_id BIGINT,
                    operation VARCHAR,
                    tech_key VARCHAR,
                    activity_label VARCHAR,
                    object_id BIGINT,
                    count_mode VARCHAR,
                    raw_line_hash VARCHAR,
                    source_file VARCHAR,
                    payload_json VARCHAR
                );
                """
            )
            con.execute(
                """
                CREATE TABLE IF NOT EXISTS events_conflicts (
                    row_key VARCHAR,
                    course_code VARCHAR,
                    teacher_id BIGINT,
                    tech_key VARCHAR,
                    operation VARCHAR,
                    object_id BIGINT,
                    note VARCHAR
                );
                """
            )
            con.execute(
                """
                CREATE TABLE IF NOT EXISTS events_canonical (
                    -- finalna tabela do statystyk
                    row_key VARCHAR,
                    course VARCHAR,
                    course_code VARCHAR,
                    wydzial_code VARCHAR,
                    kierunek_code VARCHAR,
                    track_code VARCHAR,
                    semester_code VARCHAR,
                    course_name VARCHAR,
                    ay VARCHAR,
                    term VARCHAR,
                    ts_utc TIMESTAMP,
                    teacher_id BIGINT,
                    operation VARCHAR,
                    tech_key VARCHAR,
                    activity_label VARCHAR,
                    object_id BIGINT,
                    count_mode VARCHAR,
                    counted BOOLEAN,
                    raw_line_hash VARCHAR,
                    source_file VARCHAR
                );
                """
            )
            # indeksy logiczne / dedup incremental
            con.execute("CREATE UNIQUE INDEX IF NOT EXISTS ux_events_canonical_raw_rowkey ON events_canonical_raw(row_key);")
            con.execute("CREATE UNIQUE INDEX IF NOT EXISTS ux_events_canonical_rowkey ON events_canonical(row_key);")
        finally:
            con.close()

    def insert_raw_batch(self, rows: List[Dict[str, Any]]) -> int:
        if not rows:
            return 0
        con = self._connect()
        try:
            df = pd.DataFrame(rows)
            con.register("df_batch", df)
            # INSERT OR IGNORE po unique index (duckdb: użyj anti-join)
            con.execute(
                """
                INSERT INTO events_canonical_raw
                SELECT b.*
                FROM df_batch b
                LEFT JOIN events_canonical_raw e ON e.row_key = b.row_key
                WHERE e.row_key IS NULL;
                """
            )
            return len(rows)
        finally:
            con.close()

    def insert_conflicts_batch(self, rows: List[Dict[str, Any]]) -> int:
        if not rows:
            return 0
        con = self._connect()
        try:
            df = pd.DataFrame(rows)
            con.register("df_conf", df)
            con.execute("INSERT INTO events_conflicts SELECT * FROM df_conf;")
            return len(rows)
        finally:
            con.close()

    def finalize_canonical(self) -> int:
        """
        Zasada:
        - count_mode TAK_FLAG + TAK dla tego samego (course_code, teacher_id, tech_key, object_id)
          -> counted = false dla tych zdarzeń (unieważnienie)
        - jeśli object_id IS NULL -> liczymy event-based: counted=true jeśli count_mode='TAK'
        """
        con = self._connect()
        try:
            # przetwarzaj tylko nowe row_key
            con.execute(
                """
                INSERT INTO events_canonical
                WITH base AS (
                    SELECT r.*
                    FROM events_canonical_raw r
                    LEFT JOIN events_canonical c ON c.row_key = r.row_key
                    WHERE c.row_key IS NULL
                ),
                has_both AS (
                    SELECT
                        course_code,
                        teacher_id,
                        tech_key,
                        object_id,
                        MAX(CASE WHEN upper(count_mode)='TAK' THEN 1 ELSE 0 END) AS has_tak,
                        MAX(CASE WHEN upper(count_mode)='TAK_FLAG' THEN 1 ELSE 0 END) AS has_flag
                    FROM base
                    WHERE object_id IS NOT NULL
                    GROUP BY 1,2,3,4
                )
                SELECT
                    b.row_key,
                    b.course,
                    b.course_code,
                    b.wydzial_code,
                    b.kierunek_code,
                    b.track_code,
                    b.semester_code,
                    b.course_name,
                    b.ay,
                    b.term,
                    b.ts_utc,
                    b.teacher_id,
                    b.operation,
                    b.tech_key,
                    b.activity_label,
                    b.object_id,
                    b.count_mode,
                    CASE
                        WHEN b.object_id IS NULL THEN (upper(b.count_mode)='TAK')
                        ELSE (
                            NOT EXISTS (
                                SELECT 1 FROM has_both hb
                                WHERE hb.course_code=b.course_code
                                  AND hb.teacher_id=b.teacher_id
                                  AND hb.tech_key=b.tech_key
                                  AND hb.object_id=b.object_id
                                  AND hb.has_tak=1 AND hb.has_flag=1
                            )
                            AND (upper(b.count_mode)='TAK')
                        )
                    END AS counted,
                    b.raw_line_hash,
                    b.source_file
                FROM base b;
                """
            )

            # duckdb nie zwraca rowcount wprost stabilnie; policzmy różnicę
            res = con.execute("SELECT COUNT(*) FROM events_canonical;").fetchone()
            return int(res[0]) if res else 0
        finally:
            con.close()

    def export_parquet(self) -> Optional[Path]:
        if not self.parquet_root:
            return None
        out = self.parquet_root / "events_canonical.parquet"
        self.parquet_root.mkdir(parents=True, exist_ok=True)
        con = self._connect()
        try:
            con.execute(f"COPY (SELECT * FROM events_canonical) TO '{str(out).replace('\\\\', '/')}' (FORMAT PARQUET);")
            return out
        finally:
            con.close()

================================================================================
mrna_plum\store\duckdb_store.py
================================================================================

from __future__ import annotations
from pathlib import Path
import duckdb


import json
import hashlib
from dataclasses import dataclass
from typing import Iterable, Optional

class DuckDbStore:
    def __init__(self, db_path: Path):
        db_path.parent.mkdir(parents=True, exist_ok=True)
        self.db_path = db_path

    def connect(self) -> duckdb.DuckDBPyConnection:
        return duckdb.connect(str(self.db_path))

    def init_schema(self) -> None:
        with self.connect() as con:
            con.execute("""
                CREATE TABLE IF NOT EXISTS raw_logs (
                    _source_file VARCHAR,
                    "Czas" VARCHAR,
                    "Kontekst zdarzenia" VARCHAR,
                    "Opis" VARCHAR,
                    "Składnik" VARCHAR,
                    "Nazwa zdarzenia" VARCHAR,
                    course_code VARCHAR,
                    course_id BIGINT,
                    period VARCHAR,
                    tech_key VARCHAR,
                    activity VARCHAR,
                    operation VARCHAR,
                    count_to_report BOOLEAN,
                    teacher_id VARCHAR,
                    object_id VARCHAR,
                    rule_priority INTEGER
                );
            """)
            con.execute("""
                CREATE TABLE IF NOT EXISTS stats_agg (
                    period VARCHAR,
                    course_code VARCHAR,
                    teacher_id VARCHAR,
                    tech_key VARCHAR,
                    cnt_events BIGINT,
                    cnt_objects BIGINT,
                    is_invalidated BOOLEAN
                );
            """)

    def load_parquet_to_raw(self, parquet_path: Path) -> None:
        with self.connect() as con:
            # prosty append; w praktyce możesz TRUNCATE per-run
            con.execute("INSERT INTO raw_logs SELECT * FROM read_parquet(?)", [str(parquet_path)])



@dataclass(frozen=True)
class EventRawRow:
    course: str
    course_id: Optional[int]
    time_text: Optional[str]
    time_ts_iso: Optional[str]  # ISO string (UTC/naive) lub None
    row_key: str                # sha256(normalized_fields_join)
    payload_json: str           # JSON dict: header->value
    source_file: str            # pełna ścieżka lub nazwa


def _connect(db_path: Path) -> duckdb.DuckDBPyConnection:
    db_path.parent.mkdir(parents=True, exist_ok=True)
    con = duckdb.connect(str(db_path))
    # bezpieczne ustawienia, brak wymogów
    return con


def ensure_schema(con: duckdb.DuckDBPyConnection) -> None:
    con.execute(
        """
        CREATE TABLE IF NOT EXISTS events_raw (
            course       TEXT NOT NULL,
            time_text    TEXT,
            time_ts      TIMESTAMP,
            row_key      TEXT NOT NULL,
            payload_json TEXT NOT NULL,
            source_file  TEXT NOT NULL,
            inserted_at  TIMESTAMP DEFAULT now()
        );
        """
    )
    # indeksy opcjonalnie (DuckDB "CREATE INDEX" działa w nowszych wersjach)
    try:
        con.execute("CREATE INDEX IF NOT EXISTS idx_events_raw_course ON events_raw(course);")
    except Exception:
        pass
    try:
        con.execute("CREATE INDEX IF NOT EXISTS idx_events_raw_rowkey ON events_raw(row_key);")
    except Exception:
        pass


def create_stage_table(con: duckdb.DuckDBPyConnection) -> None:
    con.execute("DROP TABLE IF EXISTS _events_raw_stage;")
    con.execute(
        """
        CREATE TEMP TABLE _events_raw_stage (
            course       TEXT NOT NULL,
            time_text    TEXT,
            time_ts      TIMESTAMP,
            row_key      TEXT NOT NULL,
            payload_json TEXT NOT NULL,
            source_file  TEXT NOT NULL
        );
        """
    )


def insert_stage_rows(con: duckdb.DuckDBPyConnection, rows: list[EventRawRow]) -> None:
    if not rows:
        return
    params = [
        (
            r.course,
            r.time_text,
            r.time_ts_iso,  # DuckDB potrafi zrzucić ISO->TIMESTAMP
            r.row_key,
            r.payload_json,
            r.source_file,
        )
        for r in rows
    ]
    con.executemany(
        """
        INSERT INTO _events_raw_stage(course, time_text, time_ts, row_key, payload_json, source_file)
        VALUES (?, ?, ?, ?, ?, ?);
        """,
        params,
    )


def merge_stage_into_events_raw(con: duckdb.DuckDBPyConnection) -> int:
    """
    Dedup: tylko jeśli CAŁY wiersz identyczny (po Trim, bez BOM, bez CR).
    My realizujemy to przez:
      - payload_json z trimowanymi wartościami
      - row_key = sha256(normalized_fields_join)
    Wstawiamy tylko jeśli (course,row_key,payload_json) nie istnieje.
    """
    res = con.execute(
        """
        INSERT INTO events_raw(course, time_text, time_ts, row_key, payload_json, source_file)
        SELECT s.course, s.time_text, s.time_ts, s.row_key, s.payload_json, s.source_file
        FROM _events_raw_stage s
        LEFT JOIN events_raw e
          ON e.course = s.course
         AND e.row_key = s.row_key
         AND e.payload_json = s.payload_json
        WHERE e.course IS NULL;
        """
    )
    # DuckDB python: rowcount by cursor.rowcount is not always reliable; use changes()
    try:
        return con.execute("SELECT changes();").fetchone()[0]
    except Exception:
        return 0


def open_store(db_path: Path) -> duckdb.DuckDBPyConnection:
    con = _connect(db_path)
    ensure_schema(con)
    return con


def export_course_to_csv(
    con: duckdb.DuckDBPyConnection,
    *,
    course: str,
    out_csv: Path,
) -> None:
    """
    CSV-compat: zapis *_full_log.csv posortowany po time_ts malejąco (jeśli jest),
    w przeciwnym razie stabilnie po inserted_at.
    Zapisujemy same payload_json jako jedną kolumnę? – NIE: eksportujemy jako CSV z kolumną payload_json.
    (Jeśli potrzebujesz 1:1 zgodności z VBA-headerami, da się to rozwinąć później na etapie parse.)
    """
    out_csv.parent.mkdir(parents=True, exist_ok=True)
    con.execute(
        f"""
        COPY (
            SELECT course, time_text, time_ts, payload_json, source_file
            FROM events_raw
            WHERE course = ?
            ORDER BY
              CASE WHEN time_ts IS NULL THEN 1 ELSE 0 END,
              time_ts DESC,
              inserted_at DESC
        )
        TO ?
        (HEADER, DELIMITER ';', QUOTE '"', ESCAPE '"');
        """,
        [course, str(out_csv)],
    )


def export_course_to_parquet(
    con: duckdb.DuckDBPyConnection,
    *,
    course: str,
    out_parquet: Path,
) -> None:
    out_parquet.parent.mkdir(parents=True, exist_ok=True)
    con.execute(
        """
        COPY (
            SELECT course, time_text, time_ts, payload_json, source_file, inserted_at
            FROM events_raw
            WHERE course = ?
            ORDER BY
              CASE WHEN time_ts IS NULL THEN 1 ELSE 0 END,
              time_ts DESC,
              inserted_at DESC
        )
        TO ?
        (FORMAT PARQUET);
        """,
        [course, str(out_parquet)],    )


================================================================================
mrna_plum\tests\__init__.py
================================================================================



================================================================================
mrna_plum\tests\test_build_activities_state.py
================================================================================

from __future__ import annotations

from datetime import datetime
import duckdb
import pytest

from mrna_plum.activities.activities_state import (
    BuildConfig, DeletionConfig, MappingConfig, IncrementalConfig, build_activities_state
)

def _cfg(**kw):
    return BuildConfig(
        deletion=DeletionConfig(
            delete_operations=kw.get("delete_operations", ["DELETE"]),
            delete_tech_keys=kw.get("delete_tech_keys", []),
            delete_activity_labels_regex=[],
            disappearance_grace_period_days=kw.get("grace", 14),
            min_missing_snapshots_to_confirm=kw.get("min_missing", 2),
            deleted_at_policy=kw.get("policy", "first_missing"),
        ),
        mapping=MappingConfig(
            use_activity_id_map_table=True,
            allow_fuzzy_name_type_match=False,
        ),
        incremental=IncrementalConfig(
            checkpoint_table="raw.pipeline_checkpoints",
            checkpoint_key="build_activities_state",
            process_only_new_snapshots=False,
            process_only_new_events=False,
        ),
    )

def setup_base(con):
    con.execute("create schema if not exists raw;")
    con.execute("create schema if not exists mart;")

    con.execute("""
      create table events_canonical (
        course_code varchar,
        ay varchar,
        term varchar,
        wydzial_code varchar,
        kierunek_code varchar,
        track_code varchar,
        semester_code varchar,
        ts_utc timestamp,
        teacher_id varchar,
        operation varchar,
        tech_key varchar,
        activity_label varchar,
        object_id varchar,
        count_mode varchar,
        row_key varchar,
        source_file varchar,
        counted boolean
      );
    """)
    con.execute("""
      create table raw.activities_snapshot(
        course_code varchar,
        activity_id varchar,
        name varchar,
        type varchar,
        visible_to_students boolean,
        captured_at timestamp,
        source_file varchar,
        row_key varchar
      );
    """)
    con.execute("""
      create table raw.activity_id_map(
        course_code varchar,
        activity_id varchar,
        object_id varchar,
        map_method varchar,
        confidence double,
        first_seen_at timestamp,
        last_seen_at timestamp,
        primary key(course_code, activity_id)
      );
    """)

def test_delete_from_logs():
    con = duckdb.connect(":memory:")
    setup_base(con)

    con.execute("""
      insert into raw.activities_snapshot values
      ('C1','101','Quiz 1','quiz', true, '2026-02-01 10:00:00','f.csv','rk1'),
      ('C1','101','Quiz 1','quiz', true, '2026-02-10 10:00:00','f.csv','rk2')
    """)

    con.execute("""
      insert into events_canonical values
      ('C1','2025/26','Z','W1','K1','T1','S1','2026-02-05 12:00:00','U1','DELETE','tk_del','', '101','object-based','e1','e.csv', true)
    """)

    stats = build_activities_state(con, _cfg(delete_operations=["DELETE"]))
    row = con.execute("select status_final, evidence_deleted, deleted_at from mart.activities_state where course_code='C1' and activity_id='101'").fetchone()
    assert row[0] == "visible_deleted"
    assert row[1] in ("log_delete_event", "both")
    assert row[2] is not None

def test_disappearance_from_snapshots():
    con = duckdb.connect(":memory:")
    setup_base(con)

    # snapshoty kursu: aktywność znika po 2026-02-01, potem mamy 2 snapshoty bez niej i grace spełniony
    con.execute("""
      insert into raw.activities_snapshot values
      ('C1','200','Page A','page', true, '2026-02-01 10:00:00','f.csv','a1'),
      ('C1','201','Other','page', true, '2026-02-08 10:00:00','f.csv','a2'),
      ('C1','201','Other','page', true, '2026-02-20 10:00:00','f.csv','a3')
    """)
    # brak eventów

    stats = build_activities_state(con, _cfg(grace=7, min_missing=2, policy="first_missing"))
    row = con.execute("select status_final, evidence_deleted from mart.activities_state where course_code='C1' and activity_id='200'").fetchone()
    assert row[1] == "snapshot_disappearance"
    assert row[0] == "visible_deleted"

def test_hidden():
    con = duckdb.connect(":memory:")
    setup_base(con)

    con.execute("""
      insert into raw.activities_snapshot values
      ('C1','300','Forum','forum', false, '2026-02-20 10:00:00','f.csv','h1')
    """)
    stats = build_activities_state(con, _cfg())
    row = con.execute("select status_final from mart.activities_state where course_code='C1' and activity_id='300'").fetchone()
    assert row[0] == "hidden"

def test_conflict_log_delete_but_snapshot_visible():
    con = duckdb.connect(":memory:")
    setup_base(con)

    con.execute("""
      insert into raw.activities_snapshot values
      ('C1','400','H5P','h5p', true, '2026-02-20 10:00:00','f.csv','c1')
    """)
    con.execute("""
      insert into events_canonical values
      ('C1','2025/26','Z','W1','K1','T1','S1','2026-02-10 12:00:00','U1','DELETE','tk_del','', '400','object-based','e1','e.csv', true)
    """)
    build_activities_state(con, _cfg(delete_operations=["DELETE"]))
    qa = con.execute("select count(*) from mart.activities_qa where qa_type='conflict_log_delete_but_visible_in_snapshot'").fetchone()[0]
    assert qa >= 1

def test_missing_mapping_activity_to_object():
    con = duckdb.connect(":memory:")
    setup_base(con)

    # snapshot istnieje, ale w events brak object_id/akcji
    con.execute("""
      insert into raw.activities_snapshot values
      ('C1','500','URL','url', true, '2026-02-20 10:00:00','f.csv','m1')
    """)
    build_activities_state(con, _cfg())
    qa = con.execute("select count(*) from mart.activities_qa where qa_type='activity_without_object_id_mapping'").fetchone()[0]
    assert qa >= 1

================================================================================
mrna_plum\tests\test_compute_stats.py
================================================================================

import duckdb
from pathlib import Path
from mrna_plum.stats.compute_stats import compute_stats

def test_pct_course_two_teachers(tmp_path: Path):
    db = tmp_path / "w.duckdb"
    con = duckdb.connect(str(db))

    con.execute("CREATE SCHEMA mart;")

    con.execute("""
        CREATE TABLE events_canonical (
            course_code VARCHAR, ay VARCHAR, term VARCHAR,
            wydzial_code VARCHAR, kierunek_code VARCHAR, track_code VARCHAR, semester_code VARCHAR,
            ts_utc TIMESTAMP, teacher_id VARCHAR, operation VARCHAR,
            tech_key VARCHAR, activity_label VARCHAR, object_id VARCHAR, count_mode VARCHAR,
            counted BOOLEAN
        );
    """)

    con.execute("""
        CREATE TABLE mart.activities_state (
            course_code VARCHAR, activity_id VARCHAR,
            status_final VARCHAR, deleted_at TIMESTAMP,
            visible_last BOOLEAN, confidence_deleted DOUBLE
        );
    """)

    # 2 nauczycieli, ta sama aktywność "PAGE"
    con.execute("""
        INSERT INTO events_canonical VALUES
        ('C1','2025/26','Z','W1','K1','T1','S1', now(), 'T_A','CREATE','PAGE','Strona','10','object-based', true),
        ('C1','2025/26','Z','W1','K1','T1','S1', now(), 'T_B','CREATE','PAGE','Strona','11','object-based', true);
    """)

    con.execute("""
        INSERT INTO mart.activities_state VALUES
        ('C1','10','visible_active', NULL, true, 1.0),
        ('C1','11','visible_active', NULL, true, 1.0);
    """)

    # mapping
    # tu najprościej: zrobisz pliki CSV/XLSX w tmp_path i wskażesz config.yaml
    # ... (pomijam dla czytelności – ale test ma sprawdzić, że oba wejdą do long)
    con.close()

    # prepare root with config + mapping files, then compute_stats(root)
    # then assert pct_course = 0.5 and 0.5

================================================================================
mrna_plum\tests\test_export_excel.py
================================================================================

import duckdb
import pytest
from pathlib import Path

from mrna_plum.reports.export_excel import export_summary_excel, ExportOverflowError


def _mk_con_with_tables():
    con = duckdb.connect(":memory:")
    con.execute("CREATE SCHEMA mart;")

    con.execute("""
        CREATE TABLE mart.metrics_long (
            full_name VARCHAR,
            teacher_id VARCHAR,
            course_code VARCHAR,
            tech_key VARCHAR,
            activity_label VARCHAR,
            count_value BIGINT,
            pct_course DOUBLE,
            pct_program DOUBLE,
            pct_faculty DOUBLE,
            pct_university DOUBLE,
            visible_active BOOLEAN
        );
    """)

    con.execute("""
        CREATE TABLE mart.metrics_qa (
            type VARCHAR,
            teacher_id VARCHAR,
            course_code VARCHAR,
            tech_key VARCHAR,
            description VARCHAR
        );
    """)
    return con


def _cfg(tmp_root: Path, **overrides):
    cfg = {
        "root": str(tmp_root),
        "report": {"ay": "2025_2026", "term": "Z"},
        "export": {"max_rows_excel": 1_000_000, "overflow_strategy": "error", "activity_column": "tech_key"},
    }
    # shallow merge
    for k, v in overrides.items():
        if k in cfg and isinstance(cfg[k], dict) and isinstance(v, dict):
            cfg[k].update(v)
        else:
            cfg[k] = v
    return cfg


def test_generates_file_when_data_exists(tmp_path: Path):
    con = _mk_con_with_tables()
    con.execute("""
        INSERT INTO mart.metrics_long VALUES
        ('Anna Nowak','10','BIO101','PAGE','Strona',3,12.3,4.5,1.1,0.2, TRUE),
        ('Anna Nowak','10','BIO101','URL','Adres URL',1, 1.0,0.5,0.2,0.1, TRUE);
    """)
    con.execute("INSERT INTO mart.metrics_qa VALUES ('teacher_id NOT_IN_HR','999','BIO101','PAGE','no HR');")

    cfg = _cfg(tmp_path)
    code, out_path = export_summary_excel(con, cfg)

    assert code == 0
    assert out_path.exists()
    assert (tmp_path / "_run" / "export-excel.ok").exists()
    assert (tmp_path / "_run" / "run.log").exists()
    assert (tmp_path / "_run" / "progress.jsonl").exists()


def test_generates_correct_row_count(tmp_path: Path):
    con = _mk_con_with_tables()
    con.execute("""
        INSERT INTO mart.metrics_long SELECT
            'Jan Kowalski', '1', 'C1', 'A', 'a', 1, 1.1, 2.2, 3.3, 4.4, TRUE
        FROM range(0, 123);
    """)
    cfg = _cfg(tmp_path)
    code, out_path = export_summary_excel(con, cfg)
    assert code == 0
    assert out_path.exists()
    # We don't parse XLSX here (fast test); we ensure INFO counts exist by querying SQL:
    # (INFO sheet writing uses SQL counts; if export didn't crash, it ran)


def test_qa_sheet_is_created_even_if_empty(tmp_path: Path):
    con = _mk_con_with_tables()
    con.execute("""
        INSERT INTO mart.metrics_long VALUES
        ('A A','1','C1','X','x',1,1,1,1,1, TRUE);
    """)
    # QA empty
    cfg = _cfg(tmp_path)
    code, out_path = export_summary_excel(con, cfg)
    assert code == 0
    assert out_path.exists()


def test_sorting_is_sql_ordered(tmp_path: Path):
    con = _mk_con_with_tables()
    # Insert in reverse order; SQL ORDER BY should output A then B, and tech_key sorted
    con.execute("""
        INSERT INTO mart.metrics_long VALUES
        ('B','2','C2','ZZ','zz',1,1,1,1,1, TRUE),
        ('A','1','C1','BB','bb',1,1,1,1,1, TRUE),
        ('A','1','C1','AA','aa',1,1,1,1,1, TRUE);
    """)
    cfg = _cfg(tmp_path)

    # We validate ordering by executing the same SQL builder logic indirectly:
    # minimal assert: export completes; deeper ordering validation would require reading XLSX.
    code, _ = export_summary_excel(con, cfg)
    assert code == 0


def test_overflow_error_strategy_error(tmp_path: Path):
    con = _mk_con_with_tables()
    con.execute("""
        INSERT INTO mart.metrics_long
        SELECT 'X','1','C','A','a',1,1,1,1,1, TRUE
        FROM range(0, 11);
    """)
    cfg = _cfg(tmp_path, export={"max_rows_excel": 10, "overflow_strategy": "error"})
    with pytest.raises(ExportOverflowError):
        export_summary_excel(con, cfg)


def test_overflow_split_creates_file(tmp_path: Path):
    con = _mk_con_with_tables()
    con.execute("""
        INSERT INTO mart.metrics_long
        SELECT 'X','1','C','A','a',1,1,1,1,1, TRUE
        FROM range(0, 25);
    """)
    cfg = _cfg(tmp_path, export={"max_rows_excel": 10, "overflow_strategy": "split"})
    code, out_path = export_summary_excel(con, cfg)
    assert code == 0
    assert out_path.exists()


def test_no_data_creates_xlsx_with_headers_and_info(tmp_path: Path):
    con = _mk_con_with_tables()
    # no rows
    cfg = _cfg(tmp_path)
    code, out_path = export_summary_excel(con, cfg)
    assert code == 0
    assert out_path.exists()

================================================================================
mrna_plum\tests\test_export_individual.py
================================================================================

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path

import duckdb
import openpyxl

from mrna_plum.reports.export_individual import export_individual_reports, sanitize_filename


@dataclass
class Cfg:
    root: str
    paths: object
    reports: object


@dataclass
class Paths:
    db_path: str


@dataclass
class Reports:
    individual_dir: str = "_out/indywidualne"
    max_workers: int = 1  # tests: deterministic, no threads
    batch_teachers: int = 50
    create_empty_individual: bool = False


def _make_db(tmp_path: Path) -> duckdb.DuckDBPyConnection:
    db_path = tmp_path / "test.duckdb"
    con = duckdb.connect(str(db_path))
    con.execute("CREATE SCHEMA mart;")
    con.execute(
        """
        CREATE TABLE mart.metrics_long (
            teacher_id      VARCHAR,
            full_name       VARCHAR,
            email           VARCHAR,
            id_bazus        VARCHAR,
            course_name     VARCHAR,
            activity_label  VARCHAR,
            count_value     BIGINT,
            pct_course      DOUBLE,
            pct_kierunek    DOUBLE,
            pct_wydzial     DOUBLE,
            pct_uczelnia    DOUBLE,
            visible_active  BOOLEAN,
            hr_wydzial      VARCHAR,
            hr_jednostka    VARCHAR
        );
        """
    )
    # Teacher A: 2 rows, one has count=0 (must be excluded)
    con.execute(
        """
        INSERT INTO mart.metrics_long VALUES
        ('10', 'Kowalski Jan', 'jan@x.pl', 'B123', 'Kurs A', 'Strona', 3,  50, 10, 5, 1, TRUE, 'WL', 'Katedra X'),
        ('10', 'Kowalski Jan', 'jan@x.pl', 'B123', 'Kurs A', 'Wiki',   0,  20, 10, 5, 1, TRUE, 'WL', 'Katedra X'),
        ('10', 'Kowalski Jan', 'jan@x.pl', 'B123', 'Kurs B', 'Adres URL', 2,  25, 10, 5, 1, TRUE, 'WL', 'Katedra X');
        """
    )
    # Teacher B: only zero rows -> should be SKIPPED and no file
    con.execute(
        """
        INSERT INTO mart.metrics_long VALUES
        ('11', 'Nowak/Anna:*?', 'a@x.pl', 'B999', 'Kurs Z', 'Strona', 0, 10, 1, 1, 1, TRUE, 'WF', 'Jedn Y');
        """
    )
    return con


def test_sanitize_filename_windows_chars():
    assert sanitize_filename('Nowak/Anna:*?') == "Nowak_Anna____"


def test_export_individual_generates_one_file(tmp_path: Path):
    con = _make_db(tmp_path)
    try:
        cfg = Cfg(
            root=str(tmp_path),
            paths=Paths(db_path=str(tmp_path / "test.duckdb")),
            reports=Reports(),
        )
        code, out_dir = export_individual_reports(con, cfg)
        assert code == 0

        out_dir = Path(out_dir)
        files = sorted(out_dir.glob("*.xlsx"))
        # only teacher_id=10 should have file
        assert len(files) == 1
        assert files[0].name.endswith("_10.xlsx")
        assert "Kowalski Jan" in files[0].name

        wb = openpyxl.load_workbook(files[0])
        assert "DANE_KURSY" in wb.sheetnames
        assert "DANE_PERS" in wb.sheetnames

        ws = wb["DANE_KURSY"]
        headers = [ws.cell(1, c).value for c in range(1, 8)]
        assert headers == ["Kurs", "Aktywność", "Liczba", "% kurs", "% kierunek", "% wydział", "% uczelnia"]

        # rows: should exclude count=0, and sorted deterministically
        rows = []
        for r in range(2, ws.max_row + 1):
            rows.append([ws.cell(r, c).value for c in range(1, 8)])
        # expect two rows
        assert len(rows) == 2
        # deterministic sort: Kurs A / Strona first, Kurs B / Adres URL second
        assert rows[0][0] == "Kurs A"
        assert rows[0][1] == "Strona"
        assert rows[1][0] == "Kurs B"
        assert rows[1][1] == "Adres URL"

        # pct in XLSX: stored as 0.xx (not 50)
        # openpyxl reads raw numeric value (formatting is separate)
        assert abs(float(rows[0][3]) - 0.50) < 1e-9

        ws2 = wb["DANE_PERS"]
        # find ID_PLUM row
        kv = {ws2.cell(r, 1).value: ws2.cell(r, 2).value for r in range(2, ws2.max_row + 1)}
        assert kv["ID_PLUM"] == "10"
        assert kv["Pełna nazwa"] == "Kowalski Jan"
        assert kv["E-mail"] == "jan@x.pl"
        assert kv["ID bazus"] == "B123"
        assert kv["Wydział"] == "WL"
        assert kv["Jednostka"] == "Katedra X"

    finally:
        con.close()


def test_export_individual_skips_teacher_with_only_zero_rows(tmp_path: Path):
    con = _make_db(tmp_path)
    try:
        cfg = Cfg(
            root=str(tmp_path),
            paths=Paths(db_path=str(tmp_path / "test.duckdb")),
            reports=Reports(),
        )
        code, out_dir = export_individual_reports(con, cfg)
        assert code == 0

        out_dir = Path(out_dir)
        # verify teacher 11 is not exported
        assert not any(p.name.endswith("_11.xlsx") for p in out_dir.glob("*.xlsx"))
    finally:
        con.close()

================================================================================
mrna_plum\ui_bridge\__init__.py
================================================================================

from .progress import ProgressWriter
__all__ = ["ProgressWriter"]


================================================================================
mrna_plum\ui_bridge\progress.py
================================================================================

from __future__ import annotations
from dataclasses import dataclass
from datetime import datetime, timezone
from zoneinfo import ZoneInfo
import json
from pathlib import Path
from typing import Any, Optional

WARSAW = ZoneInfo("Europe/Warsaw")

@dataclass
class ProgressWriter:
    path: Path

    def emit(
        self,
        step: str,
        status: str,
        message: str,
        current: Optional[int] = None,
        total: Optional[int] = None,
        extra: Optional[dict[str, Any]] = None,
    ) -> None:
        self.path.parent.mkdir(parents=True, exist_ok=True)
        payload = {
            "ts": datetime.now(WARSAW).isoformat(),
            "step": step,
            "status": status,     # start|progress|done|error
            "message": message,
            "current": current,
            "total": total,
            "extra": extra or {},
        }
        with self.path.open("a", encoding="utf-8") as f:
            f.write(json.dumps(payload, ensure_ascii=False) + "\n")


================================================================================
mrna_plum.egg-info\dependency_links.txt
================================================================================




================================================================================
mrna_plum.egg-info\entry_points.txt
================================================================================

[console_scripts]
mrna_plum = mrna_plum.cli:app


================================================================================
mrna_plum.egg-info\PKG-INFO
================================================================================

Metadata-Version: 2.4
Name: mrna-plum
Version: 0.1.0
Summary: mRNA-PLUM CLI (merge/parse/stats/export) for Moodle/PLUM logs
Requires-Python: >=3.10
Requires-Dist: typer>=0.12.0
Requires-Dist: PyYAML>=6.0
Requires-Dist: duckdb>=1.0.0
Requires-Dist: pandas>=2.0.0
Requires-Dist: pyarrow>=15.0.0
Requires-Dist: openpyxl>=3.1.0


================================================================================
mrna_plum.egg-info\requires.txt
================================================================================

typer>=0.12.0
PyYAML>=6.0
duckdb>=1.0.0
pandas>=2.0.0
pyarrow>=15.0.0
openpyxl>=3.1.0


================================================================================
mrna_plum.egg-info\SOURCES.txt
================================================================================

pyproject.toml
src/mrna_plum/__init__.py
src/mrna_plum/__main__.py
src/mrna_plum/cli.py
src/mrna_plum/config.py
src/mrna_plum/errors.py
src/mrna_plum/logging_run.py
src/mrna_plum/paths.py
src/mrna_plum.egg-info/PKG-INFO
src/mrna_plum.egg-info/SOURCES.txt
src/mrna_plum.egg-info/dependency_links.txt
src/mrna_plum.egg-info/entry_points.txt
src/mrna_plum.egg-info/requires.txt
src/mrna_plum.egg-info/top_level.txt
src/mrna_plum/io/__init__.py
src/mrna_plum/io/csv_read.py
src/mrna_plum/io/excel_keys.py
src/mrna_plum/merge/__init__.py
src/mrna_plum/merge/merge_logs.py
src/mrna_plum/parse/__init__.py
src/mrna_plum/parse/context.py
src/mrna_plum/parse/parse_logs.py
src/mrna_plum/reports/__init__.py
src/mrna_plum/reports/export_excel.py
src/mrna_plum/reports/export_individual.py
src/mrna_plum/rules/__init__.py
src/mrna_plum/rules/engine.py
src/mrna_plum/rules/models.py
src/mrna_plum/stats/__init__.py
src/mrna_plum/stats/compute_stats.py
src/mrna_plum/store/__init__.py
src/mrna_plum/store/duckdb_store.py
src/mrna_plum/ui_bridge/__init__.py
src/mrna_plum/ui_bridge/progress.py

================================================================================
mrna_plum.egg-info\top_level.txt
================================================================================

mrna_plum


================================================================================
repo_dump.py
================================================================================

from pathlib import Path

ROOT = Path(".")
OUT = Path("FULL_REPO_DUMP.txt")

EXCLUDE = {".git", "__pycache__", ".venv", ".idea"}

with OUT.open("w", encoding="utf-8") as out:
    for path in sorted(ROOT.rglob("*")):
        if path.is_dir():
            continue
        
        if any(part in EXCLUDE for part in path.parts):
            continue
        
        out.write(f"\n\n{'='*80}\n")
        out.write(f"{path.relative_to(ROOT)}\n")
        out.write(f"{'='*80}\n\n")
        
        try:
            out.write(path.read_text(encoding="utf-8"))
        except Exception:
            out.write("[BINARY OR UNREADABLE FILE]\n")

print("Zrobione.")

================================================================================
src\mrna_plum\__init__.py
================================================================================

__all__ = ["__version__"]
__version__ = "0.1.0"

================================================================================
src\mrna_plum\__main__.py
================================================================================

from .cli import app

if __name__ == "__main__":
    app()


================================================================================
src\mrna_plum\activities\__init__.py
================================================================================



================================================================================
src\mrna_plum\activities\activities_state.py
================================================================================

from __future__ import annotations

from dataclasses import dataclass
from typing import List, Any, Dict, Optional
import json
import uuid

import duckdb


# --- Config dataclasses expected by tests ---

@dataclass(frozen=True)
class DeletionConfig:
    delete_operations: List[str]
    delete_tech_keys: List[str]
    delete_activity_labels_regex: List[str]
    disappearance_grace_period_days: int
    min_missing_snapshots_to_confirm: int
    deleted_at_policy: str  # "first_missing" | "last_seen"


@dataclass(frozen=True)
class MappingConfig:
    use_activity_id_map_table: bool
    allow_fuzzy_name_type_match: bool


@dataclass(frozen=True)
class IncrementalConfig:
    checkpoint_table: str
    checkpoint_key: str
    process_only_new_snapshots: bool
    process_only_new_events: bool


@dataclass(frozen=True)
class BuildConfig:
    deletion: DeletionConfig
    mapping: MappingConfig
    incremental: IncrementalConfig


def _ensure_tables(con: duckdb.DuckDBPyConnection) -> None:
    con.execute("create schema if not exists raw;")
    con.execute("create schema if not exists mart;")

    con.execute("""
    create table if not exists mart.activities_state (
      course_code varchar not null,
      ay varchar,
      term varchar,
      wydzial_code varchar,
      kierunek_code varchar,
      track_code varchar,
      semester_code varchar,

      activity_id varchar not null,
      type varchar,
      name_last varchar,

      first_seen_at timestamp,
      last_seen_at timestamp,

      last_snapshot_at timestamp,
      last_event_at timestamp,

      visible_last boolean,
      deleted_at timestamp,

      status_final varchar not null,
      evidence_deleted varchar not null,
      confidence_deleted double not null,

      notes varchar,
      updated_at timestamp default now(),

      primary key(course_code, activity_id)
    );
    """)

    con.execute("""
    create table if not exists mart.activities_qa (
      qa_id varchar,
      qa_type varchar not null,
      course_code varchar,
      activity_id varchar,
      object_id varchar,
      details_json varchar,
      created_at timestamp default now()
    );
    """)


def _qa(con: duckdb.DuckDBPyConnection, qa_type: str,
        course_code: Optional[str], activity_id: Optional[str],
        object_id: Optional[str], details: Dict[str, Any]) -> None:
    con.execute(
        """
        insert into mart.activities_qa(qa_id, qa_type, course_code, activity_id, object_id, details_json)
        values (?,?,?,?,?,?)
        """,
        [str(uuid.uuid4()), qa_type, course_code, activity_id, object_id, json.dumps(details, ensure_ascii=False)],
    )


def build_activities_state(con: duckdb.DuckDBPyConnection, cfg: BuildConfig) -> dict:
    """
    Public API expected by tests.
    Minimal implementation to satisfy:
    - delete from logs
    - disappearance from snapshots
    - hidden
    - conflict QA
    - missing mapping QA
    """
    _ensure_tables(con)

    # Universe: wszystkie (course_code, activity_id) z snapshotów + eventów gdzie object_id nie null
    con.execute("""
    create temp table tmp_universe as
    select distinct course_code, activity_id
    from raw.activities_snapshot
    union
    select distinct course_code, cast(object_id as varchar) as activity_id
    from events_canonical
    where object_id is not null;
    """)

    # Snapshot last
    con.execute("""
    create temp table tmp_snap_last as
    select s.*
    from raw.activities_snapshot s
    join (
      select course_code, activity_id, max(captured_at) as mx
      from raw.activities_snapshot
      group by 1,2
    ) t
    on s.course_code=t.course_code and s.activity_id=t.activity_id and s.captured_at=t.mx;
    """)

    # Snapshot bounds per activity
    con.execute("""
    create temp table tmp_snap_bounds as
    select course_code, activity_id, min(captured_at) as first_snap, max(captured_at) as last_snap
    from raw.activities_snapshot
    group by 1,2;
    """)

    # Event bounds per object_id (map 1:1 -> activity_id)
    con.execute("""
    create temp table tmp_evt_bounds as
    select course_code, cast(object_id as varchar) as activity_id,
           min(ts_utc) as first_evt,
           max(ts_utc) as last_evt
    from events_canonical
    where counted = true and object_id is not null
    group by 1,2;
    """)

    # Delete from logs (operation == DELETE)
    con.execute("""
    create temp table tmp_evt_delete as
    select course_code, cast(object_id as varchar) as activity_id, min(ts_utc) as deleted_at_log
    from events_canonical
    where counted = true
      and object_id is not null
      and upper(operation) = 'DELETE'
    group by 1,2;
    """)

    # Disappearance:
    # - last_seen_snap_at = max(captured_at) for activity
    # - count snapshots of course after last_seen_snap_at (>=min_missing)
    # - last_course_snapshot_at >= first_missing + grace
    con.execute("""
    create temp table tmp_course_snaps as
    select course_code, captured_at
    from raw.activities_snapshot
    group by 1,2;
    """)

    con.execute("""
    create temp table tmp_last_seen as
    select course_code, activity_id,
           max(captured_at) as last_seen_snap_at,
           min(captured_at) as first_seen_snap_at
    from raw.activities_snapshot
    group by 1,2;
    """)

    con.execute("""
    create temp table tmp_missing as
    select
      a.course_code,
      a.activity_id,
      a.last_seen_snap_at,
      min(cs.captured_at) as first_missing_snapshot_at,
      count(*) as missing_count,
      max(cs.captured_at) as last_course_snapshot_at
    from tmp_last_seen a
    join tmp_course_snaps cs
      on cs.course_code=a.course_code and cs.captured_at > a.last_seen_snap_at
    group by 1,2,3;
    """)

    con.execute("""
    create temp table tmp_disappearance as
    select
      course_code,
      activity_id,
      case
        when ? = 'first_missing' then first_missing_snapshot_at
        else last_seen_snap_at
      end as deleted_at_snap,
      first_missing_snapshot_at,
      last_course_snapshot_at,
      missing_count
    from tmp_missing
    where missing_count >= ?
      and last_course_snapshot_at >= (first_missing_snapshot_at + (? || ' days')::interval);
    """, [cfg.deletion.deleted_at_policy, cfg.deletion.min_missing_snapshots_to_confirm, cfg.deletion.disappearance_grace_period_days])

    # Course meta (z eventów)
    con.execute("""
    create temp table tmp_course_meta as
    select course_code,
           any_value(ay) as ay,
           any_value(term) as term,
           any_value(wydzial_code) as wydzial_code,
           any_value(kierunek_code) as kierunek_code,
           any_value(track_code) as track_code,
           any_value(semester_code) as semester_code
    from events_canonical
    group by 1;
    """)

    # Final stage
    con.execute("""
    create temp table tmp_final as
    select
      u.course_code,
      m.ay, m.term, m.wydzial_code, m.kierunek_code, m.track_code, m.semester_code,
      u.activity_id,
      sl.type,
      sl.name as name_last,
      least(sb.first_snap, eb.first_evt) as first_seen_at,
      greatest(sb.last_snap, eb.last_evt) as last_seen_at,
      sb.last_snap as last_snapshot_at,
      eb.last_evt as last_event_at,
      sl.visible_to_students as visible_last,
      dlog.deleted_at_log,
      ds.deleted_at_snap,
      case
        when dlog.deleted_at_log is not null and ds.deleted_at_snap is not null then least(dlog.deleted_at_log, ds.deleted_at_snap)
        when dlog.deleted_at_log is not null then dlog.deleted_at_log
        when ds.deleted_at_snap is not null then ds.deleted_at_snap
        else null
      end as deleted_at,
      case
        when dlog.deleted_at_log is not null and ds.deleted_at_snap is not null then 'both'
        when dlog.deleted_at_log is not null then 'log_delete_event'
        when ds.deleted_at_snap is not null then 'snapshot_disappearance'
        else 'none'
      end as evidence_deleted,
      case
        when dlog.deleted_at_log is not null and ds.deleted_at_snap is not null then 0.95
        when dlog.deleted_at_log is not null then 0.80
        when ds.deleted_at_snap is not null then 0.70
        else 0.0
      end as confidence_deleted
    from tmp_universe u
    left join tmp_course_meta m using(course_code)
    left join tmp_snap_last sl using(course_code, activity_id)
    left join tmp_snap_bounds sb using(course_code, activity_id)
    left join tmp_evt_bounds eb using(course_code, activity_id)
    left join tmp_evt_delete dlog using(course_code, activity_id)
    left join tmp_disappearance ds using(course_code, activity_id);
    """)

    # QA: conflict log delete but snapshot visible after delete
    rows = con.execute("""
    select course_code, activity_id, deleted_at_log, last_snapshot_at
    from tmp_final
    where deleted_at_log is not null
      and last_snapshot_at is not null
      and last_snapshot_at > deleted_at_log
      and visible_last = true;
    """).fetchall()
    for course_code, activity_id, deleted_at_log, last_snapshot_at in rows:
        _qa(con, "conflict_log_delete_but_visible_in_snapshot", course_code, activity_id, None, {
            "deleted_at_log": str(deleted_at_log),
            "last_snapshot_at": str(last_snapshot_at),
        })

    # QA: activity without mapping (snapshot exists but no events for it)
    rows = con.execute("""
    select s.course_code, s.activity_id
    from (select distinct course_code, activity_id from raw.activities_snapshot) s
    left join (select distinct course_code, cast(object_id as varchar) as activity_id from events_canonical where object_id is not null) e
      on e.course_code=s.course_code and e.activity_id=s.activity_id
    where e.activity_id is null;
    """).fetchall()
    for course_code, activity_id in rows:
        _qa(con, "activity_without_object_id_mapping", course_code, activity_id, None, {})

    # status_final
    con.execute("""
    create temp table tmp_final2 as
    select *,
      case
        when deleted_at is not null then 'visible_deleted'
        when visible_last = true then 'visible_active'
        when visible_last = false then 'hidden'
        else 'unknown'
      end as status_final,
      cast(null as varchar) as notes
    from tmp_final;
    """)

    # Upsert (delete+insert for simplicity in tests)
    con.execute("delete from mart.activities_state;")
    con.execute("""
    insert into mart.activities_state(
      course_code, ay, term, wydzial_code, kierunek_code, track_code, semester_code,
      activity_id, type, name_last,
      first_seen_at, last_seen_at,
      last_snapshot_at, last_event_at,
      visible_last,
      deleted_at,
      status_final, evidence_deleted, confidence_deleted,
      notes
    )
    select
      course_code, ay, term, wydzial_code, kierunek_code, track_code, semester_code,
      activity_id, type, name_last,
      first_seen_at, last_seen_at,
      last_snapshot_at, last_event_at,
      visible_last,
      deleted_at,
      status_final, evidence_deleted, confidence_deleted,
      notes
    from tmp_final2;
    """)

    return {"ok": True}

================================================================================
src\mrna_plum\activities\snapshots_load.py
================================================================================

from __future__ import annotations

import csv
import hashlib
import json
import re
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Iterable, Optional

import duckdb


def _sha1(text: str) -> str:
    return hashlib.sha1(text.encode("utf-8", errors="replace")).hexdigest()


def _captured_at_from_filename(path: Path) -> datetime:
    # np. 20260206-0826_2526z_zawartosc_kursow.csv
    m = re.search(r"(\d{8})-(\d{4})", path.name)
    if not m:
        return datetime.fromtimestamp(path.stat().st_mtime)
    ymd = m.group(1)  # YYYYMMDD
    hm = m.group(2)   # HHMM
    return datetime(
        int(ymd[0:4]), int(ymd[4:6]), int(ymd[6:8]),
        int(hm[0:2]), int(hm[2:4]),
    )


@dataclass(frozen=True)
class SnapshotRowPL:
    course_code: str
    activity_id: str
    name: str
    type: str
    visible_to_students: bool
    captured_at: datetime
    source_file: str
    row_key: str


def iter_snapshot_csv_plum_visible(path: Path) -> Iterable[SnapshotRowPL]:
    captured_at = _captured_at_from_filename(path)

    with path.open("r", encoding="utf-8-sig", newline="") as f:
        reader = csv.DictReader(f)

        req = {"Nazwa kursu", "ID aktywności", "Nazwa aktywności", "Format aktywności"}
        missing = req - set(reader.fieldnames or [])
        if missing:
            raise ValueError(f"Snapshot(PL) missing columns {sorted(missing)} in {path}")

        for r in reader:
            course_code = (r.get("Nazwa kursu") or "").strip()
            activity_id = (r.get("ID aktywności") or "").strip()
            if not course_code or not activity_id:
                continue

            name = (r.get("Nazwa aktywności") or "").strip()
            typ = (r.get("Format aktywności") or "").strip()

            payload = json.dumps(
                {
                    "course_code": course_code,
                    "activity_id": activity_id,
                    "name": name,
                    "type": typ,
                    "visible_to_students": True,
                    "captured_at": captured_at.isoformat(),
                },
                ensure_ascii=False,
                separators=(",", ":"),
            )
            row_key = _sha1(payload)

            yield SnapshotRowPL(
                course_code=course_code,
                activity_id=activity_id,
                name=name,
                type=typ,
                visible_to_students=True,
                captured_at=captured_at,
                source_file=str(path),
                row_key=row_key,
            )


def ensure_snapshot_table(con: duckdb.DuckDBPyConnection) -> None:
    con.execute("create schema if not exists raw;")
    con.execute(
        """
        create table if not exists raw.activities_snapshot (
          course_code varchar not null,
          activity_id varchar not null,
          name varchar,
          type varchar,
          visible_to_students boolean,
          captured_at timestamp not null,
          source_file varchar,
          row_key varchar not null,
          inserted_at timestamp default now()
        );
        """
    )
    con.execute(
        "create unique index if not exists ux_activities_snapshot_rowkey on raw.activities_snapshot(row_key);"
    )


def load_plum_snapshot_file_into_duckdb(
    con: duckdb.DuckDBPyConnection,
    snapshot_file: Path,
) -> dict:
    ensure_snapshot_table(con)

    inserted = 0
    scanned = 0
    max_captured_at: Optional[datetime] = None

    for row in iter_snapshot_csv_plum_visible(snapshot_file):
        scanned += 1
        con.execute(
            """
            insert into raw.activities_snapshot
            (course_code, activity_id, name, type, visible_to_students, captured_at, source_file, row_key)
            select ?,?,?,?,?,?,?,?
            where not exists (select 1 from raw.activities_snapshot where row_key = ?)
            """,
            [
                row.course_code,
                row.activity_id,
                row.name,
                row.type,
                row.visible_to_students,
                row.captured_at,
                row.source_file,
                row.row_key,
                row.row_key,
            ],
        )
        inserted += int(con.execute("select changes()").fetchone()[0])

        if max_captured_at is None or row.captured_at > max_captured_at:
            max_captured_at = row.captured_at

    return {
        "snapshot_file": str(snapshot_file),
        "scanned_rows": scanned,
        "inserted_rows": inserted,
        "captured_at": max_captured_at.isoformat() if max_captured_at else None,
    }

================================================================================
src\mrna_plum\cfg_helpers.py
================================================================================

from __future__ import annotations
from pathlib import Path
from typing import Any

def cfg_get(cfg: dict, key: str, default: Any = None) -> Any:
    cur: Any = cfg
    for part in key.split("."):
        if not isinstance(cur, dict) or part not in cur:
            return default
        cur = cur[part]
    return cur

def cfg_str(cfg: dict, key: str, default: str | None = None) -> str | None:
    v = cfg_get(cfg, key, default)
    return None if v is None else str(v)

def cfg_int(cfg: dict, key: str, default: int | None = None) -> int | None:
    v = cfg_get(cfg, key, default)
    return None if v is None else int(v)

def cfg_bool(cfg: dict, key: str, default: bool = False) -> bool:
    v = cfg_get(cfg, key, default)
    return bool(v)

def cfg_path(root: Path, cfg: dict, key: str, default_rel: str | None = None) -> Path | None:
    v = cfg_get(cfg, key, default_rel)
    if v is None or str(v).strip() == "":
        return None
    p = Path(str(v))
    return p if p.is_absolute() else (root / p).resolve()

================================================================================
src\mrna_plum\cli.py
================================================================================

from __future__ import annotations

from pathlib import Path
from typing import Optional, Callable, Any

import typer

from .paths import ProjectPaths
from .config import load_config
from .logging_run import setup_file_logger
from .ui_bridge import ProgressWriter
from .errors import ConfigError, InputDataError, MixedPeriodsError, ProcessingError

app = typer.Typer(add_completion=False)

# Exit codes (wg ustaleń)
EC_OK = 0
EC_CONFIG = 2
EC_INPUT = 10
EC_MIXED = 20
EC_PROC = 30
EC_INTERNAL = 40


# -------------------------
# Helpers
# -------------------------
def _resolve_root(root: str) -> Path:
    return Path(root).resolve()


def _resolve_config(root: Path, config: str | None) -> Path:
    return Path(config).resolve() if config else (root / "config.yaml").resolve()


def _ensure_dirs(paths: ProjectPaths) -> None:
    paths.run_dir.mkdir(parents=True, exist_ok=True)
    paths.data_dir.mkdir(parents=True, exist_ok=True)
    paths.parquet_dir.mkdir(parents=True, exist_ok=True)
    # opcjonalnie: out dir jeśli masz w ProjectPaths
    try:
        paths.out_dir.mkdir(parents=True, exist_ok=True)  # type: ignore[attr-defined]
    except Exception:
        (paths.root / "_out").mkdir(parents=True, exist_ok=True)


def _write_marker(paths: ProjectPaths, step: str) -> None:
    # marker: {step}.ok w _run
    paths.marker_path(step).write_text("ok", encoding="utf-8")


def _collect_input_files(root: Path, input_glob: str) -> list[Path]:
    return sorted([p for p in root.glob(input_glob) if p.is_file()])


def _main_guard(fn: Callable[..., Any]) -> Callable[..., Any]:
    def wrapper(*args, **kwargs):
        try:
            return fn(*args, **kwargs)
        except ConfigError as e:
            typer.echo(str(e), err=True)
            raise typer.Exit(code=EC_CONFIG)
        except InputDataError as e:
            typer.echo(str(e), err=True)
            raise typer.Exit(code=EC_INPUT)
        except MixedPeriodsError as e:
            typer.echo(str(e), err=True)
            raise typer.Exit(code=EC_MIXED)
        except ProcessingError as e:
            typer.echo(str(e), err=True)
            raise typer.Exit(code=EC_PROC)
        except typer.Exit:
            raise
        except Exception as e:
            typer.echo(f"Internal error: {e}", err=True)
            raise typer.Exit(code=EC_INTERNAL)

    return wrapper


# -------------------------
# Commands
# -------------------------

@app.command("init")
@_main_guard
def cmd_init(
    root: str = typer.Option(..., "--root", help="Root folder projektu (np. ThisWorkbook.Path)"),
):
    """
    Tworzy podstawową strukturę folderów projektu.
    """
    root_p = _resolve_root(root)

    # lazy import - bez ryzyka cykli
    from .init_project import init_project

    created = init_project(root_p)
    typer.echo(f"Created {len(created)} folders")
    raise typer.Exit(code=EC_OK)


@app.command("merge-logs")
@_main_guard
def cmd_merge_logs(
    root: str = typer.Option(..., "--root", help="Root folder passed from VBA (ThisWorkbook.Path)"),
    config: str | None = typer.Option(None, "--config", help="Config path; default {root}/config.yaml"),
    mode: str = typer.Option(
        "duckdb",
        "--mode",
        help="duckdb (pipeline B) | parquet (pipeline A → merged_raw.parquet)",
        case_sensitive=False,
    ),
):
    """
    Merge logów CSV z Moodle/PLUM.

    mode=parquet:
        CSV → merged_raw.parquet (pipeline A)

    mode=duckdb:
        CSV → DuckDB raw (pipeline B / staging)
    """
    root_p = _resolve_root(root)
    cfg_p = _resolve_config(root_p, config)
    paths = ProjectPaths(root=root_p)
    _ensure_dirs(paths)

    logger = setup_file_logger(paths.run_dir / "run.log")
    progress = ProgressWriter(paths.run_dir / "progress.jsonl")

    cfg = load_config(cfg_p)

    mode = mode.lower().strip()
    if mode not in ("parquet", "duckdb"):
        raise typer.BadParameter("--mode must be one of: duckdb, parquet")

    input_files = _collect_input_files(root_p, cfg.input_glob)
    if not input_files:
        raise InputDataError(f"No input files found under {root_p} with glob {cfg.input_glob}")

    progress.emit("merge", "start", f"Starting merge ({mode})", current=0, total=len(input_files),
                  extra={"root": str(root_p), "mode": mode})
    logger.info("[merge] start mode=%s files=%s", mode, len(input_files))

    if mode == "parquet":
        # pipeline A
        from .merge import merge_logs_to_parquet

        merged_parquet = paths.parquet_dir / "merged_raw.parquet"
        total_rows = merge_logs_to_parquet(input_files, merged_parquet, dedup_per_file=True)

        progress.emit("merge", "done", "Merge finished", current=len(input_files), total=len(input_files),
                      extra={"rows": total_rows, "parquet": str(merged_parquet)})
        _write_marker(paths, "merge")
        logger.info("[merge] done rows=%s parquet=%s", total_rows, merged_parquet)
        raise typer.Exit(code=EC_OK)

    # mode == duckdb → pipeline B (używamy funkcji merge_logs_into_duckdb)
    # Importy absolutne usuwamy — wszystko względnie
    from .store.duckdb_store import open_store
    from .merge.merge_logs import merge_logs_into_duckdb

    # db_path: preferuj ProjectPaths
    db_path = paths.duckdb_path
    con = open_store(db_path)
    try:
        res = merge_logs_into_duckdb(
            root=root_p,
            con=con,
            export_mode="duckdb",
            export_dir=None,
            chunk_size=int(getattr(cfg, "chunk_rows", 2000)),  # fallback
        )
    finally:
        con.close()

    progress.emit("merge", "done", "Merge finished (duckdb)", current=len(input_files), total=len(input_files),
                  extra={"db": str(db_path), "courses": getattr(res, "courses", None), "inserted_rows": getattr(res, "inserted_rows", None)})
    _write_marker(paths, "merge")
    logger.info("[merge] done duckdb db=%s res=%s", db_path, res)
    raise typer.Exit(code=EC_OK)


@app.command("build-db")
@_main_guard
def cmd_build_db(
    root: str = typer.Option(..., "--root"),
    config: str | None = typer.Option(None, "--config"),
):
    """
    Pipeline A: merged_raw.parquet → parsed.parquet → DuckDB(raw_logs)
    """
    root_p = _resolve_root(root)
    cfg_p = _resolve_config(root_p, config)
    paths = ProjectPaths(root=root_p)
    _ensure_dirs(paths)

    logger = setup_file_logger(paths.run_dir / "run.log")
    progress = ProgressWriter(paths.run_dir / "progress.jsonl")

    cfg = load_config(cfg_p)

    # KEYS → rules
    from .io.excel_keys import load_keys_sheet
    from .rules.engine import compile_rules

    keys_wb = (root_p / cfg.keys_workbook).resolve()
    keys_df = load_keys_sheet(keys_wb, cfg.keys_sheet)
    rules = compile_rules(keys_df)

    merged_parquet = paths.parquet_dir / "merged_raw.parquet"
    if not merged_parquet.exists():
        raise InputDataError(f"Missing merged parquet. Run: mrna_plum merge-logs --mode parquet --root ...  Expected: {merged_parquet}")

    progress.emit("parse", "start", "Parsing merged logs & applying rules")
    logger.info("[parse] start rules=%s", len(rules))

    from .parse import parse_merged_parquet

    parsed_parquet = paths.parquet_dir / "parsed.parquet"
    n_rows, run_period = parse_merged_parquet(merged_parquet, parsed_parquet, cfg, rules)

    # DuckDB load
    from .store import DuckDbStore

    store = DuckDbStore(paths.duckdb_path)
    store.init_schema()

    progress.emit("db", "start", "Loading parsed parquet into DuckDB", extra={"db": str(paths.duckdb_path)})
    store.load_parquet_to_raw(parsed_parquet)

    progress.emit("db", "done", "DB built", extra={"rows": n_rows, "period": run_period})
    _write_marker(paths, "build_db")
    logger.info("[db] done rows=%s period=%s db=%s", n_rows, run_period, paths.duckdb_path)
    raise typer.Exit(code=EC_OK)


@app.command("parse-events")
@_main_guard
def cmd_parse_events(
    root: str = typer.Option(..., "--root", help="Root projektu"),
    config: str | None = typer.Option(None, "--config", help="config.yaml; default {root}/config.yaml"),
    keys_xlsx: str | None = typer.Option(None, "--keys-xlsx", help="Override ścieżki KEYS.xlsx/KEYS workbook"),
):
    """
    Pipeline B: raw (DuckDB) → events_canonical (DuckDB)
    """
    root_p = _resolve_root(root)
    cfg_p = _resolve_config(root_p, config)
    paths = ProjectPaths(root=root_p)
    _ensure_dirs(paths)

    logger = setup_file_logger(paths.run_dir / "run.log")
    progress = ProgressWriter(paths.run_dir / "progress.jsonl")

    cfg = load_config(cfg_p)

    progress.emit("parse_events", "start", "Parsing events into events_canonical",
                  extra={"db": str(paths.duckdb_path), "keys_xlsx": keys_xlsx})
    logger.info("[parse_events] start db=%s", paths.duckdb_path)

    # run_parse_events jest w Twoim dumpie importowany absolutnie — tu robimy względnie + lazy
    from .parse.parse_events import run_parse_events

    exit_code = run_parse_events(cfg, root=str(root_p), keys_xlsx_override=keys_xlsx)
    if exit_code != 0:
        raise ProcessingError(f"parse-events failed with exit code {exit_code}")

    progress.emit("parse_events", "done", "Events parsed")
    _write_marker(paths, "parse_events")
    logger.info("[parse_events] done")
    raise typer.Exit(code=EC_OK)


@app.command("build-activities-state")
@_main_guard
def cmd_build_activities_state(
    root: str = typer.Option(..., "--root"),
    config: str | None = typer.Option(None, "--config"),
    snapshot_file: str = typer.Option(..., "--snapshot-file", help="CSV 'zawartość kursów' wybrany w VBA"),
):
    """
    Pipeline B: snapshot CSV → raw.activities_snapshot → mart.activities_state
    """
    root_p = _resolve_root(root)
    cfg_p = _resolve_config(root_p, config)
    paths = ProjectPaths(root=root_p)
    _ensure_dirs(paths)

    logger = setup_file_logger(paths.run_dir / "run.log")
    progress = ProgressWriter(paths.run_dir / "progress.jsonl")

    cfg = load_config(cfg_p)

    snap_path = Path(snapshot_file).resolve()
    if not snap_path.exists():
        raise InputDataError(f"Snapshot file not found: {snap_path}")

    from .store import DuckDbStore
    from .activities.activities_state import build_activities_state, BuildConfig, DeletionConfig, MappingConfig, IncrementalConfig

    store = DuckDbStore(paths.duckdb_path)
    store.init_schema()

    progress.emit(
        "activities_state",
        "start",
        "Loading snapshots & building activities_state",
        extra={"snapshot_file": str(snap_path), "db": str(paths.duckdb_path)},
    )
    logger.info("[activities_state] start snapshot=%s", snap_path)

    # Loader – wybierz właściwy (w dumpie masz oba symbole; trzymamy jeden, względny)
    from .activities.snapshots_load import load_plum_snapshot_file_into_duckdb

    # KONFIG: na razie minimalny default (bo AppConfig jest płaski)
    # Docelowo: wczytać z YAML sekcję build_activities_state (dict) i zmapować.
    build_cfg = BuildConfig(
        deletion=DeletionConfig(
            delete_operations=["DELETE"],
            delete_tech_keys=[],
            delete_activity_labels_regex=[],
            disappearance_grace_period_days=14,
            min_missing_snapshots_to_confirm=2,
            deleted_at_policy="first_missing",
        ),
        mapping=MappingConfig(
            use_activity_id_map_table=True,
            allow_fuzzy_name_type_match=False,
        ),
        incremental=IncrementalConfig(
            checkpoint_table="raw.pipeline_checkpoints",
            checkpoint_key="build_activities_state",
            process_only_new_snapshots=True,
            process_only_new_events=True,
        ),
    )

    with store.connect() as con:
        load_stats = load_plum_snapshot_file_into_duckdb(con, snap_path)
        progress.emit("activities_state", "snapshots_loaded", "Snapshots loaded", extra=load_stats)

        stats = build_activities_state(con, build_cfg)

    progress.emit("activities_state", "done", "Activities state built", extra=stats)
    _write_marker(paths, "activities_state")
    logger.info("[activities_state] done %s", stats)
    raise typer.Exit(code=EC_OK)


@app.command("compute-stats")
@_main_guard
def cmd_compute_stats(
    root: str = typer.Option(..., "--root"),
    ay: str | None = typer.Option(None, "--ay"),
    term: str | None = typer.Option(None, "--term"),
    config: str | None = typer.Option(None, "--config"),
):
    """
    Ujednolicone compute-stats: wywołuje stats.compute_stats(root, ay, term).
    (Zgodnie z realną sygnaturą w Twoim repo.)
    """
    root_p = _resolve_root(root)
    _ = _resolve_config(root_p, config)  # zostawiamy na przyszłość (gdy compute-stats zacznie używać cfg)
    paths = ProjectPaths(root=root_p)
    _ensure_dirs(paths)

    logger = setup_file_logger(paths.run_dir / "run.log")
    progress = ProgressWriter(paths.run_dir / "progress.jsonl")

    progress.emit("stats", "start", "Computing stats", extra={"ay": ay, "term": term})
    logger.info("[stats] start ay=%s term=%s", ay, term)

    from .stats.compute_stats import compute_stats

    compute_stats(root=root_p, ay=ay, term=term)

    progress.emit("stats", "done", "Stats computed", extra={"ay": ay, "term": term})
    _write_marker(paths, "stats")
    logger.info("[stats] done")
    raise typer.Exit(code=EC_OK)


@app.command("export-excel")
@_main_guard
def cmd_export_excel(
    root: str = typer.Option(..., "--root"),
    db_path: str | None = typer.Option(None, "--db-path", help="Override ścieżki do DuckDB; default z ProjectPaths"),
    config: str | None = typer.Option(None, "--config"),
):
    """
    Eksport agregatów do Excela (docelowo: export_summary_excel).
    """
    root_p = _resolve_root(root)
    cfg_p = _resolve_config(root_p, config)
    paths = ProjectPaths(root=root_p)
    _ensure_dirs(paths)

    logger = setup_file_logger(paths.run_dir / "run.log")
    progress = ProgressWriter(paths.run_dir / "progress.jsonl")

    cfg = load_config(cfg_p)

    db = Path(db_path).resolve() if db_path else paths.duckdb_path

    progress.emit("export_excel", "start", "Exporting Excel report", extra={"db": str(db)})
    logger.info("[export_excel] start db=%s", db)

    import duckdb
    from .reports.export_excel import export_summary_excel, ExportOverflowError, EXIT_OVERFLOW

    con = duckdb.connect(str(db))
    try:
        code, out_path = export_summary_excel(con, cfg)  # cfg w Twoim repo jest dict-like
    except ExportOverflowError:
        raise typer.Exit(code=EXIT_OVERFLOW)
    finally:
        con.close()

    progress.emit("export_excel", "done", "Excel exported", extra={"out": str(out_path)})
    _write_marker(paths, "export_excel")
    logger.info("[export_excel] done out=%s", out_path)
    raise typer.Exit(code=code)


@app.command("export-individual")
@_main_guard
def cmd_export_individual(
    root: str = typer.Option(..., "--root"),
    config: str | None = typer.Option(None, "--config"),
):
    """
    Eksport paczek indywidualnych.
    UWAGA: funkcja eksportu może mieć inną nazwę w Twoim repo – import jest lazy.
    """
    root_p = _resolve_root(root)
    cfg_p = _resolve_config(root_p, config)
    paths = ProjectPaths(root=root_p)
    _ensure_dirs(paths)

    logger = setup_file_logger(paths.run_dir / "run.log")
    progress = ProgressWriter(paths.run_dir / "progress.jsonl")

    cfg = load_config(cfg_p)

    progress.emit("export_individual", "start", "Exporting individual reports")
    logger.info("[export_individual] start")

    # Dopasuj do realnej funkcji w reports/export_individual.py:
    # w dumpie było `export_individual_reports(con, cfg)` — więc tak próbujemy.
    import duckdb
    db = paths.duckdb_path
    con = duckdb.connect(str(db))
    try:
        from .reports.export_individual import export_individual_reports
        code, out_dir = export_individual_reports(con, cfg)
    finally:
        con.close()

    progress.emit("export_individual", "done", "Individual reports exported", extra={"out_dir": str(out_dir)})
    _write_marker(paths, "export_individual")
    logger.info("[export_individual] done out_dir=%s", out_dir)
    raise typer.Exit(code=code)

================================================================================
src\mrna_plum\config.py
================================================================================

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict

import yaml

from .errors import ConfigError


# Jeśli AppConfig już masz w tym pliku, NIE dubluj tej klasy.
# Ten blok zostaw tylko jeśli AppConfig nie istnieje.
@dataclass(frozen=True)
class AppConfig:
    input_glob: str = "**/*.csv"
    keys_workbook: str = "mRNA_PLUM_Form.xlsx"
    keys_sheet: str = "KEYS"
    col_time: str = "Czas"
    col_context: str = "Kontekst zdarzenia"
    col_desc: str = "Opis"
    col_component: str = "Składnik"
    col_event_name: str = "Nazwa zdarzenia"
    course_regex: str = r"Kurs:\s*([^\s]+)"
    period_regex: str = r"(\d{4}/\d{2}[zl])"
    chunk_rows: int = 200_000


def load_config_dict(path: Path) -> Dict[str, Any]:
    """
    Jedno źródło prawdy: YAML -> dict.
    Reszta projektu (stats/reports/activities_state) używa dict.
    """
    p = Path(path).resolve()
    if not p.exists():
        raise ConfigError(f"Config file not found: {p}")

    try:
        text = p.read_text(encoding="utf-8")
    except Exception as e:
        raise ConfigError(f"Cannot read config file: {p}. {e}") from e

    try:
        data = yaml.safe_load(text) or {}
    except Exception as e:
        raise ConfigError(f"Invalid YAML in config: {p}. {e}") from e

    if not isinstance(data, dict):
        raise ConfigError(f"Config root must be a mapping (dict). Got: {type(data).__name__}")

    return data


def load_app_config(path: Path) -> AppConfig:
    """
    CSV layer: merge/parse używa AppConfig.
    Bierzemy dane z sekcji `csv:` w YAML (albo fallback: root dict dla kompatybilności).
    """
    cfg = load_config_dict(path)

    # preferuj sekcję `csv:`, ale pozwól na kompatybilność wstecz
    csv_section = cfg.get("csv")
    if csv_section is None:
        csv_section = cfg
    if not isinstance(csv_section, dict):
        raise ConfigError("Config key `csv` must be a mapping (dict).")

    try:
        return AppConfig(**csv_section)
    except TypeError as e:
        raise ConfigError(f"Invalid `csv` section fields for AppConfig: {e}") from e

================================================================================
src\mrna_plum\errors.py
================================================================================

class mRnaPlumError(Exception):
    """Base app error."""


class ConfigError(mRnaPlumError):
    pass


class InputDataError(mRnaPlumError):
    pass


class MixedPeriodsError(mRnaPlumError):
    pass


class ProcessingError(mRnaPlumError):
    pass


================================================================================
src\mrna_plum\import\__init__.py
================================================================================



================================================================================
src\mrna_plum\import\import_roster.py
================================================================================

from __future__ import annotations

import csv
import json
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

import duckdb

from mrna_plum.io.csv_read import detect_csv_dialect, iter_csv_rows_streaming  # masz już w projekcie


# ======================================================================================
# Helpers
# ======================================================================================

def _cfg_get(cfg: Any, path: str, default: Any = None) -> Any:
    cur = cfg
    for part in path.split("."):
        if cur is None:
            return default
        if isinstance(cur, dict):
            cur = cur.get(part, None)
        else:
            cur = getattr(cur, part, None)
    return default if cur is None else cur


def _now_ts() -> str:
    return time.strftime("%Y-%m-%d %H:%M:%S")


def _norm_int(x: Any) -> int:
    if x is None:
        return 0
    s = str(x).strip()
    if s == "":
        return 0
    # czasem CSV ma spacje albo "1 234"
    s = s.replace(" ", "").replace("\u00a0", "")
    try:
        return int(float(s))
    except Exception:
        return 0


def _pick(payload: Dict[str, str], *keys: str) -> str:
    for k in keys:
        if k in payload:
            return payload.get(k, "") or ""
    return ""


# ======================================================================================
# Schema
# ======================================================================================

def ensure_roster_tables(con: duckdb.DuckDBPyConnection) -> None:
    con.execute("CREATE SCHEMA IF NOT EXISTS stage;")
    con.execute("CREATE SCHEMA IF NOT EXISTS dim;")
    con.execute("CREATE SCHEMA IF NOT EXISTS mart;")

    con.execute(
        """
        CREATE TABLE IF NOT EXISTS stage.course_roster_raw (
            course_id                    VARCHAR,
            course_name                  VARCHAR,

            users_total                  BIGINT,
            students_total               BIGINT,
            teachers_total               BIGINT,
            teachers_no_edit             BIGINT,
            teachers_responsible         BIGINT,

            students_enrolled            BIGINT,
            students_completed           BIGINT,
            students_in_progress         BIGINT,
            students_before_start        BIGINT,

            source_file                  VARCHAR,
            loaded_at                    TIMESTAMP DEFAULT now(),
            row_key                      VARCHAR,
            payload_json                 VARCHAR
        );
        """
    )

    # 1 rekord per course_id (ostatni import wygrywa)
    con.execute(
        """
        CREATE TABLE IF NOT EXISTS dim.course_roster (
            course_id                    VARCHAR PRIMARY KEY,
            course_name                  VARCHAR,

            users_total                  BIGINT,
            students_total               BIGINT,
            teachers_total               BIGINT,
            teachers_no_edit             BIGINT,
            teachers_responsible         BIGINT,

            students_enrolled            BIGINT,
            students_completed           BIGINT,
            students_in_progress         BIGINT,
            students_before_start        BIGINT,

            source_file                  VARCHAR,
            loaded_at                    TIMESTAMP
        );
        """
    )

    con.execute(
        """
        CREATE TABLE IF NOT EXISTS mart.roster_import_qa (
            source_file      VARCHAR,
            status           VARCHAR,   -- OK / ERROR
            message          VARCHAR,
            rows_inserted    BIGINT,
            imported_at      TIMESTAMP DEFAULT now()
        );
        """
    )


def _row_key(course_id: str, course_name: str, students_enrolled: int, teachers_total: int) -> str:
    # proste i deterministyczne
    return f"{course_id}|{course_name}|{students_enrolled}|{teachers_total}"


# ======================================================================================
# Main import
# ======================================================================================

def import_course_roster_csv(
    con: duckdb.DuckDBPyConnection,
    roster_csv: Path,
) -> Tuple[int, int]:
    """
    Import CSV -> stage.course_roster_raw, then merge into dim.course_roster.
    Returns: (rows_raw_inserted, rows_dim_merged)
    """
    roster_csv = Path(roster_csv)
    if not roster_csv.exists():
        raise FileNotFoundError(roster_csv)

    ensure_roster_tables(con)

    dialect = detect_csv_dialect(roster_csv)
    rows_to_insert: List[Tuple] = []

    # oczekiwane polskie nagłówki
    for header, row in iter_csv_rows_streaming(roster_csv, dialect=dialect):
        payload = {header[i]: (row[i] if i < len(row) else "") for i in range(len(header))}

        course_id = _pick(payload, "ID kursu", "ID kursu ", "course_id").strip()
        course_name = _pick(payload, "Nazwa kursu", "course_name").strip()

        users_total = _norm_int(_pick(payload, "Użytkownicy"))
        students_total = _norm_int(_pick(payload, "Studenci"))
        teachers_total = _norm_int(_pick(payload, "Nauczyciele"))
        teachers_no_edit = _norm_int(_pick(payload, "Nauczyciele bez praw edycji"))
        teachers_responsible = _norm_int(_pick(payload, "Nauczyciele odpowiedzialny", "Nauczyciele odpowiedzialni"))

        students_enrolled = _norm_int(_pick(payload, "Studenci zapisani"))
        students_completed = _norm_int(_pick(payload, "Studenci po ukończeniu"))
        students_in_progress = _norm_int(_pick(payload, "Studenci w trakcie"))
        students_before_start = _norm_int(_pick(payload, "Studenci przed rozpocząciem", "Studenci przed rozpoczęciem"))

        # minimalny wymóg: course_id
        if course_id == "":
            continue

        rk = _row_key(course_id, course_name, students_enrolled, teachers_total)
        payload_json = json.dumps(payload, ensure_ascii=False, separators=(",", ":"))

        rows_to_insert.append(
            (
                course_id,
                course_name,
                users_total,
                students_total,
                teachers_total,
                teachers_no_edit,
                teachers_responsible,
                students_enrolled,
                students_completed,
                students_in_progress,
                students_before_start,
                str(roster_csv),
                rk,
                payload_json,
            )
        )

    # insert raw
    con.executemany(
        """
        INSERT INTO stage.course_roster_raw (
            course_id, course_name,
            users_total, students_total, teachers_total, teachers_no_edit, teachers_responsible,
            students_enrolled, students_completed, students_in_progress, students_before_start,
            source_file, row_key, payload_json
        )
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);
        """,
        rows_to_insert,
    )
    rows_raw = len(rows_to_insert)

    # merge -> dim (last loaded_at wins)
    # bierzemy ostatni rekord z raw per course_id wg loaded_at (i ewentualnie rowid)
    con.execute(
        """
        INSERT OR REPLACE INTO dim.course_roster
        SELECT
            course_id,
            any_value(course_name) AS course_name,

            any_value(users_total) AS users_total,
            any_value(students_total) AS students_total,
            any_value(teachers_total) AS teachers_total,
            any_value(teachers_no_edit) AS teachers_no_edit,
            any_value(teachers_responsible) AS teachers_responsible,

            any_value(students_enrolled) AS students_enrolled,
            any_value(students_completed) AS students_completed,
            any_value(students_in_progress) AS students_in_progress,
            any_value(students_before_start) AS students_before_start,

            any_value(source_file) AS source_file,
            max(loaded_at) AS loaded_at
        FROM stage.course_roster_raw
        WHERE source_file = ?
        GROUP BY course_id;
        """,
        [str(roster_csv)],
    )

    # ile w dim “dotknięto” – przybliżenie: distinct course_id z tego pliku
    rows_dim = con.execute(
        "SELECT COUNT(DISTINCT course_id) FROM stage.course_roster_raw WHERE source_file = ?;",
        [str(roster_csv)],
    ).fetchone()[0]

    con.execute(
        "INSERT INTO mart.roster_import_qa(source_file, status, message, rows_inserted) VALUES (?, 'OK', ?, ?);",
        [str(roster_csv), f"Imported roster: {rows_raw} raw rows, {rows_dim} courses", rows_raw],
    )

    return int(rows_raw), int(rows_dim)


def build_course_facts_views(con: duckdb.DuckDBPyConnection) -> None:
    """
    Creates/updates views:
      - mart.course_roster_mapped  (course_id -> course_key + roster counts)
      - mart.course_teachers_active (teachers_active per course_key)
      - mart.course_facts (course-level facts for reporting)
    """
    con.execute("CREATE SCHEMA IF NOT EXISTS mart;")
    con.execute("CREATE SCHEMA IF NOT EXISTS dim;")

    # IMPORTANT: musisz mieć gdzieś mapę course_id -> course_key.
    # Najczęściej w pipeline to jest dim.courses(course_id, course_key, course_name) albo podobnie.
    # Tu zakładamy istnienie: dim.courses(course_id, course_key, course_name).
    con.execute(
        """
        CREATE OR REPLACE VIEW mart.course_roster_mapped AS
        SELECT
            r.course_id,
            c.course_key,
            COALESCE(c.course_name, r.course_name) AS course_name,

            COALESCE(r.students_enrolled, 0) AS students_enrolled,
            COALESCE(r.teachers_total, 0) AS teachers_enrolled,

            COALESCE(r.users_total, 0) AS users_total,
            COALESCE(r.students_total, 0) AS students_total,
            COALESCE(r.teachers_no_edit, 0) AS teachers_no_edit,
            COALESCE(r.teachers_responsible, 0) AS teachers_responsible,

            COALESCE(r.students_completed, 0) AS students_completed,
            COALESCE(r.students_in_progress, 0) AS students_in_progress,
            COALESCE(r.students_before_start, 0) AS students_before_start
        FROM dim.course_roster r
        LEFT JOIN dim.courses c
          ON c.course_id::VARCHAR = r.course_id::VARCHAR;
        """
    )

    con.execute(
        """
        CREATE OR REPLACE VIEW mart.course_teachers_active AS
        SELECT
            course_key,
            COUNT(DISTINCT teacher_id) AS teachers_active
        FROM mart.metrics_long
        WHERE visible_active = TRUE
          AND count_value > 0
          AND course_key IS NOT NULL
          AND TRIM(course_key) <> ''
        GROUP BY course_key;
        """
    )

    con.execute(
        """
        CREATE OR REPLACE VIEW mart.course_facts AS
        SELECT
            rm.course_key,
            rm.course_id,
            rm.course_name,
            rm.students_enrolled,
            rm.teachers_enrolled,
            COALESCE(ta.teachers_active, 0) AS teachers_active
        FROM mart.course_roster_mapped rm
        LEFT JOIN mart.course_teachers_active ta
          ON ta.course_key = rm.course_key;
        """
    )

================================================================================
src\mrna_plum\init_project.py
================================================================================

from __future__ import annotations
from pathlib import Path

DEFAULT_DIRS = [
    "IN/logs",
    "IN/activities",
    "OUT/logs",
    "OUT/db",
    "OUT/merged",
    "OUT/excel",
    "OUT/individual",
    "OUT/pdf",
]

def init_project(root: str | Path) -> list[Path]:
    root = Path(root).resolve()
    created: list[Path] = []
    for rel in DEFAULT_DIRS:
        p = root / rel
        if not p.exists():
            p.mkdir(parents=True, exist_ok=True)
            created.append(p)
    return created


================================================================================
src\mrna_plum\io\__init__.py
================================================================================



================================================================================
src\mrna_plum\io\csv_read.py
================================================================================

from __future__ import annotations

import csv
import io
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable, Iterator, Optional, Tuple, List
import pandas as pd

def read_csv_safely(path: Path) -> pd.DataFrame:
    # logi mają polskie znaki → utf-8-sig / cp1250 bywa w praktyce
    # starter: próbujemy kilka kodowań
    encodings = ["utf-8-sig", "utf-8", "cp1250", "latin2"]
    last_err: Exception | None = None

    for enc in encodings:
        try:
            return pd.read_csv(path, encoding=enc, dtype=str, low_memory=False)
        except Exception as e:
            last_err = e

    raise last_err  # type: ignore

@dataclass(frozen=True)
class CsvDialectInfo:
    delimiter: str
    encoding: str  # "utf-8-sig" lub "cp1250"


_TIME_COL_CANDIDATES = ("Czas", "Time", "Date", "TimeCreated")


def detect_encoding(path: Path) -> str:
    raw = path.read_bytes()
    sample = raw[:32768]

    if sample.startswith(b"\xef\xbb\xbf"):
        return "utf-8-sig"

    try:
        sample.decode("utf-8", errors="strict")
        return "utf-8"
    except UnicodeDecodeError:
        pass

    # najczęstsze w PL logach
    try:
        sample.decode("cp1250", errors="strict")
        return "cp1250"
    except UnicodeDecodeError:
        return "iso-8859-2"


def detect_delimiter(sample_text: str) -> str:
    """
    Wymagania: wykrywaj delimiter (TAB / ; / ,)
    Prosta i stabilna heurystyka: policz wystąpienia w pierwszych liniach.
    """
    lines = [ln for ln in sample_text.splitlines() if ln.strip()]
    head = "\n".join(lines[:20]) if lines else sample_text

    candidates = ["\t", ";", ","]
    counts = {c: head.count(c) for c in candidates}
    # Jeśli wszystko 0 -> domyślnie ';' (częste w PL)
    best = max(counts, key=lambda k: counts[k])
    return best if counts[best] > 0 else ";"


def detect_csv_dialect(path: Path) -> CsvDialectInfo:
    enc = detect_encoding(path)
    # czytaj próbkę tekstu do wykrycia delimitera
    with path.open("r", encoding=enc, errors="replace", newline="") as f:
        sample = f.read(16384)
    delim = detect_delimiter(sample)
    # jeśli enc="utf-8" bez BOM, OK; jeśli BOM był, to utf-8-sig
    if enc == "utf-8" and path.read_bytes().startswith(b"\xef\xbb\xbf"):
        enc = "utf-8-sig"
    return CsvDialectInfo(delimiter=delim, encoding=enc)


def iter_csv_rows_streaming(
    path: Path,
    *,
    dialect: Optional[CsvDialectInfo] = None,
) -> Iterator[tuple[list[str], list[str]]]:
    d = dialect or detect_csv_dialect(path)

    # Kolejność prób: najpierw to co wykryte, potem sensowne fallbacki
    enc_try = [d.encoding]
    for e in ("utf-8-sig", "utf-8", "cp1250", "latin2"):
        if e not in enc_try:
            enc_try.append(e)

    last_err: Exception | None = None

    for enc in enc_try:
        try:
            with path.open("r", encoding=enc, errors="strict", newline="") as f:
                reader = csv.reader(f, delimiter=d.delimiter)
                header: Optional[list[str]] = None

                for row in reader:
                    row = [c.strip() for c in row]
                    if not any(row):
                        continue
                    if header is None:
                        header = row
                        continue
                    yield header, row
            return  # <- cały plik przeczytany OK w tym encodingu

        except UnicodeDecodeError as e:
            last_err = e
            continue

    # jeśli nic nie zadziałało
    raise last_err  # type: ignore

def pick_time_column_index(header: list[str]) -> Optional[int]:
    """
    Rozpoznaj: "Czas" (preferowane), ale też Time/Date/TimeCreated
    """
    lowered = [h.strip().lower() for h in header]
    for cand in _TIME_COL_CANDIDATES:
        c = cand.lower()
        if c in lowered:
            return lowered.index(c)
    return None

================================================================================
src\mrna_plum\io\excel_keys.py
================================================================================

from __future__ import annotations
from dataclasses import dataclass
from pathlib import Path
from typing import List
import pandas as pd

from openpyxl import load_workbook
from ..errors import InputDataError

REQUIRED_COLS = [
    "AKTYWNOSC",
    "KLUCZ_TECHNICZNY",
    "OPERACJA",
    "LICZYC_DO_RAPORTU",
    "REGEX_DOPASOWANIA_(Opis)",
    "REGEX_USER_ID_(Opis)",
    "REGEX_OBIEKT_ID_(z dopasowania)",
    "PRIORYTET",
]

def load_keys_sheet(workbook_path: Path, sheet_name: str) -> pd.DataFrame:
    if not workbook_path.exists():
        raise InputDataError(f"KEYS workbook not found: {workbook_path}")

    wb = load_workbook(workbook_path, read_only=True, data_only=True)
    if sheet_name not in wb.sheetnames:
        raise InputDataError(f"KEYS sheet not found: {sheet_name} in {workbook_path}")

    ws = wb[sheet_name]
    rows = list(ws.values)
    if not rows:
        raise InputDataError("KEYS sheet is empty")

    header = [str(x).strip() if x is not None else "" for x in rows[0]]
    data_rows = rows[1:]
    df = pd.DataFrame(data_rows, columns=header)

    missing = [c for c in REQUIRED_COLS if c not in df.columns]
    if missing:
        raise InputDataError(f"KEYS missing columns: {missing}")

    # normalizacja
    df = df.copy()
    df["PRIORYTET"] = pd.to_numeric(df["PRIORYTET"], errors="coerce").fillna(0).astype(int)
    df["LICZYC_DO_RAPORTU"] = df["LICZYC_DO_RAPORTU"].fillna("").astype(str).str.strip()

    # usuń puste reguły
    df = df[df["KLUCZ_TECHNICZNY"].notna() & (df["KLUCZ_TECHNICZNY"].astype(str).str.strip() != "")]
    return df.reset_index(drop=True)


================================================================================
src\mrna_plum\logging_run.py
================================================================================

from __future__ import annotations
import logging
from pathlib import Path

def setup_file_logger(log_path: Path) -> logging.Logger:
    log_path.parent.mkdir(parents=True, exist_ok=True)

    logger = logging.getLogger("mrna_plum")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()

    fh = logging.FileHandler(log_path, encoding="utf-8")
    fmt = logging.Formatter("%(asctime)s %(levelname)s %(message)s")
    fh.setFormatter(fmt)
    logger.addHandler(fh)

    # opcjonalnie: stdout (przy CLI wygodne)
    sh = logging.StreamHandler()
    sh.setFormatter(fmt)
    logger.addHandler(sh)

    return logger


================================================================================
src\mrna_plum\merge\__init__.py
================================================================================

from mrna_plum.store.duckdb_store import open_store
from .merge_logs import merge_logs_into_duckdb

__all__ = [
    "open_store",
    "merge_logs_into_duckdb",
]

================================================================================
src\mrna_plum\merge\merge_logs.py
================================================================================

from __future__ import annotations

import json
import re
import hashlib
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Dict, Iterator, List, Optional

import pyarrow as pa
import pyarrow.parquet as pq

from mrna_plum.io.csv_read import detect_csv_dialect, iter_csv_rows_streaming, pick_time_column_index
from mrna_plum.store.duckdb_store import (
    EventRawRow,
    create_stage_table,
    insert_stage_rows,
    merge_stage_into_events_raw,
    export_course_to_csv,
    export_course_to_parquet,
)

_LOG_NAME_RE = re.compile(r"^logs_(?P<course>.+?)_(?P<ts>\d{8}-\d{4})\.csv$", re.IGNORECASE)

_TIME_FORMATS = (
    # ISO / quasi-ISO
    "%Y-%m-%d %H:%M:%S",
    "%Y-%m-%d %H:%M",
    "%Y-%m-%dT%H:%M:%S",
    "%Y-%m-%dT%H:%M:%S.%f",
    "%Y-%m-%dT%H:%M",
    # PL
    "%d.%m.%Y %H:%M:%S",
    "%d.%m.%Y %H:%M",

    # Moodle/PLUM (częsty eksport): 6-11-25, 15:34:53
    "%d-%m-%y, %H:%M:%S",
    "%d-%m-%y, %H:%M",
)


def merge_logs_to_parquet(
    input_files: list[Path],
    parquet_out: Path,
    *,
    dedup_per_file: bool = True,
    row_group_size: int = 50_000,
) -> int:
    """
    Streamingowy zapis do Parquet:
    - nie używa pandas
    - nie trzyma wszystkich danych w RAM
    - dedup per plik (opcjonalnie)
    """
    parquet_out.parent.mkdir(parents=True, exist_ok=True)

    writer: Optional[pq.ParquetWriter] = None
    total_rows = 0

    try:
        for fp in input_files:
            dialect = detect_csv_dialect(fp)

            # dedup tylko w obrębie tego pliku (set resetowany per plik)
            seen: set[str] = set() if dedup_per_file else set()

            header_current: Optional[list[str]] = None
            batch_cols: dict[str, list[str]] = {}
            batch_cols["_source_file"] = []

            for header, row in iter_csv_rows_streaming(fp, dialect=dialect):
                if header_current is None:
                    header_current = header
                    for h in header_current:
                        batch_cols.setdefault(h, [])

                if dedup_per_file:
                    key = "\x1f".join(row)
                    if key in seen:
                        continue
                    seen.add(key)

                for i, h in enumerate(header):
                    batch_cols[h].append(row[i] if i < len(row) else "")
                batch_cols["_source_file"].append(str(fp))

                if len(batch_cols["_source_file"]) >= row_group_size:
                    table = pa.table(batch_cols)
                    if writer is None:
                        writer = pq.ParquetWriter(parquet_out, table.schema)
                    writer.write_table(table)
                    total_rows += table.num_rows
                    batch_cols = {k: [] for k in table.schema.names}

            if header_current is not None and len(batch_cols["_source_file"]) > 0:
                table = pa.table(batch_cols)
                if writer is None:
                    writer = pq.ParquetWriter(parquet_out, table.schema)
                writer.write_table(table)
                total_rows += table.num_rows

        if writer is None:
            empty = pa.table({})
            pq.write_table(empty, parquet_out)

        return total_rows

    finally:
        if writer is not None:
            writer.close()


def _parse_time_to_iso(value: str) -> Optional[str]:
    v = value.strip()
    if not v:
        return None

    try:
        dt = datetime.fromisoformat(v.replace("Z", "+00:00"))
        return dt.isoformat()
    except Exception:
        pass

    for fmt in _TIME_FORMATS:
        try:
            dt = datetime.strptime(v, fmt)
            return dt.isoformat(sep="T")
        except Exception:
            continue
    return None


def _normalize_fields_key(fields: List[str]) -> str:
    """
    Dedup: CAŁY wiersz identyczny po Trim (reader trimuje).
    Stabilny hash niezależny od delimitera.
    """
    joined = "\x1f".join(fields)
    return hashlib.sha256(joined.encode("utf-8")).hexdigest()


def iter_log_files(root: Path, pattern: str = "*.csv") -> Iterator[Path]:
    for p in root.rglob(pattern):
        if p.is_file():
            yield p


def group_logs_by_course(root: Path) -> Dict[str, List[Path]]:
    grouped: Dict[str, List[Path]] = {}
    for p in iter_log_files(root):
        m = _LOG_NAME_RE.match(p.name)
        if not m:
            continue
        course = m.group("course")
        grouped.setdefault(course, []).append(p)

    # stabilne sortowanie po nazwie (timestamp jest w nazwie)
    for course in list(grouped.keys()):
        grouped[course].sort(key=lambda x: x.name)
    return grouped


@dataclass(frozen=True)
class MergeLogsResult:
    courses: int
    files: int
    inserted_rows: int


def merge_logs_into_duckdb(
    *,
    root: Path,
    con,  # duckdb connection
    export_mode: str = "duckdb",  # "duckdb" | "parquet" | "csv"
    export_dir: Optional[Path] = None,
    chunk_size: int = 2000,
) -> MergeLogsResult:
    """
    Publiczny entrypoint zgodny z testami/CLI.

    - root: folder z logami logs_<KURS>_<YYYYMMDD-HHMM>.csv (rekurencyjnie)
    - export_mode: "duckdb" (domyślnie) lub "csv"/"parquet"
    - export_dir: wymagany dla "csv"/"parquet"
    - chunk_size: batch insert do stage
    """
    export_mode = (export_mode or "duckdb").lower().strip()
    if export_mode not in ("duckdb", "csv", "parquet"):
        raise ValueError(f"export_mode must be 'duckdb'|'csv'|'parquet', got: {export_mode}")

    if export_mode in ("csv", "parquet") and export_dir is None:
        raise ValueError("export_dir is required for export_mode=csv/parquet")

    return _merge_logs_into_duckdb_impl(
        root=root,
        con=con,
        export_mode=export_mode,
        export_dir=export_dir,
        chunk_size=chunk_size,
    )

_COURSE_ID_PATTERNS = [
    # najczęstsze w Moodle logs (EN)
    re.compile(r"course with id\s+'(\d+)'", re.IGNORECASE),
    re.compile(r"course with id\s+(\d+)", re.IGNORECASE),

    # czasem bez apostrofów
    re.compile(r"\bcourse id\b\s*[:=]?\s*(\d+)", re.IGNORECASE),

    # PL warianty (na wszelki wypadek)
    re.compile(r"\bid kursu\b\s*[:=]?\s*(\d+)", re.IGNORECASE),
]


def _extract_course_id_from_payload(payload: Dict[str, str]) -> Optional[int]:
    """
    Szukamy course_id głównie w polu 'Opis' (czasem 'Description'),
    ale dla bezpieczeństwa przeszukujemy też cały payload.
    """
    candidates: List[str] = []

    # preferowane pola
    for k in ("Opis", "Description", "Event description", "Nazwa zdarzenia", "Kontekst zdarzenia"):
        v = payload.get(k)
        if v:
            candidates.append(v)

    # fallback: cały payload (join wartości)
    if not candidates:
        candidates.append(" | ".join([v for v in payload.values() if v]))

    text = " \n ".join(candidates)

    for rx in _COURSE_ID_PATTERNS:
        m = rx.search(text)
        if m:
            try:
                return int(m.group(1))
            except Exception:
                return None
    return None

def _merge_logs_into_duckdb_impl(
    *,
    root: Path,
    con,
    export_mode: str,
    export_dir: Optional[Path],
    chunk_size: int,
) -> MergeLogsResult:
    """
    Rdzeń logiki scalania.
    """
    try:
        grouped = group_logs_by_course(root)
        if not grouped:
            raise RuntimeError(
                f"No log files matched pattern logs_<COURSE>_<YYYYMMDD-HHMM>.csv under root={root}"
            )
        total_files = sum(len(v) for v in grouped.values())
        total_inserted = 0

        for course, files in grouped.items():
            create_stage_table(con)
            buf: List[EventRawRow] = []

            for fpath in files:
                dialect = detect_csv_dialect(fpath)

                time_idx: Optional[int] = None
                header_ref: Optional[List[str]] = None

                for header, row in iter_csv_rows_streaming(fpath, dialect=dialect):
                    if header_ref is None:
                        header_ref = header
                        time_idx = pick_time_column_index(header_ref)

                    payload = {header[i]: (row[i] if i < len(row) else "") for i in range(len(header))}
                    payload_json = json.dumps(payload, ensure_ascii=False, separators=(",", ":"))
                    course_id = _extract_course_id_from_payload(payload)

                    time_text = None
                    time_iso = None
                    if time_idx is not None and time_idx < len(row):
                        time_text = row[time_idx]
                        time_iso = _parse_time_to_iso(time_text)

                    row_key = _normalize_fields_key(row)

                    buf.append(
                        EventRawRow(
                            course=course,
                            course_id=course_id,
                            time_text=time_text,
                            time_ts_iso=time_iso,
                            row_key=row_key,
                            payload_json=payload_json,
                            source_file=str(fpath),
                        )
                    )

                    if len(buf) >= chunk_size:
                        insert_stage_rows(con, buf)
                        buf.clear()

            if buf:
                insert_stage_rows(con, buf)
                buf.clear()

            inserted = merge_stage_into_events_raw(con)
            total_inserted += inserted

            if export_mode in ("csv", "parquet"):
                assert export_dir is not None
                export_dir.mkdir(parents=True, exist_ok=True)

                if export_mode == "csv":
                    out_csv = export_dir / f"{course}_full_log.csv"
                    export_course_to_csv(con, course=course, out_csv=out_csv)
                else:
                    out_pq = export_dir / f"{course}_full_log.parquet"
                    export_course_to_parquet(con, course=course, out_parquet=out_pq)

        return MergeLogsResult(courses=len(grouped), files=total_files, inserted_rows=total_inserted)

    except UnicodeDecodeError as e:
        # Bezpieczny komunikat (bez ryzyka UnboundLocalError)
        raise RuntimeError(f"UnicodeDecodeError while reading CSV under root={root}") from e


__all__ = ["merge_logs_into_duckdb", "MergeLogsResult", "merge_logs_to_parquet"]

================================================================================
src\mrna_plum\merge\test_merge_logs.py
================================================================================

from __future__ import annotations

import csv
from pathlib import Path

import duckdb
import pytest

from mrna_plum.store.duckdb_store import open_store
from mrna_plum.merge.merge_logs import merge_logs_to_parquet
from mrna_plum.merge.merge_logs import merge_logs_into_duckdb, MergeLogsResult


def _write_csv_bytes(path: Path, data: bytes) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_bytes(data)


def _make_logs_name(course: str, ts: str) -> str:
    return f"logs_{course}_{ts}.csv"


def test_bom_utf8_and_delimiter_detection_and_insert(tmp_path: Path):
    root = tmp_path / "logs"
    db = tmp_path / "db.duckdb"

    # UTF-8 with BOM, delimiter ';'
    content = (
        "\ufeffCzas;Akcja;Opis\n"
        "2026-02-18 10:00:00;ADD;Zażółć gęślą\n"
    ).encode("utf-8")

    _write_csv_bytes(root / _make_logs_name("KURS1", "20260218-1000"), content)

    con = open_store(db)
    try:
        res = merge_logs_into_duckdb(root=root, con=con, export_mode="duckdb")
        assert res.courses == 1
        assert res.files == 1
        assert res.inserted_rows == 1

        row = con.execute(
            "SELECT course, payload_json FROM events_raw WHERE course='KURS1'"
        ).fetchone()
        assert row[0] == "KURS1"
        assert "Zażółć gęślą" in row[1]
    finally:
        con.close()


def test_windows_1250_polish_chars(tmp_path: Path):
    root = tmp_path / "logs"
    db = tmp_path / "db.duckdb"

    # cp1250, delimiter TAB
    text = "Czas\tAkcja\tOpis\n2026-02-18 10:00:00\tADD\tŚliwka w kompot\n"
    _write_csv_bytes(root / _make_logs_name("KURS2", "20260218-1000"), text.encode("cp1250"))

    con = open_store(db)
    try:
        res = merge_logs_into_duckdb(root=root, con=con, export_mode="duckdb")
        assert res.inserted_rows == 1

        payload = con.execute(
            "SELECT payload_json FROM events_raw WHERE course='KURS2'"
        ).fetchone()[0]
        assert "Śliwka w kompot" in payload
    finally:
        con.close()


def test_dedup_identical_whole_row_after_trim(tmp_path: Path):
    root = tmp_path / "logs"
    db = tmp_path / "db.duckdb"

    # dwa pliki, ten sam wiersz różniący się tylko spacingiem -> po trim ma być 1 wpis
    c1 = (
        "Czas;Akcja;Opis\n"
        "2026-02-18 10:00:00;ADD;  Duplikat  \n"
    ).encode("utf-8")
    c2 = (
        "Czas;Akcja;Opis\n"
        "2026-02-18 10:00:00;ADD;Duplikat\n"
    ).encode("utf-8")

    _write_csv_bytes(root / _make_logs_name("KURS3", "20260218-1000"), c1)
    _write_csv_bytes(root / _make_logs_name("KURS3", "20260218-1001"), c2)

    con = open_store(db)
    try:
        res = merge_logs_into_duckdb(root=root, con=con, export_mode="duckdb")
        assert res.inserted_rows == 1

        cnt = con.execute("SELECT COUNT(*) FROM events_raw WHERE course='KURS3'").fetchone()[0]
        assert cnt == 1
    finally:
        con.close()


def test_sorting_desc_by_time_on_export_csv(tmp_path: Path):
    root = tmp_path / "logs"
    db = tmp_path / "db.duckdb"
    out_dir = tmp_path / "out"

    # kolejność w pliku: starszy potem nowszy (celowo)
    content = (
        "Czas;Akcja;Opis\n"
        "2026-02-18 09:00:00;ADD;A\n"
        "2026-02-18 10:00:00;ADD;B\n"
    ).encode("utf-8")

    _write_csv_bytes(root / _make_logs_name("KURS4", "20260218-1000"), content)

    con = open_store(db)
    try:
        res = merge_logs_into_duckdb(root=root, con=con, export_mode="csv", export_dir=out_dir)
        assert res.inserted_rows == 2

        out_csv = out_dir / "KURS4_full_log.csv"
        assert out_csv.exists()

        # W eksporcie ma być B przed A (czas malejąco)
        rows = out_csv.read_text(encoding="utf-8").splitlines()
        # Header w COPY jest: course;time_text;time_ts;payload_json;source_file
        assert len(rows) >= 3
        assert '"B"' in rows[1] or "B" in rows[1]
        assert '"A"' in rows[2] or "A" in rows[2]
    finally:
        con.close()


================================================================================
src\mrna_plum\parse\__init__.py
================================================================================

from .parse_logs import parse_merged_parquet
__all__ = ["parse_merged_parquet"]


================================================================================
src\mrna_plum\parse\context.py
================================================================================

from __future__ import annotations
import re
from dataclasses import dataclass
from typing import Optional

@dataclass(frozen=True)
class ContextInfo:
    course_code: Optional[str]
    period: Optional[str]

def parse_context(context: str, course_regex: str, period_regex: str) -> ContextInfo:
    course = None
    period = None

    cm = re.search(course_regex, context or "")
    if cm:
        course = cm.group(1)

    pm = re.search(period_regex, context or "")
    if pm:
        period = pm.group(1)

    return ContextInfo(course_code=course, period=period)


================================================================================
src\mrna_plum\parse\parse_events.py
================================================================================

# src/mrna_plum/parse/parse_events.py

from __future__ import annotations

import json
import os
import re
import sys
from dataclasses import asdict
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

import pandas as pd

from mrna_plum.rules.activity_rules import ActivityRuleEngine, load_keys_rules
from mrna_plum.store.database import EventStore


COURSE_CTX_RX = re.compile(
    r"Kurs:\s*(?P<course_code>[A-Z]{1,3}/[A-Za-z]{1,6}/[A-Za-z0-9_]+/(?P<semester>\d+sem)-(?P<name>.+?)-(?P<ay>\d{4}/\d{2})(?P<term>[zl])",
    re.IGNORECASE,
)

# fallback, jeśli czas nieparsowalny
def parse_ts_to_utc(s: str) -> Optional[datetime]:
    if not s:
        return None
    s = str(s).strip()
    if not s:
        return None
    # epoch?
    if s.isdigit():
        try:
            x = int(s)
            if x > 10_000_000_000:
                x //= 1000
            return datetime.fromtimestamp(x, tz=timezone.utc)
        except Exception:
            return None
    try:
        ts = pd.to_datetime(s, utc=True, errors="coerce")
        if pd.isna(ts):
            return None
        return ts.to_pydatetime()
    except Exception:
        return None


def parse_course_context(kontekst: str) -> Optional[Dict[str, str]]:
    if not kontekst:
        return None
    m = COURSE_CTX_RX.search(kontekst)
    if not m:
        return None
    gd = m.groupdict()
    course_code = gd["course_code"]
    parts = course_code.split("/")
    # WF/An/stj/5sem-...
    wydzial = parts[0] if len(parts) > 0 else ""
    kierunek = parts[1] if len(parts) > 1 else ""
    track = parts[2] if len(parts) > 2 else ""

    return {
        "course_code": course_code,
        "wydzial_code": wydzial,
        "kierunek_code": kierunek,
        "track_code": track,
        "semester_code": gd.get("semester", ""),
        "course_name": gd.get("name", "").strip(),
        "ay": gd.get("ay", ""),
        "term": gd.get("term", "").lower(),
    }


def ensure_run_paths(root: Path) -> Dict[str, Path]:
    run_dir = root / "_run"
    run_dir.mkdir(parents=True, exist_ok=True)
    return {
        "run_dir": run_dir,
        "run_log": run_dir / "run.log",
        "progress": run_dir / "progress.jsonl",
        "ok": run_dir / "parse.ok",
    }


class RunLogger:
    def __init__(self, run_log: Path, progress: Path):
        self.run_log = run_log
        self.progress = progress

    def log(self, msg: str) -> None:
        line = f"{datetime.now().isoformat(timespec='seconds')} {msg}\n"
        self.run_log.open("a", encoding="utf-8").write(line)

    def progress_event(self, obj: Dict[str, Any]) -> None:
        self.progress.open("a", encoding="utf-8").write(json.dumps(obj, ensure_ascii=False) + "\n")


def _cfg_filters(cfg: Dict[str, Any]) -> Dict[str, Any]:
    f = cfg.get("filters", {}) or {}
    parse_cfg = cfg.get("parse_events", {}) or {}
    # dopuszczamy override w parse_events.*
    return {**f, **(parse_cfg.get("filters") or {})}


def _is_student_email(payload: Dict[str, Any], student_domain: str) -> bool:
    # payload może mieć różne kolumny; przeszukaj wszystkie wartości z '@'
    domain = student_domain.lower().strip()
    if not domain:
        return False
    blob = " ".join([str(v) for v in payload.values() if v is not None])
    return domain in blob.lower()


def _source_allowed(payload: Dict[str, Any], allowed_sources: List[str], blocked_sources: List[str]) -> bool:
    src = str(payload.get("Źródło") or payload.get("Zrodlo") or payload.get("Source") or "").strip()
    if blocked_sources and any(b.lower() in src.lower() for b in blocked_sources):
        return False
    if allowed_sources:
        return any(a.lower() in src.lower() for a in allowed_sources)
    return True


def _techkey_allowed(tech_key: str, wl: List[str], bl: List[str]) -> bool:
    if bl and tech_key in bl:
        return False
    if wl and tech_key not in wl:
        return False
    return True


def _date_allowed(ts: datetime, date_from: Optional[str], date_to: Optional[str]) -> bool:
    if date_from:
        d1 = pd.to_datetime(date_from, utc=True, errors="coerce")
        if not pd.isna(d1) and ts < d1.to_pydatetime():
            return False
    if date_to:
        d2 = pd.to_datetime(date_to, utc=True, errors="coerce")
        if not pd.isna(d2) and ts > d2.to_pydatetime():
            return False
    return True


def run_parse_events(
    cfg: Dict[str, Any],
    root: str,
    keys_xlsx_override: Optional[str] = None,
) -> int:
    """
    Exit codes:
      0 = OK
      2 = mixed periods
      1 = other error
    """
    root_p = Path(root)
    paths = ensure_run_paths(root_p)
    logger = RunLogger(paths["run_log"], paths["progress"])

    try:
        logger.log("[PARSE] start parse-events")
        store = EventStore(cfg)
        store.ensure_schema()

        # KEYS
        keys_cfg = cfg.get("parse_events", {}) or {}

        # 1️ priorytet: CLI override
        keys_xlsx = keys_xlsx_override

        # 2️ fallback: config.yaml
        if not keys_xlsx:
            keys_xlsx = keys_cfg.get("keys_xlsx")

        if not keys_xlsx:
            raise ValueError(
                "Brak KEYS: podaj --keys-xlsx lub ustaw parse_events.keys_xlsx w config.yaml"
            )

        # jeśli w config używasz {root}
        if "{root}" in keys_xlsx:
            keys_xlsx = keys_xlsx.replace("{root}", root)

        keys_sheet = keys_cfg.get("keys_sheet", "KEYS")
        rules = load_keys_rules(keys_xlsx, sheet_name=keys_sheet)
        engine = ActivityRuleEngine(rules, drop_mode_nie=True)
        logger.log(f"[PARSE] loaded KEYS rules: {len(rules)}")

        # filtry
        fil = _cfg_filters(cfg)
        student_domain = (fil.get("student_email_domain") or "@student.umw.edu.pl").lower()
        tech_wl = fil.get("tech_key_whitelist") or []
        tech_bl = fil.get("tech_key_blacklist") or []
        allowed_sources = fil.get("source_whitelist") or []
        blocked_sources = fil.get("source_blacklist") or []
        date_from = fil.get("date_from")
        date_to = fil.get("date_to")

        # DuckDB streaming z events_raw
        import duckdb
        con = duckdb.connect(str(Path(cfg["paths"]["db_path"])))
        con.execute("PRAGMA enable_progress_bar=false;")

        # incremental: bierz tylko te, których row_key nie ma w canonical_raw
        query = """
            SELECT course, time_text, time_ts_iso, row_key, payload_json, source_file
            FROM events_raw r
            WHERE NOT EXISTS (SELECT 1 FROM events_canonical_raw c WHERE c.row_key = r.row_key)
        """
        cur = con.execute(query)

        batch: List[Dict[str, Any]] = []
        conf_batch: List[Dict[str, Any]] = []

        seen_period: Optional[Tuple[str, str]] = None  # (ay, term)
        mixed_period = False

        total_read = 0
        total_matched = 0
        total_inserted = 0

        FETCH = int(keys_cfg.get("fetch_size", 5000))
        INSERT_BATCH = int(keys_cfg.get("insert_batch_size", 20000))

        while True:
            rows = cur.fetchmany(FETCH)
            if not rows:
                break

            for course, time_text, time_ts_iso, row_key, payload_json, source_file in rows:
                total_read += 1

                # payload
                try:
                    payload = json.loads(payload_json)
                except Exception:
                    continue

                # źródło filter
                if not _source_allowed(payload, allowed_sources, blocked_sources):
                    continue

                # student filter
                if student_domain and _is_student_email(payload, student_domain):
                    continue

                # czas
                ts = None
                if time_ts_iso:
                    try:
                        ts = pd.to_datetime(time_ts_iso, utc=True, errors="coerce")
                        ts = None if pd.isna(ts) else ts.to_pydatetime()
                    except Exception:
                        ts = None
                if ts is None:
                    ts = parse_ts_to_utc(str(payload.get("Czas") or payload.get("Time") or payload.get("Date") or time_text or ""))
                if ts is None:
                    continue

                if not _date_allowed(ts, date_from, date_to):
                    continue

                # kontekst kursu
                kontekst = str(payload.get("Kontekst zdarzenia") or payload.get("Event context") or "")
                ctx = parse_course_context(kontekst)
                if not ctx:
                    continue

                # okres: ay+term
                period = (ctx["ay"], ctx["term"])
                if seen_period is None:
                    seen_period = period
                elif period != seen_period:
                    mixed_period = True
                    # logujemy i kończymy po batchu
                    logger.log(f"[PARSE][ERR] mixed periods: first={seen_period} next={period} row_key={row_key}")
                    break

                # opis (KEYS dopasowanie)
                opis = str(payload.get("Opis") or payload.get("Description") or "")
                m = engine.match(opis)
                if not m:
                    continue  # aktywności spoza KEYS → pomijamy

                # whitelist/blacklist tech_key
                if not _techkey_allowed(m.tech_key, tech_wl, tech_bl):
                    continue

                total_matched += 1

                if m.conflict:
                    conf_batch.append(
                        {
                            "row_key": row_key,
                            "course_code": ctx["course_code"],
                            "teacher_id": m.teacher_id,
                            "tech_key": m.tech_key,
                            "operation": m.operation,
                            "object_id": m.object_id,
                            "note": f"KEYS conflict: multiple matches with same priority={m.priority}",
                        }
                    )

                row = {
                    "row_key": row_key,
                    "course": course,
                    "course_code": ctx["course_code"],
                    "wydzial_code": ctx["wydzial_code"],
                    "kierunek_code": ctx["kierunek_code"],
                    "track_code": ctx["track_code"],
                    "semester_code": ctx["semester_code"],
                    "course_name": ctx["course_name"],
                    "ay": ctx["ay"],
                    "term": ctx["term"],
                    "ts_utc": ts,
                    "teacher_id": m.teacher_id,
                    "operation": m.operation,
                    "tech_key": m.tech_key,
                    "activity_label": m.activity_label,
                    "object_id": m.object_id,
                    "count_mode": m.count_mode,
                    "raw_line_hash": row_key,  # row_key już jest hashem całego wiersza po trim
                    "source_file": source_file,
                    "payload_json": payload_json,
                }
                batch.append(row)

                if len(batch) >= INSERT_BATCH:
                    total_inserted += store.insert_raw_batch(batch)
                    batch.clear()
                if len(conf_batch) >= 2000:
                    store.insert_conflicts_batch(conf_batch)
                    conf_batch.clear()

            if mixed_period:
                break

            # progress co chunk
            if total_read % (FETCH * 2) == 0:
                logger.progress_event(
                    {
                        "stage": "parse-events",
                        "read": total_read,
                        "matched": total_matched,
                        "inserted_raw": total_inserted,
                        "period": {"ay": seen_period[0], "term": seen_period[1]} if seen_period else None,
                    }
                )

        if batch:
            total_inserted += store.insert_raw_batch(batch)
            batch.clear()
        if conf_batch:
            store.insert_conflicts_batch(conf_batch)
            conf_batch.clear()

        con.close()

        if mixed_period:
            logger.log("[PARSE][ERR] mixed periods -> abort")
            return 2

        # finalize counted/unieważnienia
        store.finalize_canonical()
        pq = store.export_parquet() if (cfg.get("parse_events", {}) or {}).get("export_parquet", False) else None

        paths["ok"].write_text("OK\n", encoding="utf-8")
        logger.log(f"[PARSE] OK read={total_read} matched={total_matched} inserted_raw={total_inserted} parquet={pq}")
        logger.progress_event(
            {
                "stage": "parse-events",
                "status": "ok",
                "read": total_read,
                "matched": total_matched,
                "inserted_raw": total_inserted,
                "period": {"ay": seen_period[0], "term": seen_period[1]} if seen_period else None,
            }
        )
        return 0

    except Exception as e:
        logger.log(f"[PARSE][ERR] {type(e).__name__}: {e}")
        return 1

================================================================================
src\mrna_plum\parse\parse_logs.py
================================================================================

from __future__ import annotations
from pathlib import Path
import pandas as pd

from ..config import AppConfig
from ..errors import MixedPeriodsError, ProcessingError
from ..rules.engine import match_best_rule
from .context import parse_context

def parse_merged_parquet(
    parquet_in: Path,
    parquet_out: Path,
    config: AppConfig,
    rules: list,
) -> tuple[int, str | None]:
    df = pd.read_parquet(parquet_in)
    if df.empty:
        parquet_out.parent.mkdir(parents=True, exist_ok=True)
        df.to_parquet(parquet_out, index=False)
        return 0, None

    # wymagane kolumny
    need = [config.col_time, config.col_context, config.col_desc, config.col_component, config.col_event_name]
    missing = [c for c in need if c not in df.columns]
    if missing:
        raise ProcessingError(f"Missing required CSV columns: {missing}")

    # wyciągnij kontekst (kurs, okres)
    ctx = df[config.col_context].astype(str).apply(lambda x: parse_context(x, config.course_regex, config.period_regex))
    df["course_code"] = ctx.apply(lambda c: c.course_code)
    df["period"] = ctx.apply(lambda c: c.period)

    # mixed periods check (ignorujemy None)
    periods = sorted({p for p in df["period"].dropna().unique().tolist() if str(p).strip() != ""})
    if len(periods) > 1:
        raise MixedPeriodsError(f"mixed periods detected: {periods}")
    run_period = periods[0] if periods else None

    # dopasuj regułę po opisie
    tech_keys = []
    activities = []
    operations = []
    count_flags = []
    teacher_ids = []
    object_ids = []
    matched_prio = []

    for desc in df[config.col_desc].astype(str).tolist():
        m = match_best_rule(desc, rules)
        if m is None:
            tech_keys.append(None)
            activities.append(None)
            operations.append(None)
            count_flags.append(False)
            teacher_ids.append(None)
            object_ids.append(None)
            matched_prio.append(None)
        else:
            tech_keys.append(m.tech_key)
            activities.append(m.activity)
            operations.append(m.operation)
            count_flags.append(bool(m.count_to_report))
            teacher_ids.append(m.teacher_id)
            object_ids.append(m.object_id)
            matched_prio.append(m.priority)

    df["tech_key"] = tech_keys
    df["activity"] = activities
    df["operation"] = operations
    df["count_to_report"] = count_flags
    df["teacher_id"] = teacher_ids
    df["object_id"] = object_ids
    df["rule_priority"] = matched_prio

    parquet_out.parent.mkdir(parents=True, exist_ok=True)
    df.to_parquet(parquet_out, index=False)
    return len(df), run_period


================================================================================
src\mrna_plum\paths.py
================================================================================

from dataclasses import dataclass
from pathlib import Path

@dataclass(frozen=True)
class ProjectPaths:
    root: Path

    @property
    def run_dir(self) -> Path:
        return self.root / "_run"

    @property
    def data_dir(self) -> Path:
        return self.root / "_data"

    @property
    def parquet_dir(self) -> Path:
        return self.data_dir / "parquet"

    @property
    def duckdb_path(self) -> Path:
        return self.data_dir / "mrna_plum.duckdb"

    @property
    def markers_dir(self) -> Path:
        return self.run_dir

    def marker_path(self, step: str) -> Path:
        return self.markers_dir / f"{step}.ok"


================================================================================
src\mrna_plum\reports\__init__.py
================================================================================

from .export_excel import export_summary_excel, ExportOverflowError


__all__ = ["export_summary_excel", "ExportOverflowError"]

================================================================================
src\mrna_plum\reports\export_excel.py
================================================================================

from __future__ import annotations

import json
import logging
import math
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple

# xlsxwriter is used directly (fast, pyinstaller-friendly)
import xlsxwriter


# ===== Exit codes (align with project) =====
EXIT_OK = 0
EXIT_OVERFLOW = 30


@dataclass(frozen=True)
class ExportExcelConfig:
    max_rows_excel: int = 1_000_000
    overflow_strategy: str = "error"  # error | split | skip
    activity_column: str = "activity_label"  # NOW default
    course_column: str = "course_name"       # NOW default

    include_hr_cols: bool = True
    exclude_zero_counts: bool = True
    percent_excel_format: bool = True

class ExportExcelError(RuntimeError):
    pass


class ExportOverflowError(ExportExcelError):
    """Raised when row-count exceeds Excel limit and strategy=error."""
    pass


def _ensure_dirs(root: Path) -> Tuple[Path, Path]:
    run_dir = root / "_run"
    out_dir = root / "_out"
    run_dir.mkdir(parents=True, exist_ok=True)
    out_dir.mkdir(parents=True, exist_ok=True)
    return run_dir, out_dir


def _setup_logger(run_dir: Path) -> logging.Logger:
    logger = logging.getLogger("mrna_plum.export_excel")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()

    fh = logging.FileHandler(run_dir / "run.log", encoding="utf-8")
    fmt = logging.Formatter("%(asctime)s %(levelname)s %(message)s")
    fh.setFormatter(fmt)
    logger.addHandler(fh)

    # optional console handler (kept minimal)
    sh = logging.StreamHandler()
    sh.setFormatter(fmt)
    logger.addHandler(sh)

    return logger


def _progress_append(run_dir: Path, event: Dict[str, Any]) -> None:
    p = run_dir / "progress.jsonl"
    event = dict(event)
    event["ts"] = datetime.now(timezone.utc).isoformat()
    with p.open("a", encoding="utf-8") as f:
        f.write(json.dumps(event, ensure_ascii=False) + "\n")


def _duckdb_has_column(con, table_fqn: str, col: str) -> bool:
    # Works for DuckDB: INFORMATION_SCHEMA.COLUMNS
    sql = """
        SELECT 1
        FROM information_schema.columns
        WHERE table_schema = ? AND table_name = ? AND column_name = ?
        LIMIT 1
    """
    if "." in table_fqn:
        schema, name = table_fqn.split(".", 1)
    else:
        schema, name = "main", table_fqn
    row = con.execute(sql, [schema, name, col]).fetchone()
    return row is not None


def _select_metrics_long_sql(con, activity_col: str, course_col: str,
                            include_hr_cols: bool, exclude_zero_counts: bool,
                            percent_excel_format: bool) -> str:
    table = "mart.metrics_long"

    # activity_label fallback
    if activity_col == "activity_label" and not _duckdb_has_column(con, table, "activity_label"):
        activity_col = "tech_key"

    # course_name fallback
    if course_col == "course_name" and not _duckdb_has_column(con, table, "course_name"):
        course_col = "course_code"

    pct_course = "pct_course" if _duckdb_has_column(con, table, "pct_course") else "NULL"
    pct_program = "pct_program" if _duckdb_has_column(con, table, "pct_program") else "NULL"
    pct_faculty = "pct_faculty" if _duckdb_has_column(con, table, "pct_faculty") else "NULL"
    pct_university = "pct_university" if _duckdb_has_column(con, table, "pct_university") else "NULL"

    # Excel %: zapisujemy ułamek (12.3 -> 0.123)
    def pct_expr(expr: str) -> str:
        if expr == "NULL":
            return "NULL"
        return f"({expr} / 100.0)" if percent_excel_format else expr

    pct_course_e = pct_expr(pct_course)
    pct_program_e = pct_expr(pct_program)
    pct_faculty_e = pct_expr(pct_faculty)
    pct_university_e = pct_expr(pct_university)

    order_course = "course_code" if _duckdb_has_column(con, table, "course_code") else course_col
    order_tech = "tech_key" if _duckdb_has_column(con, table, "tech_key") else activity_col

    where_parts = []
    if _duckdb_has_column(con, table, "visible_active"):
        where_parts.append("visible_active")
    if exclude_zero_counts and _duckdb_has_column(con, table, "count_value"):
        where_parts.append("count_value <> 0")

    where_clause = f"WHERE {' AND '.join(where_parts)}" if where_parts else ""

    # HR columns (optional, only if present)
    hr_cols = []
    if include_hr_cols:
        for col in ("hr_faculty", "hr_unit", "hr_department", "hr_org"):
            if _duckdb_has_column(con, table, col):
                hr_cols.append(f"{col}::VARCHAR AS {col}")

    hr_select = (",\n            " + ",\n            ".join(hr_cols)) if hr_cols else ""

    sql = f"""
        SELECT
            full_name::VARCHAR AS full_name,
            teacher_id::VARCHAR AS teacher_id{hr_select},
            {course_col}::VARCHAR AS course_value,
            {activity_col}::VARCHAR AS activity_value,
            count_value::BIGINT AS count_value,
            {pct_course_e}::DOUBLE AS pct_course,
            {pct_program_e}::DOUBLE AS pct_program,
            {pct_faculty_e}::DOUBLE AS pct_faculty,
            {pct_university_e}::DOUBLE AS pct_university,
            {order_tech}::VARCHAR AS _order_tech
        FROM {table}
        {where_clause}
        ORDER BY
            full_name ASC,
            {order_course} ASC,
            {order_tech} ASC
    """
    return sql


def _count_rows(con, base_sql: str) -> int:
    # Wrap in subquery; DuckDB handles it well.
    sql = f"SELECT COUNT(*)::BIGINT FROM ({base_sql}) t"
    return int(con.execute(sql).fetchone()[0])


def _select_metrics_qa_sql(con) -> str:
    table = "mart.metrics_qa"
    # Minimal required columns; if missing -> NULL
    type_col = "type" if _duckdb_has_column(con, table, "type") else "NULL"
    teacher_id = "teacher_id" if _duckdb_has_column(con, table, "teacher_id") else "NULL"
    course_code = "course_code" if _duckdb_has_column(con, table, "course_code") else "NULL"
    tech_key = "tech_key" if _duckdb_has_column(con, table, "tech_key") else "NULL"
    description = "description" if _duckdb_has_column(con, table, "description") else "NULL"

    sql = f"""
        SELECT
            {type_col}::VARCHAR AS type,
            {teacher_id}::VARCHAR AS teacher_id,
            {course_code}::VARCHAR AS course_code,
            {tech_key}::VARCHAR AS tech_key,
            {description}::VARCHAR AS description
        FROM {table}
        ORDER BY type ASC, teacher_id ASC, course_code ASC, tech_key ASC
    """
    return sql


def _write_sheet_header(ws, header_fmt, headers: Sequence[str]) -> None:
    ws.write_row(0, 0, list(headers), header_fmt)
    ws.freeze_panes(1, 0)


def _iter_cursor_rows(cur, batch_size: int) -> Iterable[Tuple[Any, ...]]:
    while True:
        rows = cur.fetchmany(batch_size)
        if not rows:
            break
        for r in rows:
            yield r


def _write_metrics_long_split_streaming(
    workbook: xlsxwriter.Workbook,
    con,
    sql: str,
    max_rows: int,
    overflow_strategy: str,
    main_sheet_base: str,
    logger: logging.Logger,
) -> Tuple[int, int]:
    """
    Stream rows from DuckDB and write to one or multiple sheets.
    Returns: (written_rows, sheets_count) excluding header row.
    """
    headers = [
        "Użytkownik",
        "ID",
        "Kurs",
        "Aktywność",
        "Liczba",
        "% kurs",
        "% kierunek",
        "% wydział",
        "% uczelnia",
    ]

    header_fmt = workbook.add_format({"bold": True, "border": 1})
    # Percent format: number with 1 decimal place, NOT Excel percentage
    pct_fmt = workbook.add_format({"num_format": "0.0"})
    int_fmt = workbook.add_format({"num_format": "0"})
    text_fmt = workbook.add_format({})  # default

    # column widths (optional but helpful)
    col_widths = [28, 12, 18, 26, 10, 10, 12, 12, 12]

    def make_sheet(idx: int):
        name = main_sheet_base if idx == 1 else f"{main_sheet_base}_{idx}"
        ws = workbook.add_worksheet(name[:31])
        for i, w in enumerate(col_widths):
            ws.set_column(i, i, w)
        _write_sheet_header(ws, header_fmt, headers)
        return ws

    ws = make_sheet(1)
    sheet_idx = 1
    row_in_sheet = 1  # start after header
    total_written = 0

    cur = con.execute(sql)
    colnames = [d[0] for d in cur.description]
    hr_present = [c for c in colnames if c.startswith("hr_")]
    cur.arraysize = 10_000

    for rec in _iter_cursor_rows(cur, batch_size=10_000):
        # If current sheet is full:
        if row_in_sheet > max_rows:
            if overflow_strategy == "split":
                sheet_idx += 1
                ws = make_sheet(sheet_idx)
                row_in_sheet = 1
            else:
                # overflow_strategy 'skip' shouldn't land here (handled earlier),
                # and 'error' should be prevented by pre-count logic.
                break

        # rec order:
        # full_name, teacher_id, course_value, activity_value, count_value,
        # pct_course, pct_program, pct_faculty, pct_university, _order_tech
        full_name, teacher_id, course_value, activity_value, count_value, p1, p2, p3, p4, _ = rec

        # Write row in one call (fast), with formats for numeric columns only
        ws.write(row_in_sheet, 0, full_name, text_fmt)
        ws.write(row_in_sheet, 1, teacher_id, text_fmt)
        ws.write(row_in_sheet, 2, course_value, text_fmt)
        ws.write(row_in_sheet, 3, activity_value, text_fmt)

        # count (int)
        ws.write_number(row_in_sheet, 4, float(count_value or 0), int_fmt)

        # percents (numbers with 1 decimal place display)
        # Note: ROUND is already done upstream; we only format.
        def write_pct(col: int, val: Any):
            if val is None:
                ws.write_blank(row_in_sheet, col, None)
            else:
                ws.write_number(row_in_sheet, col, float(val), pct_fmt)

        write_pct(5, p1)
        write_pct(6, p2)
        write_pct(7, p3)
        write_pct(8, p4)

        row_in_sheet += 1
        total_written += 1

    logger.info("Wrote %s rows into %s sheet(s).", total_written, sheet_idx)
    return total_written, sheet_idx


def _write_qa_sheet(workbook: xlsxwriter.Workbook, con, logger: logging.Logger) -> int:
    ws = workbook.add_worksheet("QA")
    header_fmt = workbook.add_format({"bold": True, "border": 1})
    text_fmt = workbook.add_format({})

    headers = ["type", "teacher_id", "course_code", "tech_key", "description"]
    ws.set_column(0, 0, 22)
    ws.set_column(1, 3, 16)
    ws.set_column(4, 4, 80)

    _write_sheet_header(ws, header_fmt, headers)

    sql = _select_metrics_qa_sql(con)
    cur = con.execute(sql)
    cur.arraysize = 10_000

    r = 1
    for row in _iter_cursor_rows(cur, batch_size=10_000):
        ws.write_row(r, 0, list(row), text_fmt)
        r += 1

    logger.info("QA rows: %s", r - 1)
    return r - 1


def _write_info_sheet(
    workbook: xlsxwriter.Workbook,
    con,
    base_sql_metrics_long: str,
    ay: str,
    term: str,
    generated_at: datetime,
    logger: logging.Logger,
) -> None:
    ws = workbook.add_worksheet("INFO")
    bold = workbook.add_format({"bold": True})
    ws.set_column(0, 0, 22)
    ws.set_column(1, 1, 40)

    # counts from SQL (no pandas aggregation)
    total = int(con.execute(f"SELECT COUNT(*)::BIGINT FROM ({base_sql_metrics_long}) t").fetchone()[0])

    teachers = int(
        con.execute(f"SELECT COUNT(DISTINCT teacher_id)::BIGINT FROM ({base_sql_metrics_long}) t").fetchone()[0]
    )

    courses = int(
        con.execute(f"SELECT COUNT(DISTINCT course_value)::BIGINT FROM ({base_sql_metrics_long}) t").fetchone()[0]
    )

    rows = [
        ("ay", ay),
        ("term", term),
        ("data_wygenerowania_utc", generated_at.isoformat()),
        ("liczba_nauczycieli", teachers),
        ("liczba_kursow", courses),
        ("liczba_rekordow", total),
    ]

    ws.write(0, 0, "pole", bold)
    ws.write(0, 1, "wartosc", bold)
    for i, (k, v) in enumerate(rows, start=1):
        ws.write(i, 0, k)
        ws.write(i, 1, v)

    logger.info("INFO: teachers=%s courses=%s records=%s", teachers, courses, total)


def _export_skip_strategy_sql(con, activity_col: str) -> str:
    """
    "skip" is ambiguous in prompt. We implement safe minimal output:
    aggregate per teacher + activity only, without course. Percents set NULL.
    """
    table = "mart.metrics_long"
    if activity_col == "activity_label" and not _duckdb_has_column(con, table, "activity_label"):
        activity_col = "tech_key"

    where_clause = "WHERE visible_active" if _duckdb_has_column(con, table, "visible_active") else ""
    sql = f"""
        SELECT
            full_name::VARCHAR AS full_name,
            teacher_id::VARCHAR AS teacher_id,
            NULL::VARCHAR AS course_value,
            {activity_col}::VARCHAR AS activity_value,
            SUM(count_value)::BIGINT AS count_value,
            NULL::DOUBLE AS pct_course,
            NULL::DOUBLE AS pct_program,
            NULL::DOUBLE AS pct_faculty,
            NULL::DOUBLE AS pct_university,
            {activity_col}::VARCHAR AS _order_tech
        FROM {table}
        {where_clause}
        GROUP BY 1,2,3,4,10
        ORDER BY full_name ASC, _order_tech ASC
    """
    return sql


def export_summary_excel(con, cfg: Dict[str, Any]) -> Tuple[int, Path]:
    """
    Main entry point. Returns (exit_code, output_path).
    Requires:
      cfg["paths"]["root"] or cfg["root"] (depending on your config shape)
      cfg["report"]["ay"], cfg["report"]["term"] (or equivalents)
      cfg["export"]["max_rows_excel"], cfg["export"]["overflow_strategy"] (optional)
    """
    # --- Resolve config fields (keep tolerant to shape) ---
    root = Path(cfg.get("root") or cfg.get("paths", {}).get("root") or cfg.get("paths", {}).get("output_root", "."))
    ay = str(cfg.get("report", {}).get("ay") or cfg.get("ay") or "")
    term = str(cfg.get("report", {}).get("term") or cfg.get("term") or "")

    export_cfg = ExportExcelConfig(
        max_rows_excel=int(cfg.get("export", {}).get("max_rows_excel", 1_000_000)),
        overflow_strategy=str(cfg.get("export", {}).get("overflow_strategy", "error")),
        activity_column=str(cfg.get("export", {}).get("activity_column", "activity_label")),
        course_column=str(cfg.get("export", {}).get("course_column", "course_name")),
        include_hr_cols=bool(cfg.get("export", {}).get("include_hr_cols", True)),
        exclude_zero_counts=bool(cfg.get("export", {}).get("exclude_zero_counts", True)),
        percent_excel_format=bool(cfg.get("export", {}).get("percent_excel_format", True)),
     )

    run_dir, out_dir = _ensure_dirs(root)
    logger = _setup_logger(run_dir)

    _progress_append(run_dir, {"step": "export-excel", "status": "start", "ay": ay, "term": term})

    out_path = out_dir / f"Raport_Zbiorczy_NA_{ay}_{term}.xlsx"
    ok_flag = run_dir / "export-excel.ok"

    logger.info("Exporting XLSX to: %s", out_path)
    logger.info("Export config: %s", export_cfg)

    # Build base SQL (ordered) for main sheet
    base_sql = _select_metrics_long_sql(
    con,
    export_cfg.activity_column,
    export_cfg.course_column,
    export_cfg.include_hr_cols,
    export_cfg.exclude_zero_counts,
    export_cfg.percent_excel_format,
    )

    # Pre-count rows to enforce overflow strategy deterministically
    total_rows = _count_rows(con, base_sql)
    logger.info("metrics_long rows to export: %s", total_rows)

    # Decide strategy
    strategy = export_cfg.overflow_strategy.lower().strip()
    max_rows = export_cfg.max_rows_excel

    if total_rows > max_rows and strategy == "error":
        _progress_append(
            run_dir,
            {
                "step": "export-excel",
                "status": "error",
                "reason": "overflow",
                "rows": total_rows,
                "max_rows_excel": max_rows,
                "strategy": strategy,
            },
        )
        raise ExportOverflowError(f"Too many rows for Excel: {total_rows} > {max_rows}")

    if total_rows > max_rows and strategy == "skip":
        logger.warning("Overflow with strategy=skip; exporting aggregated per teacher (course=NULL).")
        base_sql = _export_skip_strategy_sql(con, export_cfg.activity_column)
        total_rows = _count_rows(con, base_sql)

    # Create workbook (overwrite, idempotent)
    generated_at = datetime.now(timezone.utc)

    workbook = xlsxwriter.Workbook(
        out_path.as_posix(),
        {
            "constant_memory": True,  # good for large datasets
            "strings_to_numbers": False,
            "strings_to_formulas": False,
            "strings_to_urls": False,
        },
    )

    try:
        main_sheet_base = "ZLICZENIE_AKTYWNOSCI_NA"

        # Main data
        _progress_append(run_dir, {"step": "export-excel", "status": "writing_main", "rows": total_rows})
        written_rows, sheets_count = _write_metrics_long_split_streaming(
            workbook=workbook,
            con=con,
            sql=base_sql,
            max_rows=max_rows,
            overflow_strategy=("split" if strategy == "split" else "error"),
            main_sheet_base=main_sheet_base,
            logger=logger,
        )

        # QA
        _progress_append(run_dir, {"step": "export-excel", "status": "writing_qa"})
        qa_rows = _write_qa_sheet(workbook, con, logger)

        # INFO
        _progress_append(run_dir, {"step": "export-excel", "status": "writing_info"})
        _write_info_sheet(workbook, con, base_sql, ay, term, generated_at, logger)

    finally:
        workbook.close()

    ok_flag.write_text("OK\n", encoding="utf-8")
    _progress_append(
        run_dir,
        {
            "step": "export-excel",
            "status": "done",
            "output": str(out_path),
            "main_rows": written_rows,
            "qa_rows": qa_rows,
            "main_sheets": sheets_count,
        },
    )

    logger.info("DONE: %s", out_path)
    return EXIT_OK, out_path

================================================================================
src\mrna_plum\reports\export_individual.py
================================================================================

from __future__ import annotations

import json
import re
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterator, List, Optional, Sequence, Tuple

import duckdb
import xlsxwriter
from concurrent.futures import ThreadPoolExecutor, as_completed


# ======================================================================================
# Config helpers (tolerant: cfg can be dict-like or attr-like)
# ======================================================================================

def _cfg_get(cfg: Any, path: str, default: Any = None) -> Any:
    """
    Read cfg value using dotted path, supports both dict-like and attribute-like objects.
    Example: _cfg_get(cfg, "reports.max_workers", 4)
    """
    cur = cfg
    for part in path.split("."):
        if cur is None:
            return default
        if isinstance(cur, dict):
            cur = cur.get(part, None)
        else:
            cur = getattr(cur, part, None)
    return default if cur is None else cur


# ======================================================================================
# Logging & artifacts
# ======================================================================================

@dataclass(frozen=True)
class RunArtifacts:
    run_dir: Path
    run_log: Path
    progress_jsonl: Path
    ok_file: Path


def _ensure_artifacts(root: Path) -> RunArtifacts:
    run_dir = root / "_run"
    run_dir.mkdir(parents=True, exist_ok=True)
    run_log = run_dir / "run.log"
    progress_jsonl = run_dir / "progress.jsonl"
    ok_file = run_dir / "export-individual.ok"
    return RunArtifacts(run_dir, run_log, progress_jsonl, ok_file)


def _log(line: str, run_log: Path) -> None:
    ts = time.strftime("%Y-%m-%d %H:%M:%S")
    msg = f"{ts} [export-individual] {line}"
    print(msg)
    with run_log.open("a", encoding="utf-8") as f:
        f.write(msg + "\n")


def _progress(evt: Dict[str, Any], progress_jsonl: Path) -> None:
    evt = dict(evt)
    evt.setdefault("ts", time.strftime("%Y-%m-%d %H:%M:%S"))
    with progress_jsonl.open("a", encoding="utf-8") as f:
        f.write(json.dumps(evt, ensure_ascii=False) + "\n")


# ======================================================================================
# Filename sanitization
# ======================================================================================

_WIN_ILLEGAL = r'<>:"/\|?*'
_WIN_ILLEGAL_RE = re.compile(rf"[{re.escape(_WIN_ILLEGAL)}]")


def sanitize_filename(name: str, max_len: int = 120) -> str:
    """
    - replace illegal Windows filename chars with '_'
    - collapse whitespace
    - trim
    - limit length
    """
    if not name:
        return ""
    s = str(name).strip()
    s = _WIN_ILLEGAL_RE.sub("_", s)
    s = re.sub(r"\s+", " ", s)
    s = s.strip(" .")  # Windows hates trailing dot/space
    if len(s) > max_len:
        s = s[:max_len].rstrip(" .")
    return s


# ======================================================================================
# DuckDB SQL
# ======================================================================================

def _ensure_qa_table(con: duckdb.DuckDBPyConnection) -> None:
    con.execute("CREATE SCHEMA IF NOT EXISTS mart;")
    con.execute(
        """
        CREATE TABLE IF NOT EXISTS mart.individual_export_qa (
            teacher_id      VARCHAR,
            status          VARCHAR,  -- OK / SKIPPED_* / ERROR
            message         VARCHAR,
            output_file     VARCHAR,
            rows_exported   BIGINT,
            exported_at     TIMESTAMP DEFAULT now()
        );
        """
    )


def _qa_insert(
    con: duckdb.DuckDBPyConnection,
    teacher_id: str,
    status: str,
    message: str,
    output_file: Optional[str],
    rows_exported: int,
) -> None:
    con.execute(
        """
        INSERT INTO mart.individual_export_qa
            (teacher_id, status, message, output_file, rows_exported)
        VALUES (?, ?, ?, ?, ?)
        """,
        [teacher_id, status, message, output_file, rows_exported],
    )


def _detect_hr_columns(con: duckdb.DuckDBPyConnection) -> List[str]:
    """
    Prefer HR embedded in mart.metrics_long as dynamic columns hr_*.
    Return list of column names present in metrics_long matching hr_*.
    """
    rows = con.execute("DESCRIBE mart.metrics_long;").fetchall()
    col_names = [r[0] for r in rows]
    return [c for c in col_names if c.lower().startswith("hr_")]


def _list_teachers(con: duckdb.DuckDBPyConnection) -> List[Tuple[str, str, str, str]]:
    """
    Teachers to export (metrics_long already HR-whitelisted).
    EXPORT RULE:
      - require teacher_id AND email (non-empty); otherwise teacher is not included here.
      - require id_bazus for filename (we still include them here; missing bazus handled in worker).
    Deterministic order: teacher_id asc.
    Returns: (teacher_id, full_name, email, id_bazus)
    """
    rows = con.execute(
        """
        SELECT
            teacher_id::VARCHAR AS teacher_id,
            COALESCE(NULLIF(TRIM(full_name), ''), '') AS full_name,
            COALESCE(NULLIF(TRIM(email), ''), '') AS email,
            COALESCE(NULLIF(TRIM(id_bazus), ''), '') AS id_bazus
        FROM mart.metrics_long
        WHERE visible_active = TRUE
        GROUP BY 1, 2, 3, 4
        HAVING teacher_id IS NOT NULL
           AND TRIM(teacher_id) <> ''
           AND email IS NOT NULL
           AND TRIM(email) <> ''
        ORDER BY teacher_id ASC
        """
    ).fetchall()
    return [(str(tid), str(fn), str(em), str(bz)) for tid, fn, em, bz in rows]


def _fetch_teacher_rows(
    con: duckdb.DuckDBPyConnection,
    teacher_id: str,
) -> Iterator[Tuple[Any, ...]]:
    """
    Rows for DANE_KURSY:
      course_name, activity_label, count_value,
      pct_course/100.0, pct_kierunek/100.0, pct_wydzial/100.0, pct_uczelnia/100.0

    Deterministic sort, count_value>0 only.
    """
    cur = con.execute(
        """
        SELECT
            course_name,
            activity_label,
            count_value,
            (pct_course / 100.0)   AS pct_course_xlsx,
            (pct_kierunek / 100.0) AS pct_kierunek_xlsx,
            (pct_wydzial / 100.0)  AS pct_wydzial_xlsx,
            (pct_uczelnia / 100.0) AS pct_uczelnia_xlsx
        FROM mart.metrics_long
        WHERE visible_active = TRUE
          AND teacher_id::VARCHAR = ?
          AND count_value > 0
        ORDER BY
            course_name ASC,
            activity_label ASC
        """,
        [teacher_id],
    )
    while True:
        batch = cur.fetchmany(10_000)
        if not batch:
            break
        for row in batch:
            yield row


def _fetch_teacher_pers(
    con: duckdb.DuckDBPyConnection,
    teacher_id: str,
    hr_cols: List[str],
) -> Dict[str, Any]:
    """
    One row with "metrics_long embedded HR" preference.
    We take MAX(...) as safe collapse (same per teacher).
    Assumes email + id_bazus exist in schema (per your rules).
    """
    select_parts = [
        "teacher_id::VARCHAR AS teacher_id",
        "MAX(COALESCE(NULLIF(TRIM(full_name), ''), '')) AS full_name",
        "MAX(COALESCE(NULLIF(TRIM(email), ''), '')) AS email",
        "MAX(COALESCE(NULLIF(TRIM(id_bazus), ''), '')) AS id_bazus",
    ]

    for c in hr_cols:
        select_parts.append(f"MAX(COALESCE(NULLIF(TRIM({c}), ''), '')) AS {c}")

    sql = f"""
        SELECT {", ".join(select_parts)}
        FROM mart.metrics_long
        WHERE visible_active = TRUE
          AND teacher_id::VARCHAR = ?
        GROUP BY teacher_id
    """
    row = con.execute(sql, [teacher_id]).fetchone()
    if row is None:
        base = {"teacher_id": teacher_id, "full_name": "", "email": "", "id_bazus": ""}
        for c in hr_cols:
            base[c] = ""
        return base

    keys = ["teacher_id", "full_name", "email", "id_bazus"] + hr_cols
    return dict(zip(keys, row))


def _hr_human_label(col: str) -> str:
    """
    Map hr_* columns to human-friendly labels for DANE_PERS.
    Extend as you standardize HR fields.
    """
    m = {
        "hr_wydzial": "Wydział",
        "hr_jednostka": "Jednostka",
        "hr_katedra": "Katedra",
        "hr_zaklad": "Zakład",
        "hr_stanowisko": "Stanowisko",
        "hr_tytul": "Tytuł / Stopień",
        "hr_umowa": "Rodzaj umowy",
    }
    low = col.lower()
    return m.get(low, col)  # fallback to raw column name


# ======================================================================================
# XLSX writing
# ======================================================================================

_COURSES_HEADERS = ["Kurs", "Aktywność", "Liczba", "% kurs", "% kierunek", "% wydział", "% uczelnia"]


def _write_teacher_xlsx(
    out_file: Path,
    teacher_id: str,
    full_name: str,
    rows_iter: Iterator[Tuple[Any, ...]],
    pers: Dict[str, Any],
    hr_cols: List[str],
) -> int:
    """
    Returns rows_exported (DANE_KURSY count).
    """
    out_file.parent.mkdir(parents=True, exist_ok=True)

    wb = xlsxwriter.Workbook(out_file.as_posix(), {"constant_memory": True})
    try:
        fmt_header = wb.add_format({"bold": True})
        fmt_pct = wb.add_format({"num_format": "0.0%"})
        fmt_int = wb.add_format({"num_format": "0"})  # count_value

        # Sheet 1: DANE_KURSY
        ws1 = wb.add_worksheet("DANE_KURSY")
        for c, h in enumerate(_COURSES_HEADERS):
            ws1.write(0, c, h, fmt_header)

        r = 1
        for (
            course_name,
            activity_label,
            count_value,
            pct_course_x,
            pct_kierunek_x,
            pct_wydzial_x,
            pct_uczelnia_x,
        ) in rows_iter:
            ws1.write(r, 0, course_name if course_name is not None else "")
            ws1.write(r, 1, activity_label if activity_label is not None else "")

            try:
                ws1.write_number(r, 2, float(count_value), fmt_int)
            except Exception:
                ws1.write(r, 2, count_value)

            for j, v in enumerate([pct_course_x, pct_kierunek_x, pct_wydzial_x, pct_uczelnia_x], start=3):
                if v is None:
                    ws1.write_blank(r, j, None)
                else:
                    ws1.write_number(r, j, float(v), fmt_pct)

            r += 1

        rows_exported = r - 1

        # Sheet 2: DANE_PERS (vertical key -> value)
        ws2 = wb.add_worksheet("DANE_PERS")
        ws2.write(0, 0, "Pole", fmt_header)
        ws2.write(0, 1, "Wartość", fmt_header)

        # Required minimum fields + your "Imię i nazwisko"
        name_val = pers.get("full_name", full_name) or full_name or ""
        kv: List[Tuple[str, Any]] = [
            ("ID_PLUM", teacher_id),
            ("Imię i nazwisko", name_val),
            ("E-mail", pers.get("email", "") or ""),
            ("ID bazus", pers.get("id_bazus", "") or ""),
        ]

        # HR -> human labels (dynamic)
        for c in hr_cols:
            kv.append((_hr_human_label(c), pers.get(c, "") or ""))

        # Ensure at least Wydział/Jednostka rows exist even if hr cols absent
        if not any(_hr_human_label(c) == "Wydział" for c in hr_cols):
            kv.append(("Wydział", ""))
        if not any(_hr_human_label(c) == "Jednostka" for c in hr_cols):
            kv.append(("Jednostka", ""))

        for i, (k, v) in enumerate(kv, start=1):
            ws2.write(i, 0, k)
            ws2.write(i, 1, v)

        return rows_exported
    finally:
        wb.close()


# ======================================================================================
# Public API
# ======================================================================================

def export_individual_reports(
    con: duckdb.DuckDBPyConnection,
    cfg: Any,
) -> Tuple[int, str]:
    """
    Public entrypoint:
      export_individual_reports(con, cfg) -> (exit_code, out_dir)

    Rules applied:
      - SQL-first (DuckDB)
      - only visible_active
      - exclude count_value=0 from export
      - require teacher_id + email (non-empty) or SKIP
      - filename: <NazwiskoImie>_<BAZUS ID>.xlsx (sanitized)
      - pct columns: DB 0-100 -> XLSX pct/100.0 with format 0.0%
      - deterministic sort: course_name ASC, activity_label ASC
      - idempotent overwrite
    """
    root = Path(_cfg_get(cfg, "root", ".")).resolve()
    arts = _ensure_artifacts(root)

    out_rel = _cfg_get(cfg, "reports.individual_dir", "_out/indywidualne")
    out_dir = (root / out_rel).resolve()
    max_workers = int(_cfg_get(cfg, "reports.max_workers", 4))
    batch_teachers = int(_cfg_get(cfg, "reports.batch_teachers", 50))

    _log(f"root={root}", arts.run_log)
    _log(f"out_dir={out_dir}", arts.run_log)
    _log(f"max_workers={max_workers} batch_teachers={batch_teachers}", arts.run_log)

    _ensure_qa_table(con)
    hr_cols = _detect_hr_columns(con)
    _log(f"Detected HR columns in mart.metrics_long: {hr_cols}", arts.run_log)

    teachers = _list_teachers(con)  # (teacher_id, full_name, email, id_bazus) with email required
    _log(f"Teachers to export (email required): {len(teachers)}", arts.run_log)

    # For parallel: DuckDB connection is not safely shared across threads.
    # We'll open a separate connection per worker using cfg.paths.db_path.
    db_path = _cfg_get(cfg, "paths.db_path", None)

    def _worker(teacher_id: str, full_name: str, email: str, id_bazus: str) -> Tuple[str, str, str, Optional[str], int]:
        """
        Returns: (teacher_id, status, message, output_file, rows_exported)
        """
        local_con = con
        must_close = False
        try:
            # hard guards (your rule)
            if not teacher_id or not str(teacher_id).strip():
                return (str(teacher_id), "SKIPPED_NO_ID", "Missing teacher_id", None, 0)
            if not email or not str(email).strip():
                return (str(teacher_id), "SKIPPED_NO_EMAIL", "Missing email", None, 0)
            if not id_bazus or not str(id_bazus).strip():
                return (str(teacher_id), "SKIPPED_NO_BAZUS", "Missing BAZUS ID for filename", None, 0)

            if max_workers and max_workers > 1:
                if not db_path:
                    # no db_path -> do not parallelize safely
                    local_con = con
                else:
                    local_con = duckdb.connect(str(db_path))
                    must_close = True

            safe_name = sanitize_filename(full_name, max_len=120) or "UNKNOWN"
            safe_bazus = sanitize_filename(str(id_bazus), max_len=60) or "BAZUS_UNKNOWN"

            filename = f"{safe_name}_{safe_bazus}.xlsx"
            if len(filename) > 180:
                safe_name2 = sanitize_filename(safe_name, max_len=120)
                filename = f"{safe_name2}_{safe_bazus}.xlsx"

            out_file = out_dir / filename

            # fetch rows (streaming)
            rows_iter = _fetch_teacher_rows(local_con, teacher_id)

            # Need to know if any rows exist without consuming iterator -> buffer first item
            buffered: List[Tuple[Any, ...]] = []
            try:
                buffered.append(next(rows_iter))
            except StopIteration:
                buffered = []

            if not buffered:
                return (teacher_id, "SKIPPED_NO_DATA", "No rows with count_value>0", None, 0)

            def _chain() -> Iterator[Tuple[Any, ...]]:
                for x in buffered:
                    yield x
                for x in rows_iter:
                    yield x

            pers = _fetch_teacher_pers(local_con, teacher_id, hr_cols)

            # idempotent overwrite
            if out_file.exists():
                out_file.unlink()

            rows_exported = _write_teacher_xlsx(
                out_file=out_file,
                teacher_id=teacher_id,
                full_name=full_name,
                rows_iter=_chain(),
                pers=pers,
                hr_cols=hr_cols,
            )

            if rows_exported == 0:
                if out_file.exists():
                    out_file.unlink()
                return (teacher_id, "SKIPPED_NO_DATA", "No rows with count_value>0", None, 0)

            return (teacher_id, "OK", "Exported", str(out_file), int(rows_exported))

        except Exception as e:
            return (teacher_id, "ERROR", f"{type(e).__name__}: {e}", None, 0)
        finally:
            if must_close:
                try:
                    local_con.close()
                except Exception:
                    pass

    exported_ok = 0
    exported_err = 0
    exported_skip = 0

    def _batched(seq: Sequence[Tuple[str, str, str, str]], n: int) -> Iterator[List[Tuple[str, str, str, str]]]:
        for i in range(0, len(seq), n):
            yield list(seq[i : i + n])

    for batch in _batched(teachers, batch_teachers):
        if max_workers and max_workers > 1 and db_path:
            with ThreadPoolExecutor(max_workers=max_workers) as ex:
                futs = [ex.submit(_worker, tid, fn, em, bz) for (tid, fn, em, bz) in batch]
                for fut in as_completed(futs):
                    tid, status, msg, out_file, rows_exported = fut.result()
                    _qa_insert(con, tid, status, msg, out_file, rows_exported)
                    _progress(
                        {
                            "teacher_id": tid,
                            "status": status,
                            "message": msg,
                            "output_file": out_file,
                            "rows_exported": rows_exported,
                        },
                        arts.progress_jsonl,
                    )
                    if status == "OK":
                        exported_ok += 1
                    elif status.startswith("SKIPPED"):
                        exported_skip += 1
                    else:
                        exported_err += 1
        else:
            for (tid, fn, em, bz) in batch:
                tid, status, msg, out_file, rows_exported = _worker(tid, fn, em, bz)
                _qa_insert(con, tid, status, msg, out_file, rows_exported)
                _progress(
                    {
                        "teacher_id": tid,
                        "status": status,
                        "message": msg,
                        "output_file": out_file,
                        "rows_exported": rows_exported,
                    },
                    arts.progress_jsonl,
                )
                if status == "OK":
                    exported_ok += 1
                elif status.startswith("SKIPPED"):
                    exported_skip += 1
                else:
                    exported_err += 1

    _log(f"Done. OK={exported_ok} SKIPPED={exported_skip} ERROR={exported_err}", arts.run_log)

    if exported_err == 0:
        arts.ok_file.write_text("OK\n", encoding="utf-8")
        return (0, str(out_dir))
    return (2, str(out_dir))


# ======================================================================================
# Optional CLI wrapper (adapt to your cli.py / Typer)
# ======================================================================================

def cli_export_individual(root: str, config: Any) -> int:
    """
    Example CLI wrapper (adapt to your existing cli.py).
    - root: pipeline root
    - config: loaded config object/dict
    """
    if isinstance(config, dict):
        config = dict(config)
        config["root"] = root
    else:
        setattr(config, "root", root)

    db_path = _cfg_get(config, "paths.db_path", None)
    if not db_path:
        raise RuntimeError("cfg.paths.db_path is required for export-individual")

    con = duckdb.connect(str(db_path))
    try:
        code, _out = export_individual_reports(con, config)
        return int(code)
    finally:
        con.close()

================================================================================
src\mrna_plum\rules\__init__.py
================================================================================



================================================================================
src\mrna_plum\rules\activity_rules.py
================================================================================

# src/mrna_plum/rules/activity_rules.py

from __future__ import annotations

import re
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from openpyxl import load_workbook


@dataclass(frozen=True)
class KeyRule:
    activity_label: str
    tech_key: str
    operation: str          # CREATE/DELETE/UPDATE/VIEW/GRADE/...
    count_mode: str         # TAK / TAK_FLAG / NIE
    rx_match: re.Pattern
    rx_user: Optional[re.Pattern]
    rx_object_from_match_group: Optional[str]  # e.g. "object_id" or None
    priority: int


@dataclass(frozen=True)
class RuleMatch:
    activity_label: str
    tech_key: str
    operation: str
    count_mode: str
    teacher_id: Optional[int]
    object_id: Optional[int]
    priority: int
    conflict: bool


def _norm(s: Any) -> str:
    return ("" if s is None else str(s)).strip()


def _compile(pat: str) -> re.Pattern:
    # wszystkie regexy traktujemy case-insensitive
    return re.compile(pat, re.IGNORECASE)


def load_keys_rules(keys_xlsx: str, sheet_name: str = "KEYS") -> List[KeyRule]:
    """
    Oczekiwane kolumny w KEYS:
    AKTYWNOSC
    KLUCZ_TECHNICZNY
    OPERACJA
    LICZYC_DO_RAPORTU
    REGEX_DOPASOWANIA_(Opis)
    REGEX_USER_ID_(Opis)
    REGEX_OBIEKT_ID_(z dopasowania)   # opcjonalnie: nazwa grupy np. object_id
    PRIORYTET
    """
    wb = load_workbook(filename=keys_xlsx, read_only=True, data_only=True)
    if sheet_name not in wb.sheetnames:
        raise ValueError(f"KEYS: brak arkusza '{sheet_name}' w {keys_xlsx}")

    ws = wb[sheet_name]
    rows = ws.iter_rows(values_only=True)
    header = [(_norm(x)) for x in next(rows)]
    idx = {name: i for i, name in enumerate(header) if name}

    def col(name: str) -> int:
        if name not in idx:
            raise ValueError(f"KEYS: brak kolumny '{name}' (nagłówki: {header})")
        return idx[name]

    rules: List[KeyRule] = []

    for r in rows:
        activity_label = _norm(r[col("AKTYWNOSC")])
        tech_key = _norm(r[col("KLUCZ_TECHNICZNY")])
        operation = _norm(r[col("OPERACJA")]).upper()
        count_mode = _norm(r[col("LICZYC_DO_RAPORTU")]).upper()  # TAK/TAK_FLAG/NIE
        rx_match_txt = _norm(r[col("REGEX_DOPASOWANIA_(Opis)")])
        rx_user_txt = _norm(r[col("REGEX_USER_ID_(Opis)")])
        rx_obj_group = _norm(r[idx.get("REGEX_OBIEKT_ID_(z dopasowania)", -1)]) if "REGEX_OBIEKT_ID_(z dopasowania)" in idx else ""
        prio_txt = _norm(r[col("PRIORYTET")])
        if not rx_match_txt or not tech_key:
            continue

        try:
            prio = int(prio_txt) if prio_txt else 0
        except Exception:
            prio = 0

        rx_match = _compile(rx_match_txt)
        rx_user = _compile(rx_user_txt) if rx_user_txt else None
        rx_obj_group = rx_obj_group or None

        rules.append(
            KeyRule(
                activity_label=activity_label,
                tech_key=tech_key,
                operation=operation,
                count_mode=count_mode,
                rx_match=rx_match,
                rx_user=rx_user,
                rx_object_from_match_group=rx_obj_group,
                priority=prio,
            )
        )

    # najwyższy priorytet wcześniej – przyspiesza
    rules.sort(key=lambda x: x.priority, reverse=True)
    return rules


class ActivityRuleEngine:
    def __init__(self, rules: List[KeyRule], drop_mode_nie: bool = True):
        self.rules = rules
        self.drop_mode_nie = drop_mode_nie

    def match(self, opis: str) -> Optional[RuleMatch]:
        """
        Jeśli wiele reguł pasuje:
        - wybierz najwyższy PRIORYTET
        - jeśli kilka ma ten sam max PRIORYTET -> conflict=True
        """
        opis = opis or ""
        matches: List[Tuple[KeyRule, re.Match]] = []

        for rule in self.rules:
            m = rule.rx_match.search(opis)
            if m:
                matches.append((rule, m))

        if not matches:
            return None

        # wybór max priorytetu
        max_prio = max(rule.priority for rule, _ in matches)
        best = [(rule, m) for rule, m in matches if rule.priority == max_prio]

        # bierzemy pierwszy jako “winner”, ale zaznaczamy konflikt jeśli >1
        rule, m = best[0]
        conflict = len(best) > 1

        # object_id – z grupy nazwanej, jeśli KEYS wskazuje; inaczej spróbuj 1. grupy
        object_id: Optional[int] = None
        if rule.rx_object_from_match_group:
            gd = m.groupdict()
            val = gd.get(rule.rx_object_from_match_group)
            if val:
                try:
                    object_id = int(val)
                except Exception:
                    object_id = None
        else:
            try:
                if m.groups():
                    object_id = int(m.group(1))
            except Exception:
                object_id = None

        # teacher_id – regex osobny z KEYS (na Opis)
        teacher_id: Optional[int] = None
        if rule.rx_user:
            um = rule.rx_user.search(opis)
            if um:
                gd = um.groupdict()
                if "id" in gd and gd["id"]:
                    try:
                        teacher_id = int(gd["id"])
                    except Exception:
                        teacher_id = None
                else:
                    try:
                        teacher_id = int(um.group(1))
                    except Exception:
                        teacher_id = None

        if self.drop_mode_nie and rule.count_mode == "NIE":
            return None

        return RuleMatch(
            activity_label=rule.activity_label,
            tech_key=rule.tech_key,
            operation=rule.operation,
            count_mode=rule.count_mode,
            teacher_id=teacher_id,
            object_id=object_id,
            priority=rule.priority,
            conflict=conflict,
        )

================================================================================
src\mrna_plum\rules\engine.py
================================================================================

from __future__ import annotations
from dataclasses import dataclass
from typing import Optional
import re
import pandas as pd

from .models import Rule

@dataclass(frozen=True)
class MatchResult:
    tech_key: str
    activity: str
    operation: str
    count_to_report: bool
    teacher_id: Optional[str]
    object_id: Optional[str]
    priority: int

def compile_rules(keys_df: pd.DataFrame) -> list[Rule]:
    rules: list[Rule] = []
    for _, r in keys_df.iterrows():
        def _p(s: object) -> str:
            return "" if s is None else str(s)

        match_rx = re.compile(_p(r["REGEX_DOPASOWANIA_(Opis)"]))
        user_rx_s = _p(r["REGEX_USER_ID_(Opis)"]).strip()
        obj_rx_s  = _p(r["REGEX_OBIEKT_ID_(z dopasowania)"]).strip()

        user_rx = re.compile(user_rx_s) if user_rx_s else None
        obj_rx  = re.compile(obj_rx_s) if obj_rx_s else None

        count_flag = _p(r["LICZYC_DO_RAPORTU"]).upper() in ("TAK", "1", "TRUE", "YES")

        rules.append(
            Rule(
                activity=_p(r["AKTYWNOSC"]).strip(),
                tech_key=_p(r["KLUCZ_TECHNICZNY"]).strip(),
                operation=_p(r["OPERACJA"]).strip(),
                count_to_report=count_flag,
                regex_match_desc=match_rx,
                regex_user_id=user_rx,
                regex_object_id=obj_rx,
                priority=int(r["PRIORYTET"]),
            )
        )
    # wyższy priorytet pierwszy
    rules.sort(key=lambda x: x.priority, reverse=True)
    return rules

def match_best_rule(description: str, rules: list[Rule]) -> Optional[MatchResult]:
    for rule in rules:
        m = rule.regex_match_desc.search(description or "")
        if not m:
            continue

        teacher_id = None
        object_id = None

        if rule.regex_user_id:
            um = rule.regex_user_id.search(description or "")
            if um and um.groups():
                teacher_id = um.group(1)
            elif um:
                teacher_id = um.group(0)

        if rule.regex_object_id:
            # object_id może być z dopasowania głównego (m) albo z opisu
            om = rule.regex_object_id.search(m.group(0)) or rule.regex_object_id.search(description or "")
            if om and om.groups():
                object_id = om.group(1)
            elif om:
                object_id = om.group(0)

        return MatchResult(
            tech_key=rule.tech_key,
            activity=rule.activity,
            operation=rule.operation,
            count_to_report=rule.count_to_report,
            teacher_id=teacher_id,
            object_id=object_id,
            priority=rule.priority,
        )
    return None


================================================================================
src\mrna_plum\rules\models.py
================================================================================

from __future__ import annotations
from dataclasses import dataclass
import re
from typing import Optional

@dataclass(frozen=True)
class Rule:
    activity: str
    tech_key: str
    operation: str          # np. TAK / TAK_FLAG / NIE / etc.
    count_to_report: bool
    regex_match_desc: re.Pattern
    regex_user_id: Optional[re.Pattern]
    regex_object_id: Optional[re.Pattern]
    priority: int


================================================================================
src\mrna_plum\stats\__init__.py
================================================================================

from .compute_stats import compute_stats
__all__ = ["compute_stats"]


================================================================================
src\mrna_plum\stats\compute_stats.py
================================================================================

from __future__ import annotations

import json
from dataclasses import dataclass
from logging import root
from pathlib import Path
from datetime import datetime, timezone
from typing import Optional, Sequence

import duckdb
import pandas as pd
import yaml

from mrna_plum import paths
from ..errors import ConfigError
from ..paths import ProjectPaths

# ----------------------------
# Helpers / config
# ----------------------------

@dataclass(frozen=True)
class StatsConfig:
    duckdb_path: Path
    run_dir: Path
    include_deleted_in_percent: bool
    rebuild_full: bool

    # mapping sources
    map_teacher_id_email_path: Optional[Path]
    map_email_hr_path: Optional[Path]

    # rounding
    pct_round_decimals: int

    # period
    ay: Optional[str]
    term: Optional[str]


def _load_config(root: Path) -> dict:
    cfg_path = root / "config.yaml"
    if not cfg_path.exists():
        raise FileNotFoundError(f"Brak config.yaml pod: {cfg_path}")
    return yaml.safe_load(cfg_path.read_text(encoding="utf-8"))


def _resolve_path(root: Path, p: Optional[str]) -> Optional[Path]:
    if not p:
        return None
    pp = Path(p)
    return pp if pp.is_absolute() else (root / pp)


def _ensure_run_artifacts(run_dir: Path) -> None:
    run_dir.mkdir(parents=True, exist_ok=True)


def _log_progress(run_dir: Path, payload: dict) -> None:
    p = run_dir / "progress.jsonl"
    payload2 = dict(payload)
    payload2["ts"] = datetime.now(timezone.utc).isoformat()
    with p.open("a", encoding="utf-8") as f:
        f.write(json.dumps(payload2, ensure_ascii=False) + "\n")


def _write_ok(run_dir: Path) -> None:
    (run_dir / "compute-stats.ok").write_text(
        datetime.now(timezone.utc).isoformat(),
        encoding="utf-8",
    )


def _read_mapping_teacher_email(path: Optional[Path]) -> pd.DataFrame:
    """
    Oczekiwane kolumny: teacher_id, email
    """
    if path is None:
        return pd.DataFrame(columns=["teacher_id", "email"])
    if not path.exists():
        raise FileNotFoundError(f"Brak pliku mapowania teacher_id→email: {path}")

    if path.suffix.lower() in [".xlsx", ".xls"]:
        df = pd.read_excel(path, dtype=str)
    else:
        df = pd.read_csv(path, dtype=str)

    df = df.rename(columns={c: c.strip() for c in df.columns})
    # normalizacja nazw (tolerancyjnie)
    cols = {c.lower(): c for c in df.columns}
    tid = cols.get("teacher_id") or cols.get("id") or cols.get("userid")
    eml = cols.get("email") or cols.get("mail")
    if not tid or not eml:
        raise ValueError(f"Plik {path} musi mieć kolumny teacher_id oraz email (lub równoważne).")

    out = df[[tid, eml]].copy()
    out.columns = ["teacher_id", "email"]
    out["teacher_id"] = out["teacher_id"].astype(str).str.strip()
    out["email"] = out["email"].astype(str).str.strip().str.lower()
    out = out.dropna().drop_duplicates()
    return out


def read_hr_table(hr_file: Path, sheet: str | None,
                  email_col: str, full_name_col: str | None,
                  wydzial_col: str | None, jednostka_col: str | None,
                  passthrough_cols: list[str] | None = None) -> pd.DataFrame:
    if not hr_file.exists():
        raise FileNotFoundError(f"Brak pliku HR: {hr_file}")

    if hr_file.suffix.lower() in [".xlsx", ".xls"]:
        df = pd.read_excel(hr_file, sheet_name=sheet or 0, dtype=str)
    else:
        # jeśli kiedyś HR będzie w TSV/CSV
        df = pd.read_csv(hr_file, sep=None, engine="python", dtype=str)

    df = df.rename(columns={c: str(c).strip() for c in df.columns})

    def pick(colname: str | None) -> pd.Series:
        if not colname:
            return pd.Series([""] * len(df))
        if colname not in df.columns:
            raise ValueError(f"HR: brak kolumny '{colname}'. Dostępne: {list(df.columns)}")
        return df[colname].astype(str)

    out = pd.DataFrame()
    out["email"] = pick(email_col).str.strip().str.lower()
    out["full_name"] = pick(full_name_col).str.strip() if full_name_col else ""
    out["wydzial"] = pick(wydzial_col).str.strip() if wydzial_col else ""
    out["jednostka"] = pick(jednostka_col).str.strip() if jednostka_col else ""

    if passthrough_cols:
        for c in passthrough_cols:
            if c not in df.columns:
                raise ValueError(f"HR passthrough: brak kolumny '{c}'")
            out[c] = df[c].astype(str).str.strip()

    out = out[out["email"].notna() & (out["email"] != "")]
    out = out.drop_duplicates(subset=["email"])
    return out


def _read_mapping_email_hr(path: Optional[Path]) -> pd.DataFrame:
    """
    Oczekiwane minimum: email, full_name, wydzial, jednostka
    (kolumny mogą się nazywać inaczej -> dopasowanie tolerancyjne)
    """
    if path is None:
        return pd.DataFrame(columns=["email", "full_name", "wydzial", "jednostka"])
    if not path.exists():
        raise FileNotFoundError(f"Brak pliku mapowania email→HR: {path}")

    if path.suffix.lower() in [".xlsx", ".xls"]:
        df = pd.read_excel(path, dtype=str)
    else:
        df = pd.read_csv(path, dtype=str)

    df = df.rename(columns={c: c.strip() for c in df.columns})
    cols = {c.lower(): c for c in df.columns}

    email = cols.get("email") or cols.get("mail") or cols.get("e-mail")
    full_name = cols.get("full_name") or cols.get("imie_nazwisko") or cols.get("name") or cols.get("nazwiskoimie")
    wydzial = cols.get("wydzial") or cols.get("wydział")
    jednostka = cols.get("jednostka") or cols.get("unit") or cols.get("katedra")

    if not email:
        raise ValueError(f"Plik HR {path} musi mieć kolumnę email/mail/e-mail.")

    out = pd.DataFrame()
    out["email"] = df[email].astype(str).str.strip().str.lower()

    out["full_name"] = df[full_name].astype(str).str.strip() if full_name else ""
    out["wydzial"] = df[wydzial].astype(str).str.strip() if wydzial else ""
    out["jednostka"] = df[jednostka].astype(str).str.strip() if jednostka else ""

    out = out.dropna().drop_duplicates(subset=["email"])
    return out


# ----------------------------
# Main compute
# ----------------------------

def compute_stats(root: Path, ay: Optional[str] = None, term: Optional[str] = None) -> None:
    cfg = _load_config(root)

    # minimalne oczekiwane ścieżki (dopasuj do Twojego config-a)
    paths = ProjectPaths(root=root)
    duckdb_path = _resolve_path(root, cfg.get("duckdb_path") or cfg.get("warehouse", {}).get("duckdb_path") or str(paths.duckdb_path))
    run_dir = _resolve_path(root, cfg.get("run_dir") or "_run") or (root / "_run")

    aggregation = cfg.get("aggregation", {}) or {}
    include_deleted_in_percent = bool(aggregation.get("include_deleted_in_percent", False))
    rebuild_full = bool(cfg.get("rebuild_full", False))

    pct_round_decimals = int(cfg.get("stats", {}).get("pct_round_decimals", 4))

    map_teacher_id_email_path = _resolve_path(root, cfg.get("mapping", {}).get("teacher_id_email"))
    map_email_hr_path = _resolve_path(root, cfg.get("mapping", {}).get("email_hr"))

    # okres: CLI ma pierwszeństwo, potem config
    ay_eff = ay or cfg.get("period", {}).get("ay")
    term_eff = term or cfg.get("period", {}).get("term")
    if not rebuild_full and (not ay_eff or not term_eff):
        raise ConfigError(
            "Brak ay/term. Ustaw period.ay + period.term w config.yaml albo podaj w CLI, albo włącz rebuild_full=true."
        )

    sc = StatsConfig(
        duckdb_path=duckdb_path,
        run_dir=run_dir,
        include_deleted_in_percent=include_deleted_in_percent,
        rebuild_full=rebuild_full,
        map_teacher_id_email_path=map_teacher_id_email_path,
        map_email_hr_path=map_email_hr_path,
        pct_round_decimals=pct_round_decimals,
        ay=ay_eff,
        term=term_eff,
    )

    _ensure_run_artifacts(sc.run_dir)
    _log_progress(sc.run_dir, {"step": "start", "duckdb_path": str(sc.duckdb_path)})

    # wczytaj mapowania (pandas OK, ale tylko jako lookup tables)
    df_tid_email = _read_mapping_teacher_email(sc.map_teacher_id_email_path)
    df_email_hr = _read_mapping_email_hr(sc.map_email_hr_path)

    con = duckdb.connect(str(sc.duckdb_path))
    try:
        con.execute("CREATE SCHEMA IF NOT EXISTS mart;")

        # wstrzyknij mappingi do DuckDB
        con.register("map_tid_email_df", df_tid_email)
        con.register("map_email_hr_df", df_email_hr)

        con.execute("""
            CREATE OR REPLACE TEMP VIEW map_tid_email AS
            SELECT DISTINCT
                teacher_id,
                lower(trim(email)) AS email
            FROM map_tid_email_df
            WHERE teacher_id IS NOT NULL AND teacher_id <> ''
              AND email IS NOT NULL AND email <> '';
        """)

        con.execute("""
            CREATE OR REPLACE TEMP VIEW map_email_hr AS
            SELECT DISTINCT
                lower(trim(email)) AS email,
                nullif(trim(full_name), '') AS full_name,
                nullif(trim(wydzial), '') AS wydzial,
                nullif(trim(jednostka), '') AS jednostka
            FROM map_email_hr_df
            WHERE email IS NOT NULL AND email <> '';
        """)

        # okres filter
        period_where = ""
        if not sc.rebuild_full:
            period_where = "AND ay = ? AND term = ?"

        _log_progress(sc.run_dir, {"step": "prepare_events_period", "rebuild_full": sc.rebuild_full, "ay": sc.ay, "term": sc.term})

        con.execute(f"""
            CREATE OR REPLACE TEMP VIEW events_period AS
            SELECT
                course_code,
                ay,
                term,
                wydzial_code,
                kierunek_code,
                track_code,
                semester_code,
                ts_utc,
                teacher_id,
                operation,
                tech_key,
                activity_label,
                object_id,
                count_mode
            FROM events_canonical
            WHERE counted = true
            {period_where};
        """, ([] if sc.rebuild_full else [sc.ay, sc.term]))

        # join ze stanem aktywności
        _log_progress(sc.run_dir, {"step": "join_activities_state"})

        con.execute("""
            CREATE OR REPLACE TEMP VIEW joined_state AS
            SELECT
                e.*,
                s.status_final,
                s.deleted_at,
                s.visible_last,
                s.confidence_deleted,
                CASE
                    WHEN s.activity_id IS NULL THEN 1 ELSE 0
                END AS qa_missing_state
            FROM events_period e
            LEFT JOIN mart.activities_state s
              ON s.course_code = e.course_code
             AND s.activity_id = e.object_id;
        """)

        # QA: students + missing mappings
        _log_progress(sc.run_dir, {"step": "qa_teacher_mapping"})

        con.execute("""
            CREATE OR REPLACE TEMP VIEW teacher_enriched AS
            SELECT
                j.*,
                m.email,
                h.full_name,
                h.wydzial AS hr_wydzial,
                h.jednostka AS hr_jednostka,
                CASE WHEN m.email ILIKE '%@student.umw.edu.pl' THEN 1 ELSE 0 END AS is_student,
                CASE WHEN m.email IS NULL THEN 1 ELSE 0 END AS qa_missing_email,
                CASE WHEN m.email IS NOT NULL AND h.email IS NULL THEN 1 ELSE 0 END AS qa_missing_hr
            FROM joined_state j
            LEFT JOIN map_tid_email m
              ON m.teacher_id = j.teacher_id
            LEFT JOIN map_email_hr h
              ON h.email = m.email;
        """)

        # QA table init
        con.execute("""
            CREATE TABLE IF NOT EXISTS mart.metrics_qa (
                ay VARCHAR,
                term VARCHAR,
                qa_type VARCHAR,
                teacher_id VARCHAR,
                course_code VARCHAR,
                tech_key VARCHAR,
                object_id VARCHAR,
                details VARCHAR,
                created_at TIMESTAMP
            );
        """)

        # incremental delete for qa for period (opcjonalnie)
        if not sc.rebuild_full:
            con.execute("DELETE FROM mart.metrics_qa WHERE ay = ? AND term = ?;", [sc.ay, sc.term])

        # wpisy QA: student ignored
        con.execute("""
            INSERT INTO mart.metrics_qa
            SELECT
                ay, term,
                'STUDENT_IGNORED' AS qa_type,
                teacher_id, course_code, tech_key, object_id,
                'email=' || coalesce(email,'') AS details,
                now() AS created_at
            FROM teacher_enriched
            WHERE is_student = 1;
        """)

        # QA: brak email dla teacher_id
        con.execute("""
            INSERT INTO mart.metrics_qa
            SELECT
                ay, term,
                'MISSING_EMAIL_MAPPING' AS qa_type,
                teacher_id, course_code, tech_key, object_id,
                'teacher_id has no email mapping' AS details,
                now() AS created_at
            FROM teacher_enriched
            WHERE is_student = 0 AND qa_missing_email = 1;
        """)

        # QA: brak HR
        con.execute("""
            INSERT INTO mart.metrics_qa
            SELECT
                ay, term,
                'MISSING_HR_MAPPING' AS qa_type,
                teacher_id, course_code, tech_key, object_id,
                'email=' || coalesce(email,'') AS details,
                now() AS created_at
            FROM teacher_enriched
            WHERE is_student = 0 AND qa_missing_email = 0 AND qa_missing_hr = 1;
        """)

        # QA: event bez wpisu w activities_state
        con.execute("""
            INSERT INTO mart.metrics_qa
            SELECT
                ay, term,
                'EVENT_WITHOUT_ACTIVITY_STATE' AS qa_type,
                teacher_id, course_code, tech_key, object_id,
                'no activities_state row for object_id' AS details,
                now() AS created_at
            FROM teacher_enriched
            WHERE qa_missing_state = 1;
        """)

        # QA: confidence_deleted < 1
        con.execute("""
            INSERT INTO mart.metrics_qa
            SELECT
                ay, term,
                'CONFIDENCE_LT_1' AS qa_type,
                teacher_id, course_code, tech_key, object_id,
                'confidence_deleted=' || cast(confidence_deleted AS VARCHAR) AS details,
                now() AS created_at
            FROM teacher_enriched
            WHERE confidence_deleted IS NOT NULL AND confidence_deleted < 1;
        """)

        # baza do metryk: tylko teachers z HR + nie-studenci
        con.execute("""
            CREATE OR REPLACE TEMP VIEW teacher_ok AS
            SELECT *
            FROM teacher_enriched
            WHERE is_student = 0
              AND qa_missing_email = 0
              AND qa_missing_hr = 0;
        """)

        # widoczne do procentów
        con.execute("""
            CREATE OR REPLACE TEMP VIEW visible_ok AS
            SELECT *
            FROM teacher_ok
            WHERE status_final = 'visible_active';
        """)

        # agregaty QA (deleted/hidden/unknown) per teacher/course/tech_key
        # (tu zakładam status_final wartości: visible_active / deleted / hidden / unknown; dopasuj jeśli inne)
        con.execute("""
            CREATE OR REPLACE TEMP VIEW qa_counts AS
            SELECT
                ay, term,
                teacher_id,
                course_code,
                tech_key,
                SUM(CASE WHEN status_final = 'deleted' THEN 1 ELSE 0 END) AS deleted_count,
                SUM(CASE WHEN status_final = 'hidden' THEN 1 ELSE 0 END) AS hidden_count,
                SUM(CASE WHEN status_final IS NULL OR status_final = 'unknown' THEN 1 ELSE 0 END) AS unknown_count,
                MAX(CASE WHEN confidence_deleted IS NOT NULL AND confidence_deleted < 1 THEN 1 ELSE 0 END) AS confidence_flag
            FROM teacher_ok
            GROUP BY 1,2,3,4,5;
        """)

        # counts (widoczne) z uwzględnieniem count_mode
        # UWAGA: zakładam, że w obrębie (teacher, course, tech_key) count_mode jest spójny.
        con.execute("""
            CREATE OR REPLACE TEMP VIEW counts_visible AS
            SELECT
                ay, term,
                teacher_id,
                course_code,
                wydzial_code,
                kierunek_code,
                tech_key,
                any_value(activity_label) AS activity_label,
                CASE
                    WHEN max(count_mode) = 'object-based' THEN COUNT(DISTINCT object_id)
                    ELSE COUNT(*)
                END AS count_value,
                CASE WHEN min(count_mode) <> max(count_mode) THEN 1 ELSE 0 END AS qa_mixed_count_mode
            FROM visible_ok
            GROUP BY 1,2,3,4,5,6,7;
        """)

        # QA: mixed count_mode
        con.execute("""
            INSERT INTO mart.metrics_qa
            SELECT
                ay, term,
                'MIXED_COUNT_MODE' AS qa_type,
                teacher_id, course_code, tech_key, NULL AS object_id,
                'min!=max count_mode in group' AS details,
                now() AS created_at
            FROM counts_visible
            WHERE qa_mixed_count_mode = 1;
        """)

        # procenty - tylko visible_active
        # pct_course: sum per course_code+tech_key
        # pct_kierunek: sum per kierunek_code+tech_key
        # pct_wydzial: sum per wydzial_code+tech_key
        # pct_uczelnia: sum per ay+term+tech_key
        con.execute(f"""
            CREATE OR REPLACE TEMP VIEW metrics_core AS
            SELECT
                c.ay, c.term,
                c.teacher_id,
                c.course_code,
                c.wydzial_code,
                c.kierunek_code,
                c.tech_key,
                c.activity_label,
                c.count_value,

                ROUND(
                    c.count_value
                    / NULLIF(SUM(c.count_value) OVER (PARTITION BY c.ay, c.term, c.course_code, c.tech_key), 0),
                    {sc.pct_round_decimals}
                ) AS pct_course,

                ROUND(
                    c.count_value
                    / NULLIF(SUM(c.count_value) OVER (PARTITION BY c.ay, c.term, c.kierunek_code, c.tech_key), 0),
                    {sc.pct_round_decimals}
                ) AS pct_kierunek,

                ROUND(
                    c.count_value
                    / NULLIF(SUM(c.count_value) OVER (PARTITION BY c.ay, c.term, c.wydzial_code, c.tech_key), 0),
                    {sc.pct_round_decimals}
                ) AS pct_wydzial,

                ROUND(
                    c.count_value
                    / NULLIF(SUM(c.count_value) OVER (PARTITION BY c.ay, c.term, c.tech_key), 0),
                    {sc.pct_round_decimals}
                ) AS pct_uczelnia
            FROM counts_visible c;
        """)

        # metrics_long table
        con.execute("""
            CREATE TABLE IF NOT EXISTS mart.metrics_long (
                ay VARCHAR,
                term VARCHAR,
                teacher_id VARCHAR,
                full_name VARCHAR,
                email VARCHAR,
                wydzial VARCHAR,
                jednostka VARCHAR,
                course_code VARCHAR,
                tech_key VARCHAR,
                activity_label VARCHAR,
                count_value BIGINT,
                pct_course DOUBLE,
                pct_kierunek DOUBLE,
                pct_wydzial DOUBLE,
                pct_uczelnia DOUBLE,
                deleted_count BIGINT,
                hidden_count BIGINT,
                unknown_count BIGINT,
                confidence_flag BOOLEAN
            );
        """)

        # incremental delete for long for period
        if not sc.rebuild_full:
            _log_progress(sc.run_dir, {"step": "incremental_delete_long", "ay": sc.ay, "term": sc.term})
            con.execute("DELETE FROM mart.metrics_long WHERE ay = ? AND term = ?;", [sc.ay, sc.term])
        else:
            _log_progress(sc.run_dir, {"step": "rebuild_full_long"})
            con.execute("DELETE FROM mart.metrics_long;")

        # insert long = core + HR + QA counts
        _log_progress(sc.run_dir, {"step": "insert_metrics_long"})

        con.execute("""
            INSERT INTO mart.metrics_long
            SELECT
                mc.ay,
                mc.term,
                mc.teacher_id,
                h.full_name,
                m.email,
                h.hr_wydzial AS wydzial,
                h.hr_jednostka AS jednostka,
                mc.course_code,
                mc.tech_key,
                mc.activity_label,
                mc.count_value,
                mc.pct_course,
                mc.pct_kierunek,
                mc.pct_wydzial,
                mc.pct_uczelnia,
                coalesce(q.deleted_count, 0) AS deleted_count,
                coalesce(q.hidden_count, 0) AS hidden_count,
                coalesce(q.unknown_count, 0) AS unknown_count,
                coalesce(q.confidence_flag, 0) = 1 AS confidence_flag
            FROM metrics_core mc
            JOIN (
                SELECT DISTINCT teacher_id, email FROM map_tid_email
            ) m ON m.teacher_id = mc.teacher_id
            JOIN (
                SELECT DISTINCT
                    t.teacher_id,
                    t.email,
                    t.full_name,
                    t.hr_wydzial,
                    t.hr_jednostka
                FROM teacher_ok t
            ) h ON h.teacher_id = mc.teacher_id
            LEFT JOIN qa_counts q
              ON q.ay = mc.ay AND q.term = mc.term
             AND q.teacher_id = mc.teacher_id
             AND q.course_code = mc.course_code
             AND q.tech_key = mc.tech_key;
        """)

        # metrics_wide table (dynamic columns)
        con.execute("""
            CREATE TABLE IF NOT EXISTS mart.metrics_wide (
                ay VARCHAR,
                term VARCHAR,
                teacher_id VARCHAR,
                course_code VARCHAR
                -- dynamic columns added by INSERT SELECT (DuckDB allows it if table has those cols; so we recreate per period)
            );
        """)

        # dla wide: lepiej robić per okres -> create/replace temp wide i potem zapisać do tabeli partycjonowanej
        # w DuckDB najprościej: skasować okres i wstawić wynik z dynamicznego SELECT do "mart.metrics_wide_period",
        # a potem zmergować. Tu robię wersję: trzymamy wide jako "append-only per period" i usuwamy tylko okres.
        if not sc.rebuild_full:
            _log_progress(sc.run_dir, {"step": "incremental_delete_wide", "ay": sc.ay, "term": sc.term})
            con.execute("DELETE FROM mart.metrics_wide WHERE ay = ? AND term = ?;", [sc.ay, sc.term])
        else:
            con.execute("DELETE FROM mart.metrics_wide;")

        # pobierz tech_key dla okresu
        tech_keys: Sequence[str] = [r[0] for r in con.execute("""
            SELECT DISTINCT tech_key
            FROM mart.metrics_long
            WHERE ay = ? AND term = ?
            ORDER BY tech_key;
        """, [sc.ay, sc.term]).fetchall()] if not sc.rebuild_full else [r[0] for r in con.execute("""
            SELECT DISTINCT tech_key FROM mart.metrics_long ORDER BY tech_key;
        """).fetchall()]

        _log_progress(sc.run_dir, {"step": "build_metrics_wide", "tech_keys": list(tech_keys)})

        # dynamiczne kolumny (count_*, pct_course_* ...)
        # UWAGA: nazwy kolumn muszą być bezpieczne -> tech_key normalizujemy do [a-zA-Z0-9_]
        def safe_col(s: str) -> str:
            out = []
            for ch in s:
                if ch.isalnum():
                    out.append(ch)
                else:
                    out.append("_")
            return "".join(out)

        select_cols = ["ay", "term", "teacher_id", "course_code"]
        for tk in tech_keys:
            c = safe_col(tk)
            select_cols.append(f"MAX(CASE WHEN tech_key='{tk}' THEN count_value END) AS count_{c}")
            select_cols.append(f"MAX(CASE WHEN tech_key='{tk}' THEN pct_course END) AS pct_course_{c}")
            select_cols.append(f"MAX(CASE WHEN tech_key='{tk}' THEN pct_kierunek END) AS pct_kierunek_{c}")
            select_cols.append(f"MAX(CASE WHEN tech_key='{tk}' THEN pct_wydzial END) AS pct_wydzial_{c}")
            select_cols.append(f"MAX(CASE WHEN tech_key='{tk}' THEN pct_uczelnia END) AS pct_uczelnia_{c}")

        wide_sql = f"""
            INSERT INTO mart.metrics_wide
            SELECT
                {", ".join(select_cols)}
            FROM mart.metrics_long
            {"WHERE ay = ? AND term = ?" if not sc.rebuild_full else ""}
            GROUP BY ay, term, teacher_id, course_code;
        """

        if not sc.rebuild_full:
            con.execute(wide_sql, [sc.ay, sc.term])
        else:
            con.execute(wide_sql)

        # artefakty
        _log_progress(sc.run_dir, {"step": "done"})
        _write_ok(sc.run_dir)

    finally:
        con.close()

================================================================================
src\mrna_plum\store\__init__.py
================================================================================

from .duckdb_store import DuckDbStore
__all__ = ["DuckDbStore"]


================================================================================
src\mrna_plum\store\database.py
================================================================================

# src/mrna_plum/store/database.py

from __future__ import annotations

from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional

import pandas as pd


class EventStore:
    def __init__(self, cfg: Dict[str, Any]):
        self.cfg = cfg
        self.db_path = Path(cfg["paths"]["db_path"])
        self.parquet_root = Path(cfg["paths"]["parquet_root"]) if cfg.get("paths", {}).get("parquet_root") else None

    def _connect(self):
        import duckdb
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        return duckdb.connect(str(self.db_path))

    def ensure_schema(self) -> None:
        con = self._connect()
        try:
            con.execute(
                """
                CREATE TABLE IF NOT EXISTS events_canonical_raw (
                    row_key VARCHAR,
                    course VARCHAR,
                    course_code VARCHAR,
                    wydzial_code VARCHAR,
                    kierunek_code VARCHAR,
                    track_code VARCHAR,
                    semester_code VARCHAR,
                    course_name VARCHAR,
                    ay VARCHAR,
                    term VARCHAR,
                    ts_utc TIMESTAMP,
                    teacher_id BIGINT,
                    operation VARCHAR,
                    tech_key VARCHAR,
                    activity_label VARCHAR,
                    object_id BIGINT,
                    count_mode VARCHAR,
                    raw_line_hash VARCHAR,
                    source_file VARCHAR,
                    payload_json VARCHAR
                );
                """
            )
            con.execute(
                """
                CREATE TABLE IF NOT EXISTS events_conflicts (
                    row_key VARCHAR,
                    course_code VARCHAR,
                    teacher_id BIGINT,
                    tech_key VARCHAR,
                    operation VARCHAR,
                    object_id BIGINT,
                    note VARCHAR
                );
                """
            )
            con.execute(
                """
                CREATE TABLE IF NOT EXISTS events_canonical (
                    -- finalna tabela do statystyk
                    row_key VARCHAR,
                    course VARCHAR,
                    course_code VARCHAR,
                    wydzial_code VARCHAR,
                    kierunek_code VARCHAR,
                    track_code VARCHAR,
                    semester_code VARCHAR,
                    course_name VARCHAR,
                    ay VARCHAR,
                    term VARCHAR,
                    ts_utc TIMESTAMP,
                    teacher_id BIGINT,
                    operation VARCHAR,
                    tech_key VARCHAR,
                    activity_label VARCHAR,
                    object_id BIGINT,
                    count_mode VARCHAR,
                    counted BOOLEAN,
                    raw_line_hash VARCHAR,
                    source_file VARCHAR
                );
                """
            )
            # indeksy logiczne / dedup incremental
            con.execute("CREATE UNIQUE INDEX IF NOT EXISTS ux_events_canonical_raw_rowkey ON events_canonical_raw(row_key);")
            con.execute("CREATE UNIQUE INDEX IF NOT EXISTS ux_events_canonical_rowkey ON events_canonical(row_key);")
        finally:
            con.close()

    def insert_raw_batch(self, rows: List[Dict[str, Any]]) -> int:
        if not rows:
            return 0
        con = self._connect()
        try:
            df = pd.DataFrame(rows)
            con.register("df_batch", df)
            # INSERT OR IGNORE po unique index (duckdb: użyj anti-join)
            con.execute(
                """
                INSERT INTO events_canonical_raw
                SELECT b.*
                FROM df_batch b
                LEFT JOIN events_canonical_raw e ON e.row_key = b.row_key
                WHERE e.row_key IS NULL;
                """
            )
            return len(rows)
        finally:
            con.close()

    def insert_conflicts_batch(self, rows: List[Dict[str, Any]]) -> int:
        if not rows:
            return 0
        con = self._connect()
        try:
            df = pd.DataFrame(rows)
            con.register("df_conf", df)
            con.execute("INSERT INTO events_conflicts SELECT * FROM df_conf;")
            return len(rows)
        finally:
            con.close()

    def finalize_canonical(self) -> int:
        """
        Zasada:
        - count_mode TAK_FLAG + TAK dla tego samego (course_code, teacher_id, tech_key, object_id)
          -> counted = false dla tych zdarzeń (unieważnienie)
        - jeśli object_id IS NULL -> liczymy event-based: counted=true jeśli count_mode='TAK'
        """
        con = self._connect()
        try:
            # przetwarzaj tylko nowe row_key
            con.execute(
                """
                INSERT INTO events_canonical
                WITH base AS (
                    SELECT r.*
                    FROM events_canonical_raw r
                    LEFT JOIN events_canonical c ON c.row_key = r.row_key
                    WHERE c.row_key IS NULL
                ),
                has_both AS (
                    SELECT
                        course_code,
                        teacher_id,
                        tech_key,
                        object_id,
                        MAX(CASE WHEN upper(count_mode)='TAK' THEN 1 ELSE 0 END) AS has_tak,
                        MAX(CASE WHEN upper(count_mode)='TAK_FLAG' THEN 1 ELSE 0 END) AS has_flag
                    FROM base
                    WHERE object_id IS NOT NULL
                    GROUP BY 1,2,3,4
                )
                SELECT
                    b.row_key,
                    b.course,
                    b.course_code,
                    b.wydzial_code,
                    b.kierunek_code,
                    b.track_code,
                    b.semester_code,
                    b.course_name,
                    b.ay,
                    b.term,
                    b.ts_utc,
                    b.teacher_id,
                    b.operation,
                    b.tech_key,
                    b.activity_label,
                    b.object_id,
                    b.count_mode,
                    CASE
                        WHEN b.object_id IS NULL THEN (upper(b.count_mode)='TAK')
                        ELSE (
                            NOT EXISTS (
                                SELECT 1 FROM has_both hb
                                WHERE hb.course_code=b.course_code
                                  AND hb.teacher_id=b.teacher_id
                                  AND hb.tech_key=b.tech_key
                                  AND hb.object_id=b.object_id
                                  AND hb.has_tak=1 AND hb.has_flag=1
                            )
                            AND (upper(b.count_mode)='TAK')
                        )
                    END AS counted,
                    b.raw_line_hash,
                    b.source_file
                FROM base b;
                """
            )

            # duckdb nie zwraca rowcount wprost stabilnie; policzmy różnicę
            res = con.execute("SELECT COUNT(*) FROM events_canonical;").fetchone()
            return int(res[0]) if res else 0
        finally:
            con.close()

    def export_parquet(self) -> Optional[Path]:
        if not self.parquet_root:
            return None
        out = self.parquet_root / "events_canonical.parquet"
        self.parquet_root.mkdir(parents=True, exist_ok=True)
        con = self._connect()
        try:
            con.execute(f"COPY (SELECT * FROM events_canonical) TO '{str(out).replace('\\\\', '/')}' (FORMAT PARQUET);")
            return out
        finally:
            con.close()

================================================================================
src\mrna_plum\store\duckdb_store.py
================================================================================

from __future__ import annotations
from pathlib import Path
import duckdb


import json
import hashlib
from dataclasses import dataclass
from typing import Iterable, Optional

class DuckDbStore:
    def __init__(self, db_path: Path):
        db_path.parent.mkdir(parents=True, exist_ok=True)
        self.db_path = db_path

    def connect(self) -> duckdb.DuckDBPyConnection:
        return duckdb.connect(str(self.db_path))

    def init_schema(self) -> None:
        with self.connect() as con:
            con.execute("""
                CREATE TABLE IF NOT EXISTS raw_logs (
                    _source_file VARCHAR,
                    "Czas" VARCHAR,
                    "Kontekst zdarzenia" VARCHAR,
                    "Opis" VARCHAR,
                    "Składnik" VARCHAR,
                    "Nazwa zdarzenia" VARCHAR,
                    course_code VARCHAR,
                    course_id BIGINT,
                    period VARCHAR,
                    tech_key VARCHAR,
                    activity VARCHAR,
                    operation VARCHAR,
                    count_to_report BOOLEAN,
                    teacher_id VARCHAR,
                    object_id VARCHAR,
                    rule_priority INTEGER
                );
            """)
            con.execute("""
                CREATE TABLE IF NOT EXISTS stats_agg (
                    period VARCHAR,
                    course_code VARCHAR,
                    teacher_id VARCHAR,
                    tech_key VARCHAR,
                    cnt_events BIGINT,
                    cnt_objects BIGINT,
                    is_invalidated BOOLEAN
                );
            """)

    def load_parquet_to_raw(self, parquet_path: Path) -> None:
        with self.connect() as con:
            # prosty append; w praktyce możesz TRUNCATE per-run
            con.execute("INSERT INTO raw_logs SELECT * FROM read_parquet(?)", [str(parquet_path)])



@dataclass(frozen=True)
class EventRawRow:
    course: str
    course_id: Optional[int]
    time_text: Optional[str]
    time_ts_iso: Optional[str]  # ISO string (UTC/naive) lub None
    row_key: str                # sha256(normalized_fields_join)
    payload_json: str           # JSON dict: header->value
    source_file: str            # pełna ścieżka lub nazwa


def _connect(db_path: Path) -> duckdb.DuckDBPyConnection:
    db_path.parent.mkdir(parents=True, exist_ok=True)
    con = duckdb.connect(str(db_path))
    # bezpieczne ustawienia, brak wymogów
    return con


def ensure_schema(con: duckdb.DuckDBPyConnection) -> None:
    con.execute(
        """
        CREATE TABLE IF NOT EXISTS events_raw (
            course       TEXT NOT NULL,
            time_text    TEXT,
            time_ts      TIMESTAMP,
            row_key      TEXT NOT NULL,
            payload_json TEXT NOT NULL,
            source_file  TEXT NOT NULL,
            inserted_at  TIMESTAMP DEFAULT now()
        );
        """
    )
    # indeksy opcjonalnie (DuckDB "CREATE INDEX" działa w nowszych wersjach)
    try:
        con.execute("CREATE INDEX IF NOT EXISTS idx_events_raw_course ON events_raw(course);")
    except Exception:
        pass
    try:
        con.execute("CREATE INDEX IF NOT EXISTS idx_events_raw_rowkey ON events_raw(row_key);")
    except Exception:
        pass


def create_stage_table(con: duckdb.DuckDBPyConnection) -> None:
    con.execute("DROP TABLE IF EXISTS _events_raw_stage;")
    con.execute(
        """
        CREATE TEMP TABLE _events_raw_stage (
            course       TEXT NOT NULL,
            time_text    TEXT,
            time_ts      TIMESTAMP,
            row_key      TEXT NOT NULL,
            payload_json TEXT NOT NULL,
            source_file  TEXT NOT NULL
        );
        """
    )


def insert_stage_rows(con: duckdb.DuckDBPyConnection, rows: list[EventRawRow]) -> None:
    if not rows:
        return
    params = [
        (
            r.course,
            r.time_text,
            r.time_ts_iso,  # DuckDB potrafi zrzucić ISO->TIMESTAMP
            r.row_key,
            r.payload_json,
            r.source_file,
        )
        for r in rows
    ]
    con.executemany(
        """
        INSERT INTO _events_raw_stage(course, time_text, time_ts, row_key, payload_json, source_file)
        VALUES (?, ?, ?, ?, ?, ?);
        """,
        params,
    )


def merge_stage_into_events_raw(con: duckdb.DuckDBPyConnection) -> int:
    """
    Dedup: tylko jeśli CAŁY wiersz identyczny (po Trim, bez BOM, bez CR).
    My realizujemy to przez:
      - payload_json z trimowanymi wartościami
      - row_key = sha256(normalized_fields_join)
    Wstawiamy tylko jeśli (course,row_key,payload_json) nie istnieje.
    """
    res = con.execute(
        """
        INSERT INTO events_raw(course, time_text, time_ts, row_key, payload_json, source_file)
        SELECT s.course, s.time_text, s.time_ts, s.row_key, s.payload_json, s.source_file
        FROM _events_raw_stage s
        LEFT JOIN events_raw e
          ON e.course = s.course
         AND e.row_key = s.row_key
         AND e.payload_json = s.payload_json
        WHERE e.course IS NULL;
        """
    )
    # DuckDB python: rowcount by cursor.rowcount is not always reliable; use changes()
    try:
        return con.execute("SELECT changes();").fetchone()[0]
    except Exception:
        return 0


def open_store(db_path: Path) -> duckdb.DuckDBPyConnection:
    con = _connect(db_path)
    ensure_schema(con)
    return con


def export_course_to_csv(
    con: duckdb.DuckDBPyConnection,
    *,
    course: str,
    out_csv: Path,
) -> None:
    """
    CSV-compat: zapis *_full_log.csv posortowany po time_ts malejąco (jeśli jest),
    w przeciwnym razie stabilnie po inserted_at.
    Zapisujemy same payload_json jako jedną kolumnę? – NIE: eksportujemy jako CSV z kolumną payload_json.
    (Jeśli potrzebujesz 1:1 zgodności z VBA-headerami, da się to rozwinąć później na etapie parse.)
    """
    out_csv.parent.mkdir(parents=True, exist_ok=True)
    con.execute(
        f"""
        COPY (
            SELECT course, time_text, time_ts, payload_json, source_file
            FROM events_raw
            WHERE course = ?
            ORDER BY
              CASE WHEN time_ts IS NULL THEN 1 ELSE 0 END,
              time_ts DESC,
              inserted_at DESC
        )
        TO ?
        (HEADER, DELIMITER ';', QUOTE '"', ESCAPE '"');
        """,
        [course, str(out_csv)],
    )


def export_course_to_parquet(
    con: duckdb.DuckDBPyConnection,
    *,
    course: str,
    out_parquet: Path,
) -> None:
    out_parquet.parent.mkdir(parents=True, exist_ok=True)
    con.execute(
        """
        COPY (
            SELECT course, time_text, time_ts, payload_json, source_file, inserted_at
            FROM events_raw
            WHERE course = ?
            ORDER BY
              CASE WHEN time_ts IS NULL THEN 1 ELSE 0 END,
              time_ts DESC,
              inserted_at DESC
        )
        TO ?
        (FORMAT PARQUET);
        """,
        [course, str(out_parquet)],    )


================================================================================
src\mrna_plum\tests\__init__.py
================================================================================



================================================================================
src\mrna_plum\tests\test_build_activities_state.py
================================================================================

from __future__ import annotations

from datetime import datetime
import duckdb
import pytest

from mrna_plum.activities.activities_state import (
    BuildConfig, DeletionConfig, MappingConfig, IncrementalConfig, build_activities_state
)

def _cfg(**kw):
    return BuildConfig(
        deletion=DeletionConfig(
            delete_operations=kw.get("delete_operations", ["DELETE"]),
            delete_tech_keys=kw.get("delete_tech_keys", []),
            delete_activity_labels_regex=[],
            disappearance_grace_period_days=kw.get("grace", 14),
            min_missing_snapshots_to_confirm=kw.get("min_missing", 2),
            deleted_at_policy=kw.get("policy", "first_missing"),
        ),
        mapping=MappingConfig(
            use_activity_id_map_table=True,
            allow_fuzzy_name_type_match=False,
        ),
        incremental=IncrementalConfig(
            checkpoint_table="raw.pipeline_checkpoints",
            checkpoint_key="build_activities_state",
            process_only_new_snapshots=False,
            process_only_new_events=False,
        ),
    )

def setup_base(con):
    con.execute("create schema if not exists raw;")
    con.execute("create schema if not exists mart;")

    con.execute("""
      create table events_canonical (
        course_code varchar,
        ay varchar,
        term varchar,
        wydzial_code varchar,
        kierunek_code varchar,
        track_code varchar,
        semester_code varchar,
        ts_utc timestamp,
        teacher_id varchar,
        operation varchar,
        tech_key varchar,
        activity_label varchar,
        object_id varchar,
        count_mode varchar,
        row_key varchar,
        source_file varchar,
        counted boolean
      );
    """)
    con.execute("""
      create table raw.activities_snapshot(
        course_code varchar,
        activity_id varchar,
        name varchar,
        type varchar,
        visible_to_students boolean,
        captured_at timestamp,
        source_file varchar,
        row_key varchar
      );
    """)
    con.execute("""
      create table raw.activity_id_map(
        course_code varchar,
        activity_id varchar,
        object_id varchar,
        map_method varchar,
        confidence double,
        first_seen_at timestamp,
        last_seen_at timestamp,
        primary key(course_code, activity_id)
      );
    """)

def test_delete_from_logs():
    con = duckdb.connect(":memory:")
    setup_base(con)

    con.execute("""
      insert into raw.activities_snapshot values
      ('C1','101','Quiz 1','quiz', true, '2026-02-01 10:00:00','f.csv','rk1'),
      ('C1','101','Quiz 1','quiz', true, '2026-02-10 10:00:00','f.csv','rk2')
    """)

    con.execute("""
      insert into events_canonical values
      ('C1','2025/26','Z','W1','K1','T1','S1','2026-02-05 12:00:00','U1','DELETE','tk_del','', '101','object-based','e1','e.csv', true)
    """)

    stats = build_activities_state(con, _cfg(delete_operations=["DELETE"]))
    row = con.execute("select status_final, evidence_deleted, deleted_at from mart.activities_state where course_code='C1' and activity_id='101'").fetchone()
    assert row[0] == "visible_deleted"
    assert row[1] in ("log_delete_event", "both")
    assert row[2] is not None

def test_disappearance_from_snapshots():
    con = duckdb.connect(":memory:")
    setup_base(con)

    # snapshoty kursu: aktywność znika po 2026-02-01, potem mamy 2 snapshoty bez niej i grace spełniony
    con.execute("""
      insert into raw.activities_snapshot values
      ('C1','200','Page A','page', true, '2026-02-01 10:00:00','f.csv','a1'),
      ('C1','201','Other','page', true, '2026-02-08 10:00:00','f.csv','a2'),
      ('C1','201','Other','page', true, '2026-02-20 10:00:00','f.csv','a3')
    """)
    # brak eventów

    stats = build_activities_state(con, _cfg(grace=7, min_missing=2, policy="first_missing"))
    row = con.execute("select status_final, evidence_deleted from mart.activities_state where course_code='C1' and activity_id='200'").fetchone()
    assert row[1] == "snapshot_disappearance"
    assert row[0] == "visible_deleted"

def test_hidden():
    con = duckdb.connect(":memory:")
    setup_base(con)

    con.execute("""
      insert into raw.activities_snapshot values
      ('C1','300','Forum','forum', false, '2026-02-20 10:00:00','f.csv','h1')
    """)
    stats = build_activities_state(con, _cfg())
    row = con.execute("select status_final from mart.activities_state where course_code='C1' and activity_id='300'").fetchone()
    assert row[0] == "hidden"

def test_conflict_log_delete_but_snapshot_visible():
    con = duckdb.connect(":memory:")
    setup_base(con)

    con.execute("""
      insert into raw.activities_snapshot values
      ('C1','400','H5P','h5p', true, '2026-02-20 10:00:00','f.csv','c1')
    """)
    con.execute("""
      insert into events_canonical values
      ('C1','2025/26','Z','W1','K1','T1','S1','2026-02-10 12:00:00','U1','DELETE','tk_del','', '400','object-based','e1','e.csv', true)
    """)
    build_activities_state(con, _cfg(delete_operations=["DELETE"]))
    qa = con.execute("select count(*) from mart.activities_qa where qa_type='conflict_log_delete_but_visible_in_snapshot'").fetchone()[0]
    assert qa >= 1

def test_missing_mapping_activity_to_object():
    con = duckdb.connect(":memory:")
    setup_base(con)

    # snapshot istnieje, ale w events brak object_id/akcji
    con.execute("""
      insert into raw.activities_snapshot values
      ('C1','500','URL','url', true, '2026-02-20 10:00:00','f.csv','m1')
    """)
    build_activities_state(con, _cfg())
    qa = con.execute("select count(*) from mart.activities_qa where qa_type='activity_without_object_id_mapping'").fetchone()[0]
    assert qa >= 1

================================================================================
src\mrna_plum\tests\test_compute_stats.py
================================================================================

import duckdb
from pathlib import Path
from mrna_plum.stats.compute_stats import compute_stats

def test_pct_course_two_teachers(tmp_path: Path):
    db = tmp_path / "w.duckdb"
    con = duckdb.connect(str(db))

    con.execute("CREATE SCHEMA mart;")

    con.execute("""
        CREATE TABLE events_canonical (
            course_code VARCHAR, ay VARCHAR, term VARCHAR,
            wydzial_code VARCHAR, kierunek_code VARCHAR, track_code VARCHAR, semester_code VARCHAR,
            ts_utc TIMESTAMP, teacher_id VARCHAR, operation VARCHAR,
            tech_key VARCHAR, activity_label VARCHAR, object_id VARCHAR, count_mode VARCHAR,
            counted BOOLEAN
        );
    """)

    con.execute("""
        CREATE TABLE mart.activities_state (
            course_code VARCHAR, activity_id VARCHAR,
            status_final VARCHAR, deleted_at TIMESTAMP,
            visible_last BOOLEAN, confidence_deleted DOUBLE
        );
    """)

    # 2 nauczycieli, ta sama aktywność "PAGE"
    con.execute("""
        INSERT INTO events_canonical VALUES
        ('C1','2025/26','Z','W1','K1','T1','S1', now(), 'T_A','CREATE','PAGE','Strona','10','object-based', true),
        ('C1','2025/26','Z','W1','K1','T1','S1', now(), 'T_B','CREATE','PAGE','Strona','11','object-based', true);
    """)

    con.execute("""
        INSERT INTO mart.activities_state VALUES
        ('C1','10','visible_active', NULL, true, 1.0),
        ('C1','11','visible_active', NULL, true, 1.0);
    """)

    # mapping
    # tu najprościej: zrobisz pliki CSV/XLSX w tmp_path i wskażesz config.yaml
    # ... (pomijam dla czytelności – ale test ma sprawdzić, że oba wejdą do long)
    con.close()

    # prepare root with config + mapping files, then compute_stats(root)
    # then assert pct_course = 0.5 and 0.5

================================================================================
src\mrna_plum\tests\test_export_excel.py
================================================================================

import duckdb
import pytest
from pathlib import Path

from mrna_plum.reports.export_excel import export_summary_excel, ExportOverflowError


def _mk_con_with_tables():
    con = duckdb.connect(":memory:")
    con.execute("CREATE SCHEMA mart;")

    con.execute("""
        CREATE TABLE mart.metrics_long (
            full_name VARCHAR,
            teacher_id VARCHAR,
            course_code VARCHAR,
            tech_key VARCHAR,
            activity_label VARCHAR,
            count_value BIGINT,
            pct_course DOUBLE,
            pct_program DOUBLE,
            pct_faculty DOUBLE,
            pct_university DOUBLE,
            visible_active BOOLEAN
        );
    """)

    con.execute("""
        CREATE TABLE mart.metrics_qa (
            type VARCHAR,
            teacher_id VARCHAR,
            course_code VARCHAR,
            tech_key VARCHAR,
            description VARCHAR
        );
    """)
    return con


def _cfg(tmp_root: Path, **overrides):
    cfg = {
        "root": str(tmp_root),
        "report": {"ay": "2025_2026", "term": "Z"},
        "export": {"max_rows_excel": 1_000_000, "overflow_strategy": "error", "activity_column": "tech_key"},
    }
    # shallow merge
    for k, v in overrides.items():
        if k in cfg and isinstance(cfg[k], dict) and isinstance(v, dict):
            cfg[k].update(v)
        else:
            cfg[k] = v
    return cfg


def test_generates_file_when_data_exists(tmp_path: Path):
    con = _mk_con_with_tables()
    con.execute("""
        INSERT INTO mart.metrics_long VALUES
        ('Anna Nowak','10','BIO101','PAGE','Strona',3,12.3,4.5,1.1,0.2, TRUE),
        ('Anna Nowak','10','BIO101','URL','Adres URL',1, 1.0,0.5,0.2,0.1, TRUE);
    """)
    con.execute("INSERT INTO mart.metrics_qa VALUES ('teacher_id NOT_IN_HR','999','BIO101','PAGE','no HR');")

    cfg = _cfg(tmp_path)
    code, out_path = export_summary_excel(con, cfg)

    assert code == 0
    assert out_path.exists()
    assert (tmp_path / "_run" / "export-excel.ok").exists()
    assert (tmp_path / "_run" / "run.log").exists()
    assert (tmp_path / "_run" / "progress.jsonl").exists()


def test_generates_correct_row_count(tmp_path: Path):
    con = _mk_con_with_tables()
    con.execute("""
        INSERT INTO mart.metrics_long SELECT
            'Jan Kowalski', '1', 'C1', 'A', 'a', 1, 1.1, 2.2, 3.3, 4.4, TRUE
        FROM range(0, 123);
    """)
    cfg = _cfg(tmp_path)
    code, out_path = export_summary_excel(con, cfg)
    assert code == 0
    assert out_path.exists()
    # We don't parse XLSX here (fast test); we ensure INFO counts exist by querying SQL:
    # (INFO sheet writing uses SQL counts; if export didn't crash, it ran)


def test_qa_sheet_is_created_even_if_empty(tmp_path: Path):
    con = _mk_con_with_tables()
    con.execute("""
        INSERT INTO mart.metrics_long VALUES
        ('A A','1','C1','X','x',1,1,1,1,1, TRUE);
    """)
    # QA empty
    cfg = _cfg(tmp_path)
    code, out_path = export_summary_excel(con, cfg)
    assert code == 0
    assert out_path.exists()


def test_sorting_is_sql_ordered(tmp_path: Path):
    con = _mk_con_with_tables()
    # Insert in reverse order; SQL ORDER BY should output A then B, and tech_key sorted
    con.execute("""
        INSERT INTO mart.metrics_long VALUES
        ('B','2','C2','ZZ','zz',1,1,1,1,1, TRUE),
        ('A','1','C1','BB','bb',1,1,1,1,1, TRUE),
        ('A','1','C1','AA','aa',1,1,1,1,1, TRUE);
    """)
    cfg = _cfg(tmp_path)

    # We validate ordering by executing the same SQL builder logic indirectly:
    # minimal assert: export completes; deeper ordering validation would require reading XLSX.
    code, _ = export_summary_excel(con, cfg)
    assert code == 0


def test_overflow_error_strategy_error(tmp_path: Path):
    con = _mk_con_with_tables()
    con.execute("""
        INSERT INTO mart.metrics_long
        SELECT 'X','1','C','A','a',1,1,1,1,1, TRUE
        FROM range(0, 11);
    """)
    cfg = _cfg(tmp_path, export={"max_rows_excel": 10, "overflow_strategy": "error"})
    with pytest.raises(ExportOverflowError):
        export_summary_excel(con, cfg)


def test_overflow_split_creates_file(tmp_path: Path):
    con = _mk_con_with_tables()
    con.execute("""
        INSERT INTO mart.metrics_long
        SELECT 'X','1','C','A','a',1,1,1,1,1, TRUE
        FROM range(0, 25);
    """)
    cfg = _cfg(tmp_path, export={"max_rows_excel": 10, "overflow_strategy": "split"})
    code, out_path = export_summary_excel(con, cfg)
    assert code == 0
    assert out_path.exists()


def test_no_data_creates_xlsx_with_headers_and_info(tmp_path: Path):
    con = _mk_con_with_tables()
    # no rows
    cfg = _cfg(tmp_path)
    code, out_path = export_summary_excel(con, cfg)
    assert code == 0
    assert out_path.exists()

================================================================================
src\mrna_plum\tests\test_export_individual.py
================================================================================

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path

import duckdb
import openpyxl

from mrna_plum.reports.export_individual import export_individual_reports, sanitize_filename


@dataclass
class Cfg:
    root: str
    paths: object
    reports: object


@dataclass
class Paths:
    db_path: str


@dataclass
class Reports:
    individual_dir: str = "_out/indywidualne"
    max_workers: int = 1  # tests: deterministic, no threads
    batch_teachers: int = 50
    create_empty_individual: bool = False


def _make_db(tmp_path: Path) -> duckdb.DuckDBPyConnection:
    db_path = tmp_path / "test.duckdb"
    con = duckdb.connect(str(db_path))
    con.execute("CREATE SCHEMA mart;")
    con.execute(
        """
        CREATE TABLE mart.metrics_long (
            teacher_id      VARCHAR,
            full_name       VARCHAR,
            email           VARCHAR,
            id_bazus        VARCHAR,
            course_name     VARCHAR,
            activity_label  VARCHAR,
            count_value     BIGINT,
            pct_course      DOUBLE,
            pct_kierunek    DOUBLE,
            pct_wydzial     DOUBLE,
            pct_uczelnia    DOUBLE,
            visible_active  BOOLEAN,
            hr_wydzial      VARCHAR,
            hr_jednostka    VARCHAR
        );
        """
    )
    # Teacher A: 2 rows, one has count=0 (must be excluded)
    con.execute(
        """
        INSERT INTO mart.metrics_long VALUES
        ('10', 'Kowalski Jan', 'jan@x.pl', 'B123', 'Kurs A', 'Strona', 3,  50, 10, 5, 1, TRUE, 'WL', 'Katedra X'),
        ('10', 'Kowalski Jan', 'jan@x.pl', 'B123', 'Kurs A', 'Wiki',   0,  20, 10, 5, 1, TRUE, 'WL', 'Katedra X'),
        ('10', 'Kowalski Jan', 'jan@x.pl', 'B123', 'Kurs B', 'Adres URL', 2,  25, 10, 5, 1, TRUE, 'WL', 'Katedra X');
        """
    )
    # Teacher B: only zero rows -> should be SKIPPED and no file
    con.execute(
        """
        INSERT INTO mart.metrics_long VALUES
        ('11', 'Nowak/Anna:*?', 'a@x.pl', 'B999', 'Kurs Z', 'Strona', 0, 10, 1, 1, 1, TRUE, 'WF', 'Jedn Y');
        """
    )
    return con


def test_sanitize_filename_windows_chars():
    assert sanitize_filename('Nowak/Anna:*?') == "Nowak_Anna____"


def test_export_individual_generates_one_file(tmp_path: Path):
    con = _make_db(tmp_path)
    try:
        cfg = Cfg(
            root=str(tmp_path),
            paths=Paths(db_path=str(tmp_path / "test.duckdb")),
            reports=Reports(),
        )
        code, out_dir = export_individual_reports(con, cfg)
        assert code == 0

        out_dir = Path(out_dir)
        files = sorted(out_dir.glob("*.xlsx"))
        # only teacher_id=10 should have file
        assert len(files) == 1
        assert files[0].name.endswith("_10.xlsx")
        assert "Kowalski Jan" in files[0].name

        wb = openpyxl.load_workbook(files[0])
        assert "DANE_KURSY" in wb.sheetnames
        assert "DANE_PERS" in wb.sheetnames

        ws = wb["DANE_KURSY"]
        headers = [ws.cell(1, c).value for c in range(1, 8)]
        assert headers == ["Kurs", "Aktywność", "Liczba", "% kurs", "% kierunek", "% wydział", "% uczelnia"]

        # rows: should exclude count=0, and sorted deterministically
        rows = []
        for r in range(2, ws.max_row + 1):
            rows.append([ws.cell(r, c).value for c in range(1, 8)])
        # expect two rows
        assert len(rows) == 2
        # deterministic sort: Kurs A / Strona first, Kurs B / Adres URL second
        assert rows[0][0] == "Kurs A"
        assert rows[0][1] == "Strona"
        assert rows[1][0] == "Kurs B"
        assert rows[1][1] == "Adres URL"

        # pct in XLSX: stored as 0.xx (not 50)
        # openpyxl reads raw numeric value (formatting is separate)
        assert abs(float(rows[0][3]) - 0.50) < 1e-9

        ws2 = wb["DANE_PERS"]
        # find ID_PLUM row
        kv = {ws2.cell(r, 1).value: ws2.cell(r, 2).value for r in range(2, ws2.max_row + 1)}
        assert kv["ID_PLUM"] == "10"
        assert kv["Pełna nazwa"] == "Kowalski Jan"
        assert kv["E-mail"] == "jan@x.pl"
        assert kv["ID bazus"] == "B123"
        assert kv["Wydział"] == "WL"
        assert kv["Jednostka"] == "Katedra X"

    finally:
        con.close()


def test_export_individual_skips_teacher_with_only_zero_rows(tmp_path: Path):
    con = _make_db(tmp_path)
    try:
        cfg = Cfg(
            root=str(tmp_path),
            paths=Paths(db_path=str(tmp_path / "test.duckdb")),
            reports=Reports(),
        )
        code, out_dir = export_individual_reports(con, cfg)
        assert code == 0

        out_dir = Path(out_dir)
        # verify teacher 11 is not exported
        assert not any(p.name.endswith("_11.xlsx") for p in out_dir.glob("*.xlsx"))
    finally:
        con.close()

================================================================================
src\mrna_plum\ui_bridge\__init__.py
================================================================================

from .progress import ProgressWriter
__all__ = ["ProgressWriter"]


================================================================================
src\mrna_plum\ui_bridge\progress.py
================================================================================

from __future__ import annotations
from dataclasses import dataclass
from datetime import datetime, timezone
from zoneinfo import ZoneInfo
import json
from pathlib import Path
from typing import Any, Optional

WARSAW = ZoneInfo("Europe/Warsaw")

@dataclass
class ProgressWriter:
    path: Path

    def emit(
        self,
        step: str,
        status: str,
        message: str,
        current: Optional[int] = None,
        total: Optional[int] = None,
        extra: Optional[dict[str, Any]] = None,
    ) -> None:
        self.path.parent.mkdir(parents=True, exist_ok=True)
        payload = {
            "ts": datetime.now(WARSAW).isoformat(),
            "step": step,
            "status": status,     # start|progress|done|error
            "message": message,
            "current": current,
            "total": total,
            "extra": extra or {},
        }
        with self.path.open("a", encoding="utf-8") as f:
            f.write(json.dumps(payload, ensure_ascii=False) + "\n")


================================================================================
src\mrna_plum.egg-info\dependency_links.txt
================================================================================




================================================================================
src\mrna_plum.egg-info\entry_points.txt
================================================================================

[console_scripts]
mrna_plum = mrna_plum.cli:app


================================================================================
src\mrna_plum.egg-info\PKG-INFO
================================================================================

Metadata-Version: 2.4
Name: mrna-plum
Version: 0.1.0
Summary: mRNA-PLUM CLI (merge/parse/stats/export) for Moodle/PLUM logs
Requires-Python: >=3.10
Requires-Dist: typer>=0.12.0
Requires-Dist: PyYAML>=6.0
Requires-Dist: duckdb>=1.0.0
Requires-Dist: pandas>=2.0.0
Requires-Dist: pyarrow>=15.0.0
Requires-Dist: openpyxl>=3.1.0


================================================================================
src\mrna_plum.egg-info\requires.txt
================================================================================

typer>=0.12.0
PyYAML>=6.0
duckdb>=1.0.0
pandas>=2.0.0
pyarrow>=15.0.0
openpyxl>=3.1.0


================================================================================
src\mrna_plum.egg-info\SOURCES.txt
================================================================================

pyproject.toml
src/mrna_plum/__init__.py
src/mrna_plum/__main__.py
src/mrna_plum/cli.py
src/mrna_plum/config.py
src/mrna_plum/errors.py
src/mrna_plum/logging_run.py
src/mrna_plum/paths.py
src/mrna_plum.egg-info/PKG-INFO
src/mrna_plum.egg-info/SOURCES.txt
src/mrna_plum.egg-info/dependency_links.txt
src/mrna_plum.egg-info/entry_points.txt
src/mrna_plum.egg-info/requires.txt
src/mrna_plum.egg-info/top_level.txt
src/mrna_plum/io/__init__.py
src/mrna_plum/io/csv_read.py
src/mrna_plum/io/excel_keys.py
src/mrna_plum/merge/__init__.py
src/mrna_plum/merge/merge_logs.py
src/mrna_plum/parse/__init__.py
src/mrna_plum/parse/context.py
src/mrna_plum/parse/parse_logs.py
src/mrna_plum/reports/__init__.py
src/mrna_plum/reports/export_excel.py
src/mrna_plum/reports/export_individual.py
src/mrna_plum/rules/__init__.py
src/mrna_plum/rules/engine.py
src/mrna_plum/rules/models.py
src/mrna_plum/stats/__init__.py
src/mrna_plum/stats/compute_stats.py
src/mrna_plum/store/__init__.py
src/mrna_plum/store/duckdb_store.py
src/mrna_plum/ui_bridge/__init__.py
src/mrna_plum/ui_bridge/progress.py

================================================================================
src\mrna_plum.egg-info\top_level.txt
================================================================================

mrna_plum
